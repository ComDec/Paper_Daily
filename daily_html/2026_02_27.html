<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Preprint Digest - 2026-02-27</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.16.4/framer-motion.dev.js"></script>
    <!-- Example using Font Awesome (replace with your preferred icon library if needed) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');

        :root {
            /* New Palette: Light, Clean, Futuristic with Teal/Aqua accents */
            --bg-color: #f8fafc; /* Tailwind slate-50 (Very Light Gray) */
            --card-bg-color: #ffffff; /* White */
            --text-color: #1e293b; /* Tailwind slate-800 (Dark Gray-Blue) */
            --text-muted-color: #64748b; /* Tailwind slate-500 (Medium Gray-Blue) */
            --header-color: #0f172a; /* Tailwind slate-900 (Very Dark Blue) */
            --highlight-primary: #14b8a6; /* Tailwind teal-500 */
            --highlight-secondary: #67e8f9; /* Tailwind cyan-300 */
            --border-color: #e2e8f0; /* Tailwind slate-200 (Light Gray) */
            --shadow-color: rgba(15, 23, 42, 0.08); /* Subtle shadow based on slate-900 */
        }

        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: 'Inter', sans-serif;
            overflow-x: hidden; /* Prevent horizontal scroll */
            line-height: 1.6;
        }

        .bento-grid {
            display: grid;
            gap: 1.5rem; /* Tailwind gap-6 */
            grid-template-columns: 1fr; /* Force single column */
            padding-bottom: 4rem; /* Add padding at the bottom */
        }

        .bento-item {
            /* Apply semi-transparent white background and blur */
            background-color: rgba(255, 255, 255, 0.7); /* White with 70% opacity */
            backdrop-filter: blur(10px); /* Apply blur effect */
            -webkit-backdrop-filter: blur(10px); /* Safari prefix */
            border-radius: 1rem; /* Slightly larger radius */
            padding: 1.75rem; /* Slightly more padding */
            border: 1px solid rgba(226, 232, 240, 0.5); /* Lighter border with transparency */
            box-shadow: 0 4px 12px var(--shadow-color);
            transition: transform 0.3s ease-out, box-shadow 0.3s ease-out, background-color 0.3s ease-out;
            overflow: hidden; /* Ensure content doesn't overflow */
            position: relative; /* For potential pseudo-elements */
        }

        /* Removed ::before pseudo-element for a cleaner look */


        .bento-item:hover {
            transform: translateY(-6px);
            box-shadow: 0 10px 20px var(--shadow-color), 0 4px 8px rgba(15, 23, 42, 0.06); /* Adjusted hover shadow */
        }

        .paper-title {
            font-size: 1.125rem; /* Tailwind text-lg */
            font-weight: 600; /* Tailwind font-semibold */
            color: var(--highlight-primary); /* Use new primary highlight */
            margin-bottom: 0.75rem; /* Tailwind mb-3 */
            line-height: 1.4;
        }

        .paper-summary {
            font-size: 0.875rem; /* Tailwind text-sm */
            color: var(--text-muted-color);
            margin-bottom: 1.25rem; /* Tailwind mb-5 */
            line-height: 1.6;
        }

        .paper-link {
            display: inline-flex; /* Use flex for icon alignment */
            align-items: center;
            font-size: 0.875rem; /* Tailwind text-sm */
            font-weight: 600;
            color: var(--highlight-primary);
            text-decoration: none;
            padding: 0.5rem 1rem; /* Add padding */
            border-radius: 0.5rem; /* Slightly rounder */
            background-color: rgba(20, 184, 166, 0.08); /* Subtle teal background */
            border: 1px solid rgba(20, 184, 166, 0.2);
            transition: background-color 0.3s ease, color 0.3s ease, transform 0.2s ease;
        }

        .paper-link i {
            margin-right: 0.5rem; /* Tailwind mr-2 */
            transition: transform 0.3s ease;
        }

        .paper-link:hover {
            background-color: rgba(20, 184, 166, 0.15);
            color: #0d9488; /* Darker teal on hover */
            transform: translateY(-1px);
        }
        .paper-link:hover i {
             transform: translateX(2px);
        }

        .paper-authors {
            font-size: 0.75rem; /* Tailwind text-xs */
            color: var(--text-muted-color);
            margin-top: 1rem; /* Tailwind mt-4 */
            font-style: italic;
        }

        .header {
            text-align: center;
            margin-bottom: 3rem; /* Tailwind mb-12 */
            padding-top: 3rem; /* Tailwind pt-12 */
        }

        .header h1 {
            font-size: 2.5rem; /* Tailwind text-4xl or 5xl */
            font-weight: 700; /* Tailwind font-bold */
            color: var(--header-color);
            letter-spacing: -0.025em; /* Tailwind tracking-tight */
            margin-bottom: 0.5rem;
            /* Optional: Add a subtle text gradient */
            /* background: linear-gradient(90deg, var(--highlight-primary), var(--highlight-secondary)); */
            /* -webkit-background-clip: text; */
            /* -webkit-text-fill-color: transparent; */
        }

        .header p {
            font-size: 1.125rem; /* Tailwind text-lg */
            color: var(--text-muted-color);
            margin-top: 0.5rem; /* Tailwind mt-2 */
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
        }

        .footer {
            text-align: center;
            color: var(--text-muted-color);
            font-size: 0.875rem; /* Tailwind text-sm */
            padding-top: 2rem;
            padding-bottom: 2rem; /* Tailwind py-8 */
            border-top: 1px solid var(--border-color);
            margin-top: 4rem;
        }

        /* Simple line graphic element (optional) */
        .line-graphic {
            height: 1px; /* Thinner line */
            background: linear-gradient(90deg, rgba(20, 184, 166, 0), var(--highlight-primary), rgba(20, 184, 166, 0));
            opacity: 0.6;
            margin: 1.5rem 0; /* Adjust margin */
        }

        /* Framer Motion requires the script, styles enhance appearance */
        [data-motion-element] {
             /* Base styles for elements animated by Framer Motion */
        }

        .paper-tldr {
            font-size: 0.95rem; /* Slightly bigger than summary */
            color: #475569; /* Changed to Tailwind slate-600 (slightly darker than summary) */
            margin-top: 0.75rem; /* Tailwind mt-3 */
            margin-bottom: 0.75rem; /* Tailwind mb-2 */
            /* font-style: italic; */
            font-weight: bold;
        }

        .paper-rating {
            margin-top: 1rem; /* Tailwind mt-4 */
            margin-bottom: 1rem; /* Tailwind mb-4 */
            color: #f59e0b; /* Tailwind amber-500 */
        }

        .paper-rating i {
            margin-right: 0.125rem; /* Tailwind mr-0.5 */
        }

        /* Apply consistent star color to sub-ratings */
        .paper-sub-ratings .rating-item i {
            color: #f59e0b; /* Match overall rating star color (amber-500) */
            margin-right: 0.125rem; /* Consistent spacing */
        }

    </style>
</head>
<body class="container mx-auto px-4 antialiased">

    <motion.div
        initial="{ opacity: 0, y: -30 }"
        animate="{ opacity: 1, y: 0 }"
        transition="{ duration: 0.6, ease: 'easeOut' }"
        class="header"
        data-motion-element
    >
        <h1>Daily Preprint Digest</h1>
        
        <p>Generative AI and AI4Science papers from arXiv, bioRxiv, and ChemRxiv</p>
        
        <p>February 27, 2026</p>
        <div class="line-graphic mt-4 mb-8 mx-auto w-1/4"></div> <!-- Added line graphic -->
    </motion.div>

    <div class="bento-grid" id="paper-grid">
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</h2>
            <p class="paper-summary">Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.CR
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23262v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23262v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jasmine Bayrooti, Weiwei Kong, Natalia Ponomareva, Carlos Esteves, Ameesh Makadia, Amanda Prorok</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.05, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving</h2>
            <p class="paper-summary">With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of "only driving like the expert" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI, cs.RO
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23259v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23259v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jiangxin Sun, Feng Xue, Teng Long, Chang Liu, Jian-Fang Hu, Wei-Shi Zheng, Nicu Sebe</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Large Multimodal Models as General In-Context Classifiers</h2>
            <p class="paper-summary">Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their "in-context" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23229v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23229v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Marco Garosi, Matteo Farina, Alessandro Conti, Massimiliano Mancini, Elisa Ricci</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.15000000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction</h2>
            <p class="paper-summary">Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.LG, eess.IV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23214v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23214v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Chenhe Du, Xuanyu Tian, Qing Wu, Muyu Liu, Jingyi Yu, Hongjiang Wei, Yuyao Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation</h2>
            <p class="paper-summary">Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23203v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23203v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Junhu Fu, Shuyu Liang, Wutong Li, Chen Ma, Peng Huang, Kehao Wang, Ke Chen, Shengli Lin, Pinghong Zhou, Zeju Li, Yuanyuan Wang, Yi Guo</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Uni-Animator: Towards Unified Visual Colorization</h2>
            <p class="paper-summary">We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23191v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23191v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xinyuan Chen, Yao Xu, Shaowen Wang, Pengjie Song, Bowen Deng</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.30000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios</h2>
            <p class="paper-summary">Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23166v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23166v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhaochen Su, Jincheng Gao, Hangyu Guo, Zhenhua Liu, Lueyang Zhang, Xinyu Geng, Shijue Huang, Peng Xia, Guanyu Jiang, Cheng Wang, Yue Zhang, Yi R. Fung, Junxian He</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.35000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation</h2>
            <p class="paper-summary">Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23165v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23165v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yichen Peng, Jyun-Ting Song, Siyeol Jung, Ruofan Liu, Haiyang Liu, Xuangeng Chu, Ruicong Liu, Erwin Wu, Hideki Koike, Kris Kitani</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Efficient Encoder-Free Fourier-based 3D Large Multimodal Model</h2>
            <p class="paper-summary">Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23153v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23153v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Yiming Wang, Fabio Poiesi</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors</h2>
            <p class="paper-summary">We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23141v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23141v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Tao Liu, Gang Wan, Kan Ren, Shibo Wen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</h2>
            <p class="paper-summary">Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23114v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23114v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xudong Yan, Songhe Feng, Jiaxin Wang, Xin Su, Yi Jin</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.55, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval</h2>
            <p class="paper-summary">Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a "retrieve-verify-refine" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23029v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23029v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Tianyue Wang, Leigang Qu, Tianyu Yang, Xiangzhao Hao, Yifan Xu, Haiyun Guo, Jinqiao Wang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.6000000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis</h2>
            <p class="paper-summary">Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23022v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23022v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xinglong Luo, Ao Luo, Zhengning Wang, Yueqi Yang, Chaoyu Feng, Lei Lei, Bing Zeng, Shuaicheng Liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets</h2>
            <p class="paper-summary">Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CE, cs.CV, eess.IV, eess.SP, stat.ML
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22974v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22974v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: L. Martino, M. M. Garcia, P. S. Paradas, E. Curbelo</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.7000000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models</h2>
            <p class="paper-summary">World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22960v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22960v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Tianxing Xu, Zixuan Wang, Guangyuan Wang, Li Hu, Zhongyi Zhang, Peng Zhang, Bang Zhang, Song-Hai Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Can Agents Distinguish Visually Hard-to-Separate Diseases in a Zero-Shot Setting? A Pilot Study</h2>
            <p class="paper-summary">The rapid progress of multimodal large language models (MLLMs) has led to increasing interest in agent-based systems. While most prior work in medical imaging concentrates on automating routine clinical workflows, we study an underexplored yet clinically significant setting: distinguishing visually hard-to-separate diseases in a zero-shot setting. We benchmark representative agents on two imaging-only proxy diagnostic tasks, (1) melanoma vs. atypical nevus and (2) pulmonary edema vs. pneumonia, where visual features are highly confounded despite substantial differences in clinical management. We introduce a multi-agent framework based on contrastive adjudication. Experimental results show improved diagnostic performance (an 11-percentage-point gain in accuracy on dermoscopy data) and reduced unsupported claims on qualitative samples, although overall performance remains insufficient for clinical deployment. We acknowledge the inherent uncertainty in human annotations and the absence of clinical context, which further limit the translation to real-world settings. Within this controlled setting, this pilot study provides preliminary insights into zero-shot agent performance in visually confounded scenarios.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22959v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22959v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zihao Zhao, Frederik Hauke, Juliana De Castilhos, Sven Nebelung, Daniel Truhn</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.8, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis</h2>
            <p class="paper-summary">Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: https://github.com/gfnnnb/MM-NeuroOnco</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22955v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22955v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Feng Guo, Jiaxiang Liu, Yang Li, Qianqian Shi, Mingkun Xu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.8500000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding</h2>
            <p class="paper-summary">Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\% accuracy gain upon the base MLLM, and 1.1\% higher accuracy than strongest baseline method.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22932v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22932v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Wenhui Tan, Xiaoyi Yu, Jiaze Li, Yijing Chen, Jianzhong Ju, Zhenbo Luo, Ruihua Song, Jian Luan</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins</h2>
            <p class="paper-summary">A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22919v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22919v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Haofan Wu, Nay Aung, Theodoros N. Arvanitis, Joao A. C. Lima, Steffen E. Petersen, Le Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.9500000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Towards Multimodal Domain Generalization with Few Labels</h2>
            <p class="paper-summary">Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22917v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22917v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Hongzhao Li, Hao Dong, Hualei Wan, Shupan Li, Mingliang Xu, Muhammad Haris Khan</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion</h2>
            <p class="paper-summary">This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.RO, cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22862v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22862v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Enda Xiang, Haoxiang Ma, Xinzhu Ma, Zicheng Liu, Di Huang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.05, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models</h2>
            <p class="paper-summary">As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22859v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22859v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Hongrui Jia, Chaoya Jiang, Shikun Zhang, Wei Ye</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Face Time Traveller : Travel Through Ages Without Losing Identity</h2>
            <p class="paper-summary">Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22819v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22819v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Purbayan Kar, Ayush Ghadiya, Vishal Chudasama, Pankaj Wasnik, C. V. Jawahar</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.1500000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning</h2>
            <p class="paper-summary">With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22809v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22809v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Mingde Yao, Zhiyuan You, Tam-King Man, Menglu Wang, Tianfan Xue</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.2000000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation</h2>
            <p class="paper-summary">We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22785v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22785v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ling Wang, Hao-Xiang Guo, Xinzhou Wang, Fuchun Sun, Kai Sun, Pengkun Liu, Hang Xiao, Zhong Wang, Guangyuan Fu, Eric Li, Yang Liu, Yikai Wang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation</h2>
            <p class="paper-summary">Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22745v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22745v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Fengming Liu, Tat-Jen Cham, Chuanxia Zheng</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.3, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Asymmetric Idiosyncrasies in Multimodal Models</h2>
            <p class="paper-summary">In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22734v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22734v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Muzi Tao, Chufan Shi, Huijuan Wang, Shengbang Tong, Xuezhe Ma</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.35, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling</h2>
            <p class="paper-summary">Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22717v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22717v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Shuoqi Chen, Yujia Wu, Geoffrey P. Luke</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.4000000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs</h2>
            <p class="paper-summary">3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22716v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22716v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Guanting Ye, Qiyan Zhao, Wenhao Yu, Liangyu Yuan, Mingkai Li, Xiaofeng Zhang, Jianmin Ji, Yanyong Zhang, Qing Jiang, Ka-Veng Yuen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.4500000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings</h2>
            <p class="paper-summary">Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.CR
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22689v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22689v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, Sung-Eui Yoon</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses</h2>
            <p class="paper-summary">The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22683v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22683v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhuohang Jiang, Xu Yuan, Haohao Qu, Shanru Lin, Kanglong Liu, Wenqi Fan, Qing Li</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.55, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Scaling Audio-Visual Quality Assessment Dataset via Crowdsourcing</h2>
            <p class="paper-summary">Audio-visual quality assessment (AVQA) research has been stalled by limitations of existing datasets: they are typically small in scale, with insufficient diversity in content and quality, and annotated only with overall scores. These shortcomings provide limited support for model development and multimodal perception research. We propose a practical approach for AVQA dataset construction. First, we design a crowdsourced subjective experiment framework for AVQA, breaks the constraints of in-lab settings and achieves reliable annotation across varied environments. Second, a systematic data preparation strategy is further employed to ensure broad coverage of both quality levels and semantic scenarios. Third, we extend the dataset with additional annotations, enabling research on multimodal perception mechanisms and their relation to content. Finally, we validate this approach through YT-NTU-AVQ, the largest and most diverse AVQA dataset to date, consisting of 1,620 user-generated audio and video (A/V) sequences. The dataset and platform code are available at https://github.com/renyu12/YT-NTU-AVQ</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.MM
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22659v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22659v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Renyu Yang, Jian Jin, Lili Meng, Meiqin Liu, Yilin Wang, Balu Adsumilli, Weisi Lin</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.6, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache</h2>
            <p class="paper-summary">Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22654v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22654v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Bowen Cui, Yuanbin Wang, Huajiang Xu, Biaolong Chen, Aixi Zhang, Hao Jiang, Zhengzheng Jin, Xu Liu, Pipei Huang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.6500000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Plug, Play, and Fortify: A Low-Cost Module for Robust Multimodal Image Understanding Models</h2>
            <p class="paper-summary">Missing modalities present a fundamental challenge in multimodal models, often causing catastrophic performance degradation. Our observations suggest that this fragility stems from an imbalanced learning process, where the model develops an implicit preference for certain modalities, leading to the under-optimization of others. We propose a simple yet efficient method to address this challenge. The central insight of our work is that the dominance relationship between modalities can be effectively discerned and quantified in the frequency domain. To leverage this principle, we first introduce a Frequency Ratio Metric (FRM) to quantify modality preference by analyzing features in the frequency domain. Guided by FRM, we then propose a Multimodal Weight Allocation Module, a plug-and-play component that dynamically re-balances the contribution of each branch during training, promoting a more holistic learning paradigm. Extensive experiments demonstrate that MWAM can be seamlessly integrated into diverse architectural backbones, such as those based on CNNs and ViTs. Furthermore, MWAM delivers consistent performance gains across a wide range of tasks and modality combinations. This advancement extends beyond merely optimizing the performance of the base model; it also manifests as further performance improvements to state-of-the-art methods addressing the missing modality problem.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22644v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22644v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Siqi Lu, Wanying Xu, Yongbin Zheng, Wenting Luan, Peng Sun, Jianhang Yao</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.7000000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">CRAG: Can 3D Generative Models Help 3D Assembly?</h2>
            <p class="paper-summary">Most existing 3D assembly methods treat the problem as pure pose estimation, rearranging observed parts via rigid transformations. In contrast, human assembly naturally couples structural reasoning with holistic shape inference. Inspired by this intuition, we reformulate 3D assembly as a joint problem of assembly and generation. We show that these two processes are mutually reinforcing: assembly provides part-level structural priors for generation, while generation injects holistic shape context that resolves ambiguities in assembly. Unlike prior methods that cannot synthesize missing geometry, we propose CRAG, which simultaneously generates plausible complete shapes and predicts poses for input parts. Extensive experiments demonstrate state-of-the-art performance across in-the-wild objects with diverse geometries, varying part counts, and missing pieces. Our code and models will be released.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22629v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22629v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zeyu Jiang, Sihang Li, Siqi Tan, Chenyang Xu, Juexiao Zhang, Julia Galway-Witham, Xue Wang, Scott A. Williams, Radu Iovita, Chen Feng, Jing Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Instruction-based Image Editing with Planning, Reasoning, and Generation</h2>
            <p class="paper-summary">Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22624v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22624v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Liya Ji, Chenyang Qi, Qifeng Chen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.8, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion</h2>
            <p class="paper-summary">Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22610v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22610v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Tao Huang, Jiayang Meng, Xu Yang, Chen Hou, Hong Chen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.85, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">$$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models</h2>
            <p class="paper-summary">Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22601v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22601v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Thanh-Dat Truong, Huu-Thien Tran, Jackson Cothren, Bhiksha Raj, Khoa Luu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.9000000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">BetterScene: 3D Scene Synthesis with Representation-Aligned Generative Model</h2>
            <p class="paper-summary">We present BetterScene, an approach to enhance novel view synthesis (NVS) quality for diverse real-world scenes using extremely sparse, unconstrained photos. BetterScene leverages the production-ready Stable Video Diffusion (SVD) model pretrained on billions of frames as a strong backbone, aiming to mitigate artifacts and recover view-consistent details at inference time. Conventional methods have developed similar diffusion-based solutions to address these challenges of novel view synthesis. Despite significant improvements, these methods typically rely on off-the-shelf pretrained diffusion priors and fine-tune only the UNet module while keeping other components frozen, which still leads to inconsistent details and artifacts even when incorporating geometry-aware regularizations like depth or semantic conditions. To address this, we investigate the latent space of the diffusion model and introduce two components: (1) temporal equivariance regularization and (2) vision foundation model-aligned representation, both applied to the variational autoencoder (VAE) module within the SVD pipeline. BetterScene integrates a feed-forward 3D Gaussian Splatting (3DGS) model to render features as inputs for the SVD enhancer and generate continuous, artifact-free, consistent novel views. We evaluate on the challenging DL3DV-10K dataset and demonstrate superior performance compared to state-of-the-art methods.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22596v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22596v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yuci Han, Charles Toth, John E. Anderson, William J. Shuart, Alper Yilmaz</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 1.9500000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Causal Motion Diffusion Models for Autoregressive Motion Generation</h2>
            <p class="paper-summary">Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22594v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22594v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Qing Yu, Akihisa Watanabe, Kent Fujiwara</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">GIFSplat: Generative Prior-Guided Iterative Feed-Forward 3D Gaussian Splatting from Sparse Views</h2>
            <p class="paper-summary">Feed-forward 3D reconstruction offers substantial runtime advantages over per-scene optimization, which remains slow at inference and often fragile under sparse views. However, existing feed-forward methods still have potential for further performance gains, especially for out-of-domain data, and struggle to retain second-level inference time once a generative prior is introduced. These limitations stem from the one-shot prediction paradigm in existing feed-forward pipeline: models are strictly bounded by capacity, lack inference-time refinement, and are ill-suited for continuously injecting generative priors. We introduce GIFSplat, a purely feed-forward iterative refinement framework for 3D Gaussian Splatting from sparse unposed views. A small number of forward-only residual updates progressively refine current 3D scene using rendering evidence, achieve favorable balance between efficiency and quality. Furthermore, we distill a frozen diffusion prior into Gaussian-level cues from enhanced novel renderings without gradient backpropagation or ever-increasing view-set expansion, thereby enabling per-scene adaptation with generative prior while preserving feed-forward efficiency. Across DL3DV, RealEstate10K, and DTU, GIFSplat consistently outperforms state-of-the-art feed-forward baselines, improving PSNR by up to +2.1 dB, and it maintains second-scale inference time without requiring camera poses or any test-time gradient optimization.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22571v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22571v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Tianyu Chen, Wei Xiang, Kang Han, Yu Lu, Di Wu, Gaowen Liu, Ramana Rao Kompella</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.0500000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation</h2>
            <p class="paper-summary">Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22570v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22570v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Dian Xie, Shitong Shao, Lichen Bai, Zikai Zhou, Bojun Cheng, Shuo Yang, Jun Wu, Zeke Xie</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation</h2>
            <p class="paper-summary">Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22549v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22549v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhechao Wang, Yiming Zeng, Lufan Ma, Zeqing Fu, Chen Bai, Ziyao Lin, Cheng Lu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.15, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI</h2>
            <p class="paper-summary">Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each modality contributes to the prediction. The method combines (i) a Partial Information Decomposition (PID)-guided, vector-quantized encoder that partitions latent information into redundant, unique, and complementary components, and (ii) a Half-UNet decoder that preserves anatomical detail using pseudo-skip connections conditioned on structural edge cues rather than direct encoder feature reuse. Across multiple baselines (VAE, VQ-VAE, and UNet), DisQ-HNet maintains reconstruction fidelity and better preserves disease-relevant signal for downstream AD tasks, including Braak staging, tau localization, and classification. PID-based Shapley analysis provides modality-specific attribution of synthesized uptake patterns.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22545v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22545v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Agamdeep S. Chopra, Caitlin Neher, Tianyi Ren, Juampablo E. Heras Rivera, Mehmet Kurt</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Space Syntax-guided Post-training for Residential Floor Plan Generation</h2>
            <p class="paper-summary">Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy.
  To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22507v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22507v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhuoyang Jiang, Dongqing Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation</h2>
            <p class="paper-summary">Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.IR
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22462v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22462v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Raiyan Jahangir, Nafiz Imtiaz Khan, Amritanand Sudheerkumar, Vladimir Filkov</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.3000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge</h2>
            <p class="paper-summary">We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22455v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22455v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Giuseppe Lando, Rosario Forte, Antonino Furnari</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.35, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read</h2>
            <p class="paper-summary">Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22426v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22426v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yibo Peng, Peng Xia, Ding Zhong, Kaide Zeng, Siwei Han, Yiyang Zhou, Jiaqi Liu, Ruiyi Zhang, Huaxiu Yao</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.4000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">CLIP Is Shortsighted: Paying Attention Beyond the First Sentence</h2>
            <p class="paper-summary">CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22419v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22419v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Marc-Antoine Lavoie, Anas Mahmoud, Aldo Zaimi, Arsene Fansi Tchango, Steven L. Waslander</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion</h2>
            <p class="paper-summary">Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22405v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22405v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Syed Omer Shah, Mohammed Maqsood Ahmed, Danish Mohiuddin Mohammed, Shahnawaz Alam, Mohd Vahaj ur Rahman</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos</h2>
            <p class="paper-summary">Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22209v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22209v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yufei Ye, Jiaman Li, Ryan Rong, C. Karen Liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.5500000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Solaris: Building a Multiplayer Video World Model in Minecraft</h2>
            <p class="paper-summary">Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22208v2" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22208v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Georgy Savva, Oscar Michel, Daohan Lu, Suppakit Waiwitlikhit, Timothy Meehan, Dhairya Mishra, Srivats Poddar, Jack Lu, Saining Xie</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.6, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</h2>
            <p class="paper-summary">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers" using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image's utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22197v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22197v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.6500000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness</h2>
            <p class="paper-summary">Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CV
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22159v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22159v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Wenhao Guo, Zhaoran Zhao, Peng Lu, Sheng Li, Qian Qiao, RuiDe Li</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.7, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?</h2>
            <p class="paper-summary">Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23225v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23225v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Pengxiang Li, Dilxat Muhtar, Lu Yin, Tianlong Chen, Shiwei Liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">The Trinity of Consistency as a Defining Principle for General World Models</h2>
            <p class="paper-summary">The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23152v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23152v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jingxuan Wei, Siyuan Li, Yuhang Xu, Zheng Sun, Junjie Jiang, Hexuan Jin, Caijun Jia, Honghao He, Xinglong Xu, Xi bai, Chang Yu, Yumou Liu, Junnan Zhu, Xuanhe Zhou, Jintao Chen, Xiaobin Hu, Shancheng Pang, Bihui Yu, Ran He, Zhen Lei, Stan Z. Li, Conghui He, Shuicheng Yan, Cheng Tan</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.8000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs</h2>
            <p class="paper-summary">Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL, cs.AI, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23136v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23136v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jayadev Billa</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.85, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy</h2>
            <p class="paper-summary">As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22971v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22971v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Peiyao Xiao, Xiaogang Li, Chengliang Xu, Jiayi Wang, Ben Wang, Zichao Chen, Zeyu Wang, Kejun Yu, Yueqian Chen, Xulin Liu, Wende Xiao, Bing Zhao, Hu Wei</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.9000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning</h2>
            <p class="paper-summary">Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22963v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22963v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zehao Li, Hongwei Yu, Hao Jiang, Qiang Sheng, Yilong Xu, Baolong Bi, Yang Li, Zhenlong Yuan, Yujun Cai, Zhaoqi Wang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 2.95, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching</h2>
            <p class="paper-summary">Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22871v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22871v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Roy Miles, Aysim Toker, Andreea-Maria Oncescu, Songcen Xu, Jiankang Deng, Ismail Elezi</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics</h2>
            <p class="paper-summary">The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22822v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22822v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yunhua Zhong, Yixuan Tang, Yifan Li, Jie Yang, Pan Liu, Jun Xia</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.0500000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving</h2>
            <p class="paper-summary">Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.RO, cs.AI, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22801v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22801v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yinan Zheng, Tianyi Tan, Bin Huang, Enguang Liu, Ruiming Liang, Jianlin Zhang, Jianwei Cui, Guang Chen, Kun Ma, Hangjun Ye, Long Chen, Ya-Qin Zhang, Xianyuan Zhan, Jingjing Liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Generative Data Transformation: From Mixed to Unified Data</h2>
            <p class="paper-summary">Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22743v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22743v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jiaqing Zhang, Mingjia Yin, Hao Wang, Yuxin Tian, Yuyang Ye, Yawen Li, Wei Guo, Yong Liu, Enhong Chen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.1500000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue</h2>
            <p class="paper-summary">The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22697v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22697v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ning Gao, Wei Zhang, Yuqin Dai, Ling Shi, Ziyin Wang, Yujie Wang, Wei He, Jinpeng Wang, Chaozheng Wang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">dLLM: Simple Diffusion Language Modeling</h2>
            <p class="paper-summary">Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL, cs.AI, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22661v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22661v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhanhui Zhou, Lingjie Chen, Hanghang Tong, Dawn Song</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising</h2>
            <p class="paper-summary">In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22650v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22650v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xinxin Yang, Yangyang Tang, Yikun Zhou, Yaolei Liu, Yun Li, Bo Yang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.3000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion</h2>
            <p class="paper-summary">Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.AI, cs.CL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22586v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22586v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Donghong Cai, Jiarui Feng, Yanbo Wang, Da Zheng, Yixin Chen, Muhan Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.35, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Addressing Climate Action Misperceptions with Generative AI</h2>
            <p class="paper-summary">Mitigating climate change requires behaviour change. However, even climate-concerned individuals often hold misperceptions about which actions most reduce carbon emissions. We recruited 1201 climate-concerned individuals to examine whether discussing climate actions with a large language model (LLM) equipped with climate knowledge and prompted to provide personalised responses would foster more accurate perceptions of the impacts of climate actions and increase willingness to adopt feasible, high-impact behaviours. We compared this to having participants run a web search, have a conversation with an unspecialised LLM, and no intervention. The personalised climate LLM was the only condition that led to increased knowledge about the impacts of climate actions and greater intentions to adopt impactful behaviours. While the personalised climate LLM did not outperform a web search in improving understanding of climate action impacts, the ability of LLMs to deliver personalised, actionable guidance may make them more effective at motivating impactful pro-climate behaviour change.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.HC, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22564v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22564v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Miriam Remshard, Yara Kyrychenko, Sander van der Linden, Matthew H. Goldberg, Anthony Leiserowitz, Elena Savoia, Jon Roozenbeek</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.4000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Autoregressive Visual Decoding from EEG Signals</h2>
            <p class="paper-summary">Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22555v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22555v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Sicheng Dai, Hongwang Xiao, Shan Yu, Qiwei Ye</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Generative Agents Navigating Digital Libraries</h2>
            <p class="paper-summary">In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user search patterns due to privacy concerns. In this context, we introduce Agent4DL, a user search behavior simulator specifically designed for digital library environments. Agent4DL generates realistic user profiles and dynamic search sessions that closely mimic actual search strategies, including querying, clicking, and stopping behaviors tailored to specific user profiles. Our simulator's accuracy in replicating real user interactions has been validated through comparisons with real user data. Notably, Agent4DL demonstrates competitive performance compared to existing user search simulators such as SimIIR 2.0, particularly in its ability to generate more diverse and context-aware user behaviors.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.AI, cs.DL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22529v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22529v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Saber Zerhoudi, Michael Granitzer</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation</h2>
            <p class="paper-summary">We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.
  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.
  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.RO, cs.AI, eess.SY
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22514v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22514v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xinyu Tan, Ningwei Bai, Harry Gardener, Zhengyang Zhong, Luoyu Zhang, Liuhaichen Yang, Zhekai Duan, Monkgogi Galeitsiwe, Zezhi Tang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.5500000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">ECHO: Encoding Communities via High-order Operators</h2>
            <p class="paper-summary">Community detection in attributed networks faces a fundamental divide: topological algorithms ignore semantic features, while Graph Neural Networks (GNNs) encounter devastating computational bottlenecks. Specifically, GNNs suffer from a Semantic Wall of feature over smoothing in dense or heterophilic networks, and a Systems Wall driven by the O(N^2) memory constraints of pairwise clustering. To dismantle these barriers, we introduce ECHO (Encoding Communities via High order Operators), a scalable, self supervised architecture that reframes community detection as an adaptive, multi scale diffusion process. ECHO features a Topology Aware Router that automatically analyzes structural heuristics sparsity, density, and assortativity to route graphs through the optimal inductive bias, preventing heterophilic poisoning while ensuring semantic densification. Coupled with a memory sharded full batch contrastive objective and a novel chunked O(N \cdot K) similarity extraction method, ECHO completely bypasses traditional O(N^2) memory bottlenecks without sacrificing the mathematical precision of global gradients. Extensive evaluations demonstrate that this topology feature synergy consistently overcomes the classical resolution limit. On synthetic LFR benchmarks scaled up to 1 million nodes, ECHO achieves scale invariant accuracy despite severe topological noise. Furthermore, on massive real world social networks with over 1.6 million nodes and 30 million edges, it completes clustering in mere minutes with throughputs exceeding 2,800 nodes per second matching the speed of highly optimized purely topological baselines. The implementation utilizes a unified framework that automatically engages memory sharded optimization to support adoption across varying hardware constraints. GitHub Repository: https://github.com/emilioferrara/ECHO-GNN</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22446v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22446v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Emilio Ferrara</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.6, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Calibrated Test-Time Guidance for Bayesian Inference</h2>
            <p class="paper-summary">Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22428v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22428v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Daniel Geyfman, Felix Draxler, Jan Groeneveld, Hyunsoo Lee, Theofanis Karaletsos, Stephan Mandt</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.6500000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">ArchAgent: Agentic AI-driven Computer Architecture Discovery</h2>
            <p class="paper-summary">Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.
  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.
  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar "winning margin" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.
  Agentic flows also enable "post-silicon hyperspecialization" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.
  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of "simulator escapes", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.AI, cs.AR
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22425v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22425v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Raghav Gupta, Akanksha Jain, Abraham Gonzalez, Alexander Novikov, Po-Sen Huang, Matej Balog, Marvin Eisenberger, Sergey Shirobokov, Ngn V, Martin Dixon, Borivoje Nikoli, Parthasarathy Ranganathan, Sagar Karandikar</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.7, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</h2>
            <p class="paper-summary">Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (_i, _i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.SE, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22368v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jiahao Zhang, Yifan Zhang, Kevin Leach, Yu Huang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Decoder-based Sense Knowledge Distillation</h2>
            <p class="paper-summary">Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL, cs.AI
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22351v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22351v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Qitong Wang, Mohammed J. Zaki, Georgios Kollias, Vasileios Kalantzis</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.8000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads</h2>
            <p class="paper-summary">Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagement metrics. Analyzing this brief window is challenging due to the multimodal nature of video content, which blends visual, auditory, and textual elements. Traditional methods often miss the nuanced interplay of these components, requiring advanced frameworks for thorough evaluation.
  This study presents a framework using transformer-based multimodal large language models (MLLMs) to analyze the hooking period of video ads. It tests two frame sampling strategies, uniform random sampling and key frame selection, to ensure balanced and representative acoustic feature extraction, capturing the full range of design elements. The hooking video is processed by state-of-the-art MLLMs to generate descriptive analyses of the ad's initial impact, which are distilled into coherent topics using BERTopic for high-level abstraction. The framework also integrates features such as audio attributes and aggregated ad targeting information, enriching the feature set for further analysis.
  Empirical validation on large-scale real-world data from social media platforms demonstrates the efficacy of our framework, revealing correlations between hooking period features and key performance metrics like conversion per investment. The results highlight the practical applicability and predictive power of the approach, offering valuable insights for optimizing video ad strategies. This study advances video ad analysis by providing a scalable methodology for understanding and enhancing the initial moments of video advertisements.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.MM, cs.AI, cs.CL, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22299v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22299v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Kunpeng Zhang, Poppy Zhang, Shawndra Hill, Amel Awadelkarim</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.85, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models</h2>
            <p class="paper-summary">Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, q-bio.BM
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23179v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23179v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Gal Kesten-Pomeranz, Yaniv Nikankin, Anja Reusch, Tomer Tsaban, Ora Schueler-Furman, Yonatan Belinkov</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.9000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MetaOthello: A Controlled Study of Multiple World Models in Transformers</h2>
            <p class="paper-summary">Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting "world models". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, and train small GPTs on mixed-variant data to study how multiple world models are organized in a shared representation space. We find that transformers trained on mixed-game data do not partition their capacity into isolated sub-models; instead, they converge on a mostly shared board-state representation that transfers causally across variants. Linear probes trained on one variant can intervene on another's internal state with effectiveness approaching that of matched probes. For isomorphic games with token remapping, representations are equivalent up to a single orthogonal rotation that generalizes across layers. When rules partially overlap, early layers maintain game-agnostic representations while a middle layer identifies game identity, and later layers specialize. MetaOthello offers a path toward understanding not just whether transformers learn world models, but how they organize many at once.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23164v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23164v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Aviral Chawla, Galen Hall, Juniper Lovato</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 3.95, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Prediction of Diffusion Coefficients in Mixtures with Tensor Completion</h2>
            <p class="paper-summary">Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23142v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23142v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zeno Romero, Kerstin Mnnemann, Hans Hasse, Fabian Jirasek</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation</h2>
            <p class="paper-summary">Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23132v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23132v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ruochen Yang, Xiaodong Li, Jiawei Sheng, Jiangxia Cao, Xinkui Lin, Shen Wang, Shuang Yang, Zhaojie Liu, Tingwen Liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.05, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Physics-informed neural particle flow for the Bayesian update step</h2>
            <p class="paper-summary">The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations. Conversely, existing deep learning approximations typically treat the update as a black-box task or rely on asymptotic relaxation, neglecting the exact geometric structure of the finite-horizon probability transport. In this work, we propose a physics-informed neural particle flow, which is an amortized inference framework. To construct the flow, we couple the log-homotopy trajectory of the prior to posterior density function with the continuity equation describing the density evolution. This derivation yields a governing partial differential equation (PDE), referred to as the master PDE. By embedding this PDE as a physical constraint into the loss function, we train a neural network to approximate the transport velocity field. This approach enables purely unsupervised training, eliminating the need for ground-truth posterior samples. We demonstrate that the neural parameterization acts as an implicit regularizer, mitigating the numerical stiffness inherent to analytic flows and reducing online computational complexity. Experimental validation on multimodal benchmarks and a challenging nonlinear scenario confirms better mode coverage and robustness compared to state-of-the-art baselines.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23089v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23089v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Domonkos Csuzdi, Tams Bcsi, Olivr Tr</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.1000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Q-Tag: Watermarking Quantum Circuit Generative Models</h2>
            <p class="paper-summary">Quantum cloud platforms have become the most widely adopted and mainstream approach for accessing quantum computing resources, due to the scarcity and operational complexity of quantum hardware. In this service-oriented paradigm, quantum circuits, which constitute high-value intellectual property, are exposed to risks of unauthorized access, reuse, and misuse. Digital watermarking has been explored as a promising mechanism for protecting quantum circuits by embedding ownership information for tracing and verification. However, driven by recent advances in generative artificial intelligence, the paradigm of quantum circuit design is shifting from individually and manually constructed circuits to automated synthesis based on quantum circuit generative models (QCGMs). In such generative settings, protecting only individual output circuits is insufficient, and existing post hoc, circuit-centric watermarking methods are not designed to integrate with the generative process, often failing to simultaneously ensure stealthiness, functional correctness, and robustness at scale. These limitations highlight the need for a new watermarking paradigm that is natively integrated with quantum circuit generative models. In this work, we present the first watermarking framework for QCGMs, which embeds ownership signals into the generation process while preserving circuit fidelity. We introduce a symmetric sampling strategy that aligns watermark encoding with the model's Gaussian prior, and a synchronization mechanism that counteracts adversarial watermark attack through latent drift correction. Empirical results confirm that our method achieves high-fidelity circuit generation and robust watermark detection across a range of perturbations, paving the way for scalable, secure copyright protection in AI-powered quantum design.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: quant-ph, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23085v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23085v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yang Yang, Yuzhu Long, Han Fang, Zhaoyun Chen, Zhonghui Li, Weiming Zhang, Guoping Guo</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.15, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection</h2>
            <p class="paper-summary">Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23060v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23060v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xin Wang, Burcu Ozek, Aruna Mohan, Amirhossein Ravari, Or Zilbershot, Fatemeh Afghah</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Sequential Regression for Continuous Value Prediction using Residual Quantization</h2>
            <p class="paper-summary">Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23012v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23012v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Runpeng Cui, Zhipeng Sun, Chi Lu, Peng Jiang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress</h2>
            <p class="paper-summary">With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22913v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22913v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yang Yu, Lei Kou, Huaikuan Yi, Bin Chen, Yayu Cao, Lei Shen, Chao Zhang, Bing Wang, Xiaoyi Zeng</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.3, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA</h2>
            <p class="paper-summary">Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22903v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22903v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yunpeng Hong, Chenyang Bu, Jie Zhang, Yi He, Di Wu, Xindong Wu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.3500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Unsupervised Continual Learning for Amortized Bayesian Inference</h2>
            <p class="paper-summary">Amortized Bayesian Inference (ABI) enables efficient posterior estimation using generative neural networks trained on simulated data, but often suffers from performance degradation under model misspecification. While self-consistency (SC) training on unlabeled empirical data can enhance network robustness, current approaches are limited to static, single-task settings and fail to handle sequentially arriving data or distribution shifts. We propose a continual learning framework for ABI that decouples simulation-based pre-training from unsupervised sequential SC fine-tuning on real-world data. To address the challenge of catastrophic forgetting, we introduce two adaptation strategies: (1) SC with episodic replay, utilizing a memory buffer of past observations, and (2) SC with elastic weight consolidation, which regularizes updates to preserve task-critical parameters. Across three diverse case studies, our methods significantly mitigate forgetting and yield posterior estimates that outperform standard simulation-based training, achieving estimates closer to MCMC reference, providing a viable path for trustworthy ABI across a range of different tasks.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: stat.ML, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22884v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22884v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Aayush Mishra, imon Kucharsk, Paul-Christian Brkner</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Generative Recommendation for Large-Scale Advertising</h2>
            <p class="paper-summary">Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22732v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22732v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ben Xue, Dan Liu, Lixiang Wang, Mingjie Sun, Peng Wang, Pengfei Zhang, Shaoyun Shi, Tianyu Xu, Yunhao Sha, Zhiqiang Liu, Bo Kong, Bo Wang, Hang Yang, Jieting Xue, Junhao Wang, Shengyu Wang, Shuping Hui, Wencai Ye, Xiao Lin, Yongzhi Li, Yuhang Chen, Zhihui Yin, Quan Chen, Shiyang Wen, Wenjin Wu, Han Li, Guorui Zhou, Changcheng Li, Peng Jiang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators</h2>
            <p class="paper-summary">Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.IR, cs.CL, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22647v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22647v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhengyang Su, Isay Katsman, Yueqi Wang, Ruining He, Lukasz Heldt, Raghunandan Keshavan, Shao-Chuan Wang, Xinyang Yi, Mingyan Gao, Onkar Dalal, Lichan Hong, Ed Chi, Ningren Han</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation</h2>
            <p class="paper-summary">The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22537v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22537v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Shouwei Gao, Xu Zheng, Dongsheng Luo, Sheng Di, Wenqian Dong</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.55, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Sharp Convergence Rates for Masked Diffusion Models</h2>
            <p class="paper-summary">Discrete diffusion models have achieved strong empirical performance in text and other symbolic domains, with masked (absorbing-rate) variants emerging as competitive alternatives to autoregressive models. Among existing samplers, the Euler method remains the standard choice in many applications, and more recently, the First-Hitting Sampler (FHS) has shown considerable promise for masked diffusion models. Despite their practical success, the theoretical understanding of these samplers remains limited. Existing analyses are conducted in Kullback-Leibler (KL) divergence, which often yields loose parameter dependencies and requires strong assumptions on score estimation. Moreover, these guarantees do not cover recently developed high-performance sampler of FHS. In this work, we first develop a direct total-variation (TV) based analysis for the Euler method that overcomes these limitations. Our results relax assumptions on score estimation, improve parameter dependencies, and establish convergence guarantees without requiring any surrogate initialization. Also for this setting, we provide the first convergence lower bound for the Euler sampler, establishing tightness with respect to both the data dimension $d$ and the target accuracy $\varepsilon$. Finally, we analyze the FHS sampler and show that it incurs no sampling error beyond that induced by score estimation, which we show to be tight with a matching lower error bound. Overall, our analysis introduces a direct TV-based error decomposition along the CTMC trajectory and a decoupling-based path-wise analysis for FHS, which may be of independent interest.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG, stat.ML
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22505v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22505v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yuchen Liang, Zhiheng Tan, Ness Shroff, Yingbin Liang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.6000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Flow Matching is Adaptive to Manifold Structures</h2>
            <p class="paper-summary">Flow matching has emerged as a simulation-free alternative to diffusion-based generative modeling, producing samples by solving an ODE whose time-dependent velocity field is learned along an interpolation between a simple source distribution (e.g., a standard normal) and a target data distribution. Flow-based methods often exhibit greater training stability and have achieved strong empirical performance in high-dimensional settings where data concentrate near a low-dimensional manifold, such as text-to-image synthesis, video generation, and molecular structure generation. Despite this success, existing theoretical analyses of flow matching assume target distributions with smooth, full-dimensional densities, leaving its effectiveness in manifold-supported settings largely unexplained. To this end, we theoretically analyze flow matching with linear interpolation when the target distribution is supported on a smooth manifold. We establish a non-asymptotic convergence guarantee for the learned velocity field, and then propagate this estimation error through the ODE to obtain statistical consistency of the implicit density estimator induced by the flow-matching objective. The resulting convergence rate is near minimax-optimal, depends only on the intrinsic dimension, and reflects the smoothness of both the manifold and the target distribution. Together, these results provide a principled explanation for how flow matching adapts to intrinsic data geometry and circumvents the curse of dimensionality.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: stat.ML, cs.LG, math.ST
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22486v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22486v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Shivam Kumar, Yixin Wang, Lizhen Lin</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering</h2>
            <p class="paper-summary">Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.RO, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22474v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22474v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jessie Yuan, Yilin Wu, Andrea Bajcsy</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.7, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">mmWave Radar Aware Dual-Conditioned GAN for Speech Reconstruction of Signals With Low SNR</h2>
            <p class="paper-summary">Millimeter-wave (mmWave) radar captures are band-limited and noisy, making for difficult reconstruction of intelligible full-bandwidth speech. In this work, we propose a two-stage speech reconstruction pipeline for mmWave using a Radar-Aware Dual-conditioned Generative Adversarial Network (RAD-GAN), which is capable of performing bandwidth extension on signals with low signal-to-noise ratios (-5 dB to -1 dB), captured through glass walls. We propose an mmWave-tailored Multi-Mel Discriminator (MMD) and a Residual Fusion Gate (RFG) to enhance the generator input to process multiple conditioning channels. The proposed two-stage pipeline involves pretraining the model on synthetically clipped clean speech and finetuning on fused mel spectrograms generated by the RFG. We empirically show that the proposed method, trained on a limited dataset, with no pre-trained modules, and no data augmentations, outperformed state-of-the-art approaches for this specific task. Audio examples of RAD-GAN are available online at https://rad-gan-demo-site.vercel.app/.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.SD, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22431v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22431v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jash Karani, Adithya Chittem, Deepan Roy, Sandeep Joshi</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">TopoEdit: Fast Post-Optimization Editing of Topology Optimized Structures</h2>
            <p class="paper-summary">Despite topology optimization producing high-performance structures, late-stage localized revisions remain brittle: direct density-space edits (e.g., warping pixels, inserting holes, swapping infill) can sever load paths and sharply degrade compliance, while re-running optimization is slow and may drift toward a qualitatively different design. We present TopoEdit, a fast post-optimization editor that demonstrates how structured latent embeddings from a pre-trained topology foundation model (OAT) can be repurposed as an interface for physics-aware engineering edits. Given an optimized topology, TopoEdit encodes it into OAT's spatial latent, applies partial noising to preserve instance identity while increasing editability, and injects user intent through an edit-then-denoise diffusion pipeline. We instantiate three edit operators: drag-based topology warping with boundary-condition-consistent conditioning updates, shell-infill lattice replacement using a lattice-anchored reference latent with updated volume-fraction conditioning, and late-stage no-design region enforcement via masked latent overwrite followed by diffusion-based recovery. A consistency-preserving guided DDIM procedure localizes changes while allowing global structural adaptation; multiple candidates can be sampled and selected using a compliance-aware criterion, with optional short SIMP refinement for warps. Across diverse case studies and large edit sweeps, TopoEdit produces intention-aligned modifications that better preserve mechanical performance and avoid catastrophic failure modes compared to direct density-space edits, while generating edited candidates in sub-second diffusion time per sample.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.GR, cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22430v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22430v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Hongrui Chen, Josephine V. Carstensen, Faez Ahmed</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.800000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization</h2>
            <p class="paper-summary">Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.LG
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22387v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22387v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yixuan Li, Archer Y. Yang, Yue Li</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.8500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference</h2>
            <p class="paper-summary">Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \times$ inference speedup without any quality degradation.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22868v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22868v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yushi Ye, Feng Hong, Huangjie Zheng, Xu Chen, Zhiyong Chen, Yanfeng Wang, Jiangchao Yao</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Imagination Helps Visual Reasoning, But Not Yet in Latent Space</h2>
            <p class="paper-summary">Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22766v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22766v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: You Li, Chi Chen, Yanghao Li, Fanhu Zeng, Kaiyu Huang, Jinan Xu, Maosong Sun</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 4.95, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Deepfake Word Detection by Next-token Prediction using Fine-tuned Whisper</h2>
            <p class="paper-summary">Deepfake speech utterances can be forged by replacing one or more words in a bona fide utterance with semantically different words synthesized by speech generative models. While a dedicated synthetic word detector could be developed, we investigate a cost-effective method that fine-tunes a pre-trained Whisper model to detect synthetic words while transcribing the input utterance via next-token prediction. We further investigate using partially vocoded utterances as the fine-tuning data, thereby reducing the cost of data collection. Our experiments demonstrate that, on in-domain test data, the fine-tuned Whisper yields low synthetic-word detection error rates and transcription error rates. On out-of-domain test data with synthetic words produced by unseen speech generative models, the fine-tuned Whisper remains on par with a dedicated ResNet-based detection model; however, the overall performance degradation calls for strategies to improve its generalization capability.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: eess.AS, cs.CL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22658v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22658v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Hoan My Tran, Xin Wang, Wanying Ge, Xuechen Liu, Junichi Yamagishi</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context</h2>
            <p class="paper-summary">Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22404v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22404v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Aishwarya Verma, Laud Ammah, Olivia Nercy Ndlovu Lucas, Andrew Zaldivar, Vinodkumar Prabhakaran, Sunipa Dev</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.050000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework</h2>
            <p class="paper-summary">Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cs.CL
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22391v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22391v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Rakib Ullah, Mominul islam, Md Sanjid Hossain, Md Ismail Hossain</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.1000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">An Active Learning Framework for Data-Efficient, Human-in-the-Loop Enzyme Function Prediction</h2>
            <p class="paper-summary">Generalizable protein function prediction is increasingly constrained by the growing mismatch between exponentially expanding sequences of environmental proteins and the comparatively slow accumulation of experimentally verified functional data. Active learning offers a promising path forward for accelerating biological function prediction, by selecting the most informative proteins to experimentally annotate for data-efficient training, yet its potential remains largely unexplored. We introduce HATTER (Human-in-the-loop Adaptive Toolkit for Transferable Enzyme Representations), a modular framework that integrates multiple active learning strategies with human-in-the-loop experimental annotation to efficiently fine tune function prediction models. We compare active learning training to standard supervised training for biological enzyme function prediction, demonstrating that active learning achieves performance comparable to standard training across diverse protein sequence evaluation datasets while requiring fewer model updates, processing less data, and substantially reducing computational cost. Interestingly, point-based uncertainty sampling methods like entropy or margin sampling perform as well or better than more complex acquisition functions such as bayesian sampling or BALD, highlighting the relative importance of sequence diversity in training datasets and model architecture design. These results demonstrate that human-in-the-loop active learning can efficiently accelerate enzyme discovery, providing a flexible platform for adaptive, scalable, and expert-guided protein function prediction.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: q-bio.QM
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23269v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23269v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ashley Babjac, Adrienne Hoarfrost</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.15, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Efficient training of generative models from multireference simulations and its application to the design of Dy complexes with large magnetic anisotropy</h2>
            <p class="paper-summary">Generative machine learning models can potentially provide direct access to novel and relevant portions of the full chemical space, overcoming the cost of systematic sampling. However, the training of these models generally requires a large amount of data, often precluding the use of expensive high-level ab initio simulations for this task. The generation of coordination compounds of Dy with large magnetic anisotropy represents a topical example, where multireference simulations of large molecules are necessary to perform reliable predictions. Here, we show that a semi-supervised chemically-inspired training-by-proxy of generative variational autoencoders can reduce the cost associated with building a training set from multireference simulations by two orders of magnitude. We illustrate the power of this approach by generating 100s of new organic ligands for Dy(III) pentagonal bipyramidal complexes exhibiting record values of magnetic anisotropy, while starting from datasets as small as 1k multireference calculations. This work thus paves the way to the computational generation of molecules as complex coordination compounds with target electronic and magnetic properties.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cond-mat.mtrl-sci, physics.chem-ph
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23230v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23230v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zahra Khatibi, Lorenzo A. Mariano, Lion Frangoulis, Alessandro Lunghi</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Evidence of orbital mixing upon ionization via Cooper minimum photoelectron dynamics in epichlorohydrin. Experiment and Theory</h2>
            <p class="paper-summary">A peculiar electron correlation effect, leading to orbital rotation upon ionization, theoretically predicted long ago, was never experimentally characterized. The effect is expected to appear prominently in the photoionization of chiral molecules, due to the lack of symmetry constraints to wave-functions mixing. This is observed to have a profound effect on the photoelectron dynamics, as here demonstrated by investigating \b{eta} asymmetry parameters and partial cross-section observables in the Cl 3p Cooper minimum region of epichlorohydrin, a chiral prototype system. Angle-resolved photoelectron spectroscopy with tunable synchrotron radiation allowed measuring Cooper minimum $$ oscillations, which were observed for solely two valence photoionization channels. The nature and number of channels exhibiting such dynamical behavior, along with the extent of the observed oscillation amplitudes, could not be accounted for by predictions based on Hartree-Fock (HF) and Density Functional Theory (DFT). These features could only be explained by incorporating correlation effects, which mix single-hole configurations of identical symmetry, in the characterization of the four lowest-lying molecular cation states, via equation-of-motion coupled cluster singles and doubles Dyson orbitals.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: physics.chem-ph
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23223v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23223v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: L. Schio, M. Alagia, T. Moitra, D. Toffoli, A. Ponzi, M. Stener, S. Coriani, P. Decleva, O. Rebrov, V. Zhaunerchyk, M. Larsson, S. Falcinelli, A. A. Dias, D. Catone, S. Turchini, N. Zema, F. Salvador, D. Benedetti, D. Vivoda, B. Botta, S. Stranges</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Learning Thermal Response Forces: A Method for Extending the Thermodynamic Transferability of Coarse-Grained Models via Machine-Learning</h2>
            <p class="paper-summary">Machine-learned (ML) coarse-grained (CG) models are a promising tool for significantly enhancing the efficiency of molecular simulations by systematically removing degrees of freedom while retaining fidelity to the underlying fine-grained model. The CG potential of mean force (PMF) is inherently dependent on thermodynamic conditions and, hence, a CG force-field (FF) which is trained at one thermodynamic state point is not necessarily accurate at another. We propose, in this work, a novel and data-efficient means of learning temperature dependence into ML CG force-fields via training on the thermal response forces of the PMF. We demonstrate how incorporating these terms into ML CG FFs confers significantly improved transferability for CG water models and demonstrate how this transferability enables accurate and predictive CG dynamics.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: physics.chem-ph
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.23198v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.23198v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Patrick G. Sahrmann, Benjamin T. Nebgen, Kipton Barros, Brenden W. Hamilton</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.300000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Benchmarking short-range machine learning potentials for atomistic simulations of metal/electrolyte interfaces</h2>
            <p class="paper-summary">Atomistic simulations of electrochemical interfaces remain challenging due to the long time scales required to adequately sample the structure of the electric double layer. The emergence of efficient, short-range machine learning interatomic potentials (MLIPs) offers a promising alternative to computationally expensive density functional theory-based molecular dynamics (DFT-MD) simulations in this regard. However, in standard periodic DFT calculations of metal surfaces, the surface charge is implicitly set by the number of counterions in the simulation cell, making it a global property that is difficult to represent with strictly local MLIPs. Here, we benchmark common MLIP architectures (DP, ACE, MACE) for charged Au/water interfaces containing solvated sodium ions. We find that MLIPs trained on datasets spanning multiple surface charge states yield inconsistent predictions of interfacial water orientation and ion distributions, although message-passing models with a larger receptive field exhibit greater robustness to training on mixed-charge datasets. In contrast, models trained on a single charge state produce consistent equilibrium interfacial properties. Finally, we assess the performance of the eSEN model trained on the recently released Open Catalyst 2025 dataset, which includes solid/liquid interfaces that span a wide range of surface charge densities. Overall, our results characterize the limitations of short-range MLIPs for simulations of electrochemical interfaces and provide practical guidance for constructing training datasets for simulations of charged metal/electrolyte interfaces.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: physics.chem-ph
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22931v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22931v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Lucas B. T. de Kam, Jia-Xin Zhu, Ankit Mathanker, Katharina Doblhoff-Dier, Nitish Govindarajan</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.3500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">An Information-theoretic Collective Variable for Configurational Entropy</h2>
            <p class="paper-summary">Entropy governs molecular self-assembly, phase transitions, and material stability, yet remains challenging to quantify and directly control in molecular systems. Here, we demonstrate that the computable information density (CID), a data compression-based information theoretic metric, provides an instantaneous general measure of configurational entropy in molecular dynamics simulations, reflecting both local and long-range structural organization. We validate the CID across systems of increasing complexity, beginning with single-component Lennard-Jones melting before examining binary phase separation, polymer condensation and dispersion, and assembly of amorphous carbon networks at multiple densities. Unlike conventional order parameters, CID requires no a priori knowledge of relevant structural features and captures entropic signatures across a variety of molecular systems and discretization resolutions. By establishing entropy as a directly accessible structural metric, this framework lays a foundation for future entropy-driven materials design and optimization strategies.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: arxiv
              
              &middot; Categories: cond-mat.stat-mech, cond-mat.mtrl-sci, cond-mat.soft, physics.chem-ph
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://arxiv.org/pdf/2602.22440v1" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="http://arxiv.org/abs/2602.22440v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ashley Z. Guo, Kaelyn Chang, Nicholas J. Corrente</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Detection of bacteria through taste receptors primes the cellular immune response</h2>
            <p class="paper-summary">Animals use their sensory system to detect cues in their external environment, then communicate, process, and integrate these cues through the nervous system in order to elicit a specific response. Taste is an important cue used by animals to explore their external environment and can modulate various aspects of animal behavior and physiology. A major ongoing challenge for animals is to detect and respond to the presence of a variety of microbes in their environment. However, to date, the links between the sensory system and the response to pathogenic threats remain poorly understood. Here we show that Drosophila larvae use their taste system to detect bacterial peptidoglycans in their environment and respond by modulating the activity of their cellular immune system. We show that specific PeptidoGlycan Receptor Proteins (PGRPs) act in aversive taste neurons, via a non-canonical IMmune Deficiency (IMD) pathway. These PGRPs mediate signaling in taste neurons and control immune cells production in the larval hematopoietic organ, the lymph gland. Taste-mediated sensing of bacteria in larvae primes the immune system, and improves survival after infection in adult flies. These results demonstrate that sensory inputs such as taste play an important role in protecting animals from bacterial infection by providing a powerful adaptive response to potential pathogens. Overall, our findings add to the growing list of examples of crosstalk between the nervous and immune systems and provide novel and important mechanisms for linking them.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: immunology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2024.09.26.615243v5.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2024.09.26.615243v5" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Mazariegos, A. N., Maniere, G., Sillon, L., Milleville, R., Berthelot-Grosjean, M., Aruci, E., Camp, D., Alves, G., Khaul, R., Duval, C. J., Chauvel, I., Royet, J., Grosjean, Y., Musso, P.-y., Tanentzapf, G.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">A universal scaling law for mitotic spindles across eukaryotes driven by chromosome crowding</h2>
            <p class="paper-summary">Cells regulate the size of their internal structures to maintain function in diverse biological settings1. The mitotic spindle, a molecular micro-machine responsible for chromosome segregation2, must scale to accommodate genomes varying in size by over 10,000-fold across eukaryotes3. Yet, how spindle biomechanics adapts to vastly different genome sizes remains unknown. Here, we uncover a universal spindle scaling law, where metaphase plate width scales with genome size following a power law with an exponent of ~1/3. We hypothesize that chromosome crowding within the metaphase plate generates compressive forces as chromosomes push against each other, thereby determining spindle size and shape. Our experiments with altered chromosome number and mechanical properties in healthy and cancerous human and mouse cells, together with a theoretical model based on inter-chromosome pushing forces and mechanical manipulations of cells, confirm this hypothesis. Extending these insights across eukaryotes, we demonstrate that chromosome crowding predicts the observed power-law scaling. The biophysical constraint of chromosome crowding offers a mechanistic explanation for the evolution of open mitosis and mitotic cell rounding, enabling the division of larger genomes. Spindle adaptability to larger genomes may promote the proliferation of polyploid cells, driving not only tumor progression but also speciation during evolution.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.03.05.641650v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.03.05.641650v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Gudlin, L., Vukusic, K., Novak, M., Trupinic, M., Ljulj, M., Dundovic, I., Petelinec, A., Petrusic, L., Hertel, A., van Ravesteyn, T., Trakala, M., Kops, G. J. P. L., Storchova, Z., Tambaca, J., Pavin, N., Tolic, I. M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Defining the ordered pathway for ZAP-mediated RNA decay</h2>
            <p class="paper-summary">Zinc-finger Antiviral Protein (ZAP)-mediated RNA decay (ZMD) restricts replication of viruses containing CpG dinucleotide clusters. However, why ZAP isoforms differ in antiviral activity and how they recruit cofactors to mediate RNA decay is unclear. Therefore, we determined the ordered events of the ZMD pathway. The long ZAP isoform preferentially binds viral RNA and has distinct binding motifs compared to the short isoform. The endoribonuclease KHNYN then cleaves viral RNA at positions of ZAP binding. The 5' cleavage fragment undergoes TUT4/TUT7-mediated 3' uridylation and degradation by DIS3L2. The 3' cleavage fragment is degraded by XRN1. ZAP and TRIM25 interact with KHNYN, TUT7, DIS3L2 and XRN1 in a RNase-resistant manner. Viral infection promotes the interaction between TRIM25 with these enzymes, leading to viral RNA decay while also decreasing the abundance of cellular transcripts. Overall, the long isoform of ZAP recruits key enzymes to assemble an RNA decay complex on viral RNA.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: immunology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.04.28.650959v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.04.28.650959v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Bouton, C. R., Gimpelj Domjanic, G., Lista, M. J., Galao, R. P., Courty, T., Kwiatkowski, P., Wilson, H. D., Hill, P. W. S., Mischo, H. E., Chakrabarti, A. M., Poljak, M., Ule, J., Neil, S. J. D., Swanson, C. M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.550000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Intracranial hypertension drives astrocyte-mediated neuroinflammation through Piezo1-dependent EGFR activation</h2>
            <p class="paper-summary">Intracranial hypertension is a major driver of secondary injury after acute subdural hematoma (ASDH), yet how mechanical stress is translated into neuroinflammatory signaling remains poorly understood. Here, we identify a mechanosensitive astrocyte signaling pathway that links elevated intracranial pressure (ICP) to inflammatory amplification in the injured brain. Using a clinically relevant porcine ASDH model combined with mechanistic studies in human iPSC-derived astrocytes, we demonstrate that sustained ICP elevation induces bilateral neuroinflammation together with coordinated upregulation of mechanosensitive ion channels and receptor tyrosine kinase (RTK) pathways. Integrative analysis of molecular and physiological datasets identified astrocytes as the principal cellular responders to ICP and revealed epidermal growth factor receptor (EGFR) as the astrocyte-associated RTK most strongly correlated with ICP dynamics, inflammatory chemokine expression, and survival. Pharmacological activation of the mechanosensitive channel Piezo1 in human astrocytes was sufficient to trigger EGFR internalization, site-specific phosphorylation, and ERK signaling, promoting structural remodeling and robust induction of pro-inflammatory mediators including CCL2, IL-6, and IL-8. Conversely, EGFR inhibition attenuated inflammatory signaling while enhancing astrocytic programs associated with water handling and edema containment. In vivo, increased expression of EGFR ligands together with elevated EGFR phosphorylation supported sustained pathway engagement following ASDH, and correlation analyses linked Piezo1 expression and EGFR activation with ICP severity and adverse outcome. Together, these findings define a mechanotransduction axis in which astrocytic Piezo1 signaling integrates mechanical stress with EGFR-dependent neuroimmune responses, positioning EGFR as a translationally accessible target to modulate inflammation-driven secondary brain injury.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: neuroscience
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.05.02.651431v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.05.02.651431v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Zhao, Z., Hoffmann, A., Sun, F., Merz, T., Olde Heuvel, F., Oezkan, B., Muenz, F., Calzia, E., Groeger, M., Kress, S., Radermacher, P., Roselli, F., Kapapa, T., Pagliarini, M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.6000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Secretory carrier membrane proteins regulate water transport in Arabidopsis.</h2>
            <p class="paper-summary">Aquaporins belonging to the plasma membrane intrinsic protein (PIP) subfamily are channel proteins that control water flow across cells, allowing plants to rapidly adjust hydraulic conductivity and thereby sustain growth, gas exchange, and recovery from drought. We show that secretory carrier membrane proteins (SCAMP) regulate the abundance of the aquaporins and thereby water transport in Arabidopsis root cells. SCAMPs are evolutionarily conserved multi-spanning transmembrane proteins. In animal cells, they function in secretion, endocytosis and autophagy. Knowledge on their role in plants is restricted to localization and trafficking experiments in heterologous systems and few genetic perturbation experiments. Here, we analysed all five members of the Arabidopsis SCAMP family. We identified conserved tyrosine motifs assisting in transport to the plasma membrane and N-terminally located NPF motifs that are required for internalization. SCAMPs dimerize both at the plasma membrane and endosomes, and dimerization is required for their internalization. Functionally, several PIPs were identified as common targets of multiple SCAMP isoforms. Triple and quintuple scamp mutants show mild developmental delay under standard growth conditions, but they are more tolerant to drought. The drought tolerance phenotype cannot be explained by altered stomatal dynamics or density. However, scamp mutant root protoplasts display reduced water transport capacity, which correlates with a reduced PIP abundance compared to wild type. In conclusion, our research identifies the SCAMP membrane trafficking proteins as regulators of PIP abundance at the plasma membrane in root cells. We propose that the reduced PIP levels in the scamp mutants may act as a priming mechanism, leading to the observed drought tolerance phenotype.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: plant biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.07.03.662988v3.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.07.03.662988v3" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Jiang, Q., Hdedeh, O., Vandorpe, M., fox, a. R., Liu, H., Ding, L., Vermeersch, M., Mylle, E., Nolf, J., Cuadrado, A. F., Kraus, J., Eeckhout, D., Kocourkova, d., Podmanicka, T. K., Jacobs, T. B., Dragwidge, J. M., De Rybel, B., De Smet, I., Pleskot, R., Chaumont, F., Van Damme, D.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Microfluidic Platform for Automatic Quantification of Malaria Parasite Invasion Under Physiological Flow Conditions</h2>
            <p class="paper-summary">Understanding the impact of forces generated by blood flow on biological processes in the circulatory system, such as the invasion of human red blood cells by malaria parasites, is currently limited by the lack of experimental systems that integrate them. Recent systematic quantification of the growth of Plasmodium falciparum, the species that causes the majority of malaria mortality, under a range of shaking conditions has shown that parasite invasion of erythrocytes is affected by the shear stress to which the interacting P. falciparum merozoites and their target red blood cells are exposed. Blood flow could similarly impact shear stress and therefore invasion in vivo, but there is currently no method to test the impact of flow-induced forces on parasite invasion. We have developed a microfluidic device with four channels, each with dimensions similar to those of a post-capillary venule, but with different flow velocities. Highly synchronised P. falciparum parasites are injected into the device, and parasite egress and invasion rates are quantified using newly developed custom video analysis, which fully automates cell type identification and trajectory tracking. The device was tested with both wild-type P. falciparum lines and lines in which genes encoding proteins involved in parasite invasion had been deleted. Deletion of Erythrocyte Binding Antigen 175 (PfEBA175) has a significant impact on invasion under flow, but not in static culture. These findings establish for the first time that flow conditions can critically affect parasite invasion in a genotype-dependent manner. The method can be applied to other biological processes affected by fluid motion, such as cell adhesion, migration, and mechanotransduction.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.07.29.667357v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.07.29.667357v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Kals, E., Kals, M., Introini, V., Vodenicharski, B., Kotar, J., Rayner, J. C., Cicuta, P.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.7, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">EZH1-dependent H3K27me1 is an adaptive chromatin barrier that limits DNMT inhibitor response in colorectal cancer</h2>
            <p class="paper-summary">Abnormal DNA methylation patterning is a defining epigenetic hallmark of human cancer and is therapeutically targetable with DNA methyltransferase inhibitors (DNMTis). However, DNMTi-induced DNA hypomethylation promotes adaptive chromatin remodeling that limits molecular and therapeutic responses to these drugs. Here, we identify EZH1-dependent H3K27 mono-methylation (H3K27me1) as a previously unrecognized adaptive barrier to DNMTi response in colorectal cancer. While EZH2-selective inhibitors deplete H3K27me2 and H3K27me3, they preserve EZH1-dependent H3K27me1 at Polycomb-enriched genomic regions. In contrast, dual EZH1/2 inhibition eliminates all H3K27 methylation states and robustly synergizes with DNMTi to enhance transcriptional activation and growth suppression. Mechanistically, dual EZH1/2 inhibition induces a redistribution of p300/CBP-dependent H3K27 acetylation (H3K27ac), generating a therapy-associated bivalent chromatin state characterized by coexisting DNA methylation and H3K27ac. DNMT inhibition resolves this induced bivalency, enabling activation of tumor-suppressive transcriptional programs. At the same time, coordinated loss of H3K27me1 and gene-body DNA methylation, together with depletion of promoter-associated H3K27ac, suppresses MYC- and E2F-driven oncogenic transcription networks that define the cancer cell-intrinsic therapeutic response. Collectively, these findings establish EZH1-dependent H3K27me1 as a key mediator of adaptive epigenetic plasticity and provide mechanistic rationale for combining DNMT inhibitors with dual EZH1/2i inhibitors to reprogram chromatin and suppress oncogenic transcription in solid tumors.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: cancer biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.09.16.676613v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.09.16.676613v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Chomiak, A. A., Wiseman, A. K., Hrit, J. A., Liu, Y., Stransky, S., Cui, Y., Kong, X., Topper, M., Baylin, S., Sidoli, S., Tiedemann, R. L., Rothbart, S. B.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Bridging Histology and Tractography: First In-Vivo Visualization of Short-Range Prefrontal Connections Informed by Primate Tract-Tracing</h2>
            <p class="paper-summary">Decades of histological research in non-human primates have revealed a dense web of short-range connections underpinning prefrontal cortex (PFC) function. However, translating this anatomical ground-truth to the living human brain has been a major challenge, leaving our understanding of the PFC's intrinsic wiring incomplete. These short-range fibers are difficult to resolve with non-invasive methods like diffusion tractography, which are often hampered by false positives. Here, we provide the first systematic in-vivo visualization of these pathways in the human brain. By informing high-resolution probabilistic tractography with established tract-tracing findings, we mapped 91 histologically-defined short-range connections within and between five major PFC subdivisions in 1,003 individuals (547 F, 456 M). Our anatomically-informed approach successfully reconstructed these intricate connections with high precision (>80%) and accuracy (>70%) relative to histological findings. The resulting tracts not only captured broad organizational principles but also replicated fine-grained patterns previously only seen in invasive studies. Furthermore, these connections showed high test-retest reliability within individuals alongside significant variability between them, highlighting a stable yet unique anatomical fingerprint. Ultimately, this study shows how linking histology to tractography provides a powerful framework to advance our understanding of the human connectome and opens avenues to investigate local circuitry that underpins cognition and disease.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: neuroscience
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.10.22.683760v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.10.22.683760v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Amandola, M., Kim, M. E., Rheault, F., Landman, B. A., Schilling, K.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.800000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Extrinsic cues unlock cross-germ layer differentiation potential of CNS stem cells during regeneration</h2>
            <p class="paper-summary">During CNS regeneration, neuroepithelial-derived oligodendrocyte progenitor cells (OPCs) can cross germ-layer boundaries to generate neural-crest-derived Schwann cells (SCs). However, the underlying mechanism and disease relevance of this unique phenomenon of cellular plasticity is unclear. Here, we combine single cell genomics in rodent models of CNS injury and samples from multiple sclerosis patients to characterise OPC-derived SCs. We discover that integrin signalling and bone morphogenetic protein (BMP) activation activate a core SOX10/OLIG2 transcriptional circuit to drive OPC-to-SC differentiation. We show that OPC-derived SCs myelinate in vitro and in vivo, and unlike peripheral SCs, can integrate into astrocyte-rich territories. Together, these findings define a conserved molecular mechanism of adult plasticity, enabling the control of OPC fate choices beyond their germ layer origin for CNS repair.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: neuroscience
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.11.24.690179v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.1101/2025.11.24.690179v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Chen, C. Z., Yu, Y., Murphy, N., Cubillos, J. F., Rawji, K. S., Zhao, C., Hill, M., Arthur-Farraj, P., Franklin, R. J. M., Neumann, B.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.8500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">ProChoreo: de novo Binder Design from Conformational Ensembles with Generative Deep Learning</h2>
            <p class="paper-summary">Deep learning has transformed protein structure prediction and de novo protein design; however, most existing frameworks operate on a single static conformation and underutilize the conformational heterogeneity that governs protein binding and function. We introduce ProChoreo, a generalizable framework for de novo binder design that explicitly incorporates conformational ensembles. ProChoreo is pretrained with multimodal contrastive learning to align protein sequences with corresponding molecular dynamics (MD)-derived ensembles, producing a shared latent representation that captures both sequence-level and dynamic structural information. This representation is then integrated into an autoregressive generator to design protein binders conditioned on receptor sequences. Designed binders are evaluated using Boltz 1 for complex structure and interaction quality, followed by MD simulations of complexes with two representative receptors: the human sweet taste receptor TAS1R2 and FGFR2. ProChoreo designs binders that encode conformational features, highlighting dynamics-informed design as a route to protein design.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.01.23.701298v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.01.23.701298v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ding, S., Zhang, Y.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MOSAIC: A Spectral Framework for Integrative Phenotypic Characterization Using Population-Level Single-Cell Multi-Omics</h2>
            <p class="paper-summary">Population-scale single-cell multi-omics offers unprecedented opportunities to link molecular variation to human health and disease. However, existing methods for single-cell multi-omics analysis are either cell-centric, prioritizing batch-corrected cell embeddings that neglect feature relationships, or feature-centric, imposing global feature representations that overlook inter-sample heterogeneity. To address these limitations, we present MOSAIC, a spectral framework that learns a high-resolution feature $\times$ sample joint embedding from population-scale single-cell multi-omics data. For each individual, MOSAIC constructs a sample-specific coupling matrix capturing complete intra- and cross-modality feature interactions, then projects these into a shared latent space via spectral decomposition. The joint feature x sample embedding defines each feature's connectivity profile per sample, enabling three downstream applications. Differential Connectivity analysis identifies features with regulatory network rewiring across conditions even when their abundance remains unchanged, revealing rewiring of proliferation programs in activated T cells from a vaccination cohort. Unsupervised subgroup detection isolates coherent feature modules to discover hidden patient subtypes, uncovering a stress-driven neuronal subtype within an HIV+ cohort. Clinical outcome prediction using connectivity-derived features complements abundance-based analysis, improving COVID-19 severity classification when integrated. MOSAIC provides a general-purpose framework for systems-level phenotypic characterization, bridging network-level discovery with clinical outcome prediction in population-scale single-cell studies.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.10.705077v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.10.705077v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Lu, C., Kluger, Y., Ma, R.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 5.95, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">QuantiTrack: A unified software to study protein dynamics in living cells</h2>
            <p class="paper-summary">Linking the spatiotemporal dynamics of proteins in live cells to physiological functions is a fundamental challenge in biology and robust quantification of protein dynamics is a major step towards this endeavor. Single molecule tracking (SMT) has emerged as a powerful technique to investigate protein dynamics at the single molecule level in living cells. Most SMT analyses require familiarity with biophysical models and programming and the results from different analyses cannot be easily integrated. To mitigate these shortcomings, we developed QuantiTrack - a MATLAB-based SMT analysis software that can be operated from a simple graphical user interface. This provides a much-needed end-to-end solution where a user can load a movie, track single molecules, and perform a range of analyses. In addition to a detailed user guide with step-by-step instructions, QuantiTrack includes quality control metrics that can be used to systematically determine tracking parameters. As a practical example, we address by QuantiTrack a question relevant to hormonal therapy: How does the glucocorticoid receptor (GR), a hormone-regulated transcription factor (TF), respond to treatment and washout of its cognate hormone. Hormone washout results in rapid (in minutes) downregulation of GR target genes to basal levels. We observe dynamics of the Halo tagged GR (Halo-GR) and by integrating several analyses, show that hormone washout results in a substantially lower bound fraction of GR, reduced occupancy in the mobility state associated with GR activation, and shorter GR dwell times. These analyses showcase QuantiTrack as a convenient tool for comprehensive SMT analysis for a wide range of biologists.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.19.706877v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.19.706877v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ball, D. A., Wagh, K., Stavreva, D. A., Hoang, L., Schiltz, R. L., Chari, R., Raziuddin, R., Mazza, D., Upadhyaya, A., Hager, G. L., Karpova, T. S.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">A High-fat, High-salt Diet Model of MDAKD Impairs Bioenergetic Efficiency for ATP Synthesis</h2>
            <p class="paper-summary">Metabolic dysfunction-associated kidney disease (MDAKD) is closely linked to dietary excess, but models that capture early kidney injury without obesity are limited. We fed male C57BL/6J (6J) and C57BL/6N (6N) mice a high-fat, high-sodium (HF/HNa) or control diet for 16 weeks. HF/HNa feeding did not alter body weight, adiposity, or total food intake; however, it increased dietary energy and sodium exposure, kidney mass, water intake, and urine volume. GFR declined modestly in 6J mice, whereas 6N mice maintained or slightly increased GFR. Both substrains showed increased urinary albumin, creatinine, KIM-1, and NGAL, while cystatin C rose predominantly in 6N mice, indicating strain-dependent tubular injury. Whole-kidney trichrome staining revealed increased fibrotic area with HF/HNa, particularly in 6N mice, without significant changes in glomerular morphology. In isolated renal mitochondria, oxygen consumption was preserved, but ATP production and ATP:O ratios were reduced, with unchanged citrate synthase activity and OXPHOS protein abundance, consistent with early mitochondrial bioenergetic uncoupling. Exploratory urinary proteomics in 6J mice identified HF/HNa-associated changes in proteins linked to tubular stress and extracellular matrix remodeling. These findings define an early MDAKD-like renal phenotype with strain-specific functional responses, tubular injury, fibrosis, and impaired mitochondrial ATP efficiency.

Translational StatementMetabolic Dysfunction-Associated Kidney Disease (MDAKD) is a leading driver of chronic kidney disease (CKD) in the world. In addition to obesity and related comorbidities, renal mitochondrial dysfunction is thought to be a key contributor to the development of CKD in patients with MDAKD; however, few models recapitulate the progression of MDAKD. We couple well-established mouse models of obesity, namely the C57Bl/6J and C57Bl/6N mouse lines, with a high-fat, high-salt diet to induce renal mitochondrial dysfunction, leading to early stages of MDAKD as indicated by widespread fibrosis and mild reduction in glomerular filtration rate, though these effects were strain-dependent. We identify diet-induced mitochondrial dysfunction as a common feature in both mouse strains, suggesting impairments in mitochondrial respiration and oxidative ATP production are indeed a contributing factor to the development of MDAKD. This study highlights the role of energetic impairments in the pathogenesis of MDAKD and may guide future therapies for CKD.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: physiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.20.707069v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.20.707069v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Decker, S. T., Smith, Z. T., Opurum, P. C., Paula, V. L., Moses, K. N., Stuart, D., Kurian, A. S., Rout, S., Ramkumar, N., Funai, K.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.050000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Generalized Morphogenesis Theory: A Flow-Inertia Modeling Framework for Cross-Scale Dynamics of Dissipative Structures</h2>
            <p class="paper-summary">Understanding structural similarities across dynamical systems at different scales remains a central problem in nonlinear science [1, 3]. Here we propose a modeling framework for cross-scale morphogenetic dynamics, termed Generalized Morphogenesis Theory (GMT), based on a flow-inertia formulation: O_FD O_INLINEFIG[Formula 1]C_INLINEFIGM_FD(1)C_FD where S denotes system state, E environmental input, F (E, S) a driving function, and {micro}(S) an inertia function representing resistance to change.

This formulation provides a structural representation that encompasses several classical dynamical models--including Newtonian relaxation, logistic growth, and reaction-diffusion systems [13]--under appropriate parameterizations. Non-dimensionalization reveals a small set of control parameters governing regime transitions.

Empirical validation is performed across two independent scales. At the organism scale, crop growth time-series datasets from multiple species exhibit consistent multiplicative dynamics F (E, S) = f (E) {middle dot} S, statistically preferred over additive alternatives in 5 of 6 independently tested systems ({Delta}AIC ranging from +2 to +891; R2 up to 0.98). Independently estimated inertia time constants agree in two plant systems (cucumber:{tau} = 3.7 days, CV=3.3%; maize:{tau} = 36.8 days, CV=17.3%), with the 10-fold ratio consistent with structural complexity differences. At the molecular scale, publicly available perturbation transcriptomics datasets (Perturb-seq) show directional response structures consistent with the proposed flow-inertia decomposition (93% causal direction agreement across three independent datasets; p < 10-25).

Across domains, recurrent dynamical motifs are organized into 12 canonical design patterns associated with stability classes and bifurcation conditions. These results suggest that the flow-inertia formulation functions as a domain-independent structural modeling principle for dissipative morphogenesis.

Lead ParagraphDissipative structures--from growing organisms to differentiating cells--share a common dynamical tension between driving forces that push change and inertial resistance that maintains identity. We formalize this tension as [Formula]. While this ODE form is mathematically general, the empirical content lies in treating {micro}(S) not as a free parameter but as an operationally measurable quantity, and in demonstrating that the multiplicative coupling F (E, S) = f (E) S{middle dot} is statistically preferred over additive alternatives. Applying this single equation across two independent biological scales, we find quantitative structural consistency: two crop systems yield inertia time constants of 3.7 days (cucumber, CV=3.3%) and 36.8 days (maize, CV=17.3%), with the 10-fold ratio reflecting structural complexity differences, while gene perturbation datasets show 93% agreement in causal response directions across three independent experiments. These cross-scale results, together with 12 canonical design patterns organized by stability class, suggest that the flow-inertia decomposition captures a domain-independent structural principle of dissipative morphogenesis.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: systems biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.23.707312v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.23.707312v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Iwao, T., Kimura, Y., Iida, T.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.1000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Immunogenicity and protective efficacy of a Brucella abortus L7/L12 DNA vaccine delivered via chitosan modified PLGA nanoparticles in mice</h2>
            <p class="paper-summary">The present study evaluated the immunogenicity and protective efficacy of the chitosan (CS)-modified poly-lactide-co-glycolic acid (PLGA) nanoparticles (NPs) delivering Brucella abortus L7/L12 DNA in a mouse model. The NPs were prepared by solvent displacement method and characterized for size, charge, morphology, cellular uptake and cytotoxicity. The cationic CS-PLGA NPs were spherical with a mean size of ~165 nm with a positive zeta potential (+20 mV). DNA loading efficiency of 1.2% and DNA adsorption shifted zeta potential to -45 mV. In vitro studies in RAW 264.7 cell line demonstrated efficient uptake of the DNA loaded cationic NPs and expression of L7/L12 protein. Intramuscular immunization of the L7/L12 DNA vaccine loaded CS-PLGA NPs elicited both humoral and cell-mediated immunity with upregulation of Th1 and Th2 cytokines along with induction of IgG antibodies in mice. IFN-{gamma}, IL-2, and IL-4 levels were significantly (P < 0.001) higher than control group. The protective efficacy of the DNA loaded NPs against virulent B. abortus 544 infection (105 CFU) was significantly higher than that of the naked DNA (P<0.001). These findings suggest that the CS-PLGA NPs were efficiently delivered L7/L12 DNA and exhibited adjuvant potential, conferring protection against experimental murine brucellosis.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707861v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707861v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Chaudhari, U., Dandapat, S., Panickan, S., Kumar, V., Sawant, P. M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.15, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Optimal transport fate mapping resolves T cell differentiation dynamics across tissues</h2>
            <p class="paper-summary">Immune responses evolve across time and tissues through coordinated programs of proliferation, differentiation, and migration, yet most single-cell measurements capture only static molecular snapshots. As a result, reconstructing how immune cells transition between alternative fates remains challenging, particularly for CD8 T cells, whose differentiation is highly dynamic and shaped by rapid expansion, contraction, and tissue trafficking. Here, we introduce an optimal transport-based fate mapping framework that reconstructs continuous CD8 T cell trajectories across time and tissues. Applied to longitudinal single-cell RNA-seq data from CD8 T cells responding to acute viral infection in mice, this approach accurately recapitulates population dynamics and resolves coherent effector and memory T cell differentiation trajectories. Extending the model to multiple tissues, we identify and experimentally validate temporally distinct waves of migration into the small intestine that give rise to divergent tissue-resident memory (Trm) fates, long-lived T cells crucial in immunosurveillance. By integrating optimal transport inference with time-resolved in vivo labeling, we demonstrate that CD52 marks recent tissue entrants and distinguishes them from Trm precursors. Finally, trajectory-guided analysis of transcription factor regulons reveals both shared and context-specific gene regulatory programs and identifies AP4 as a key regulator of circulating versus tissue-resident specification. Together, these results establish optimal transport as a principled framework for reconstructing immune cell fate dynamics and provide a quantitative map of early events governing antiviral CD8 T cell differentiation across tissues.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: immunology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.24.707057v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.24.707057v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Plotkin, A. L., Mullins, G. N., Green, W. D., Shi, H., Chung, H. K., Yi, H., Stanley, N., Milner, J. J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Membrane Proteome Remodeling in Female APP Mice Following Muscarinic Acetylcholine Receptor M1 Modulation Revealed by Peptidisc Enabled DIA-MS.</h2>
            <p class="paper-summary">Alzheimer disease (AD) is linked to profound dysregulation of membrane-embedded and membrane-associated proteins that govern amyloid processing, synaptic signaling, and neuronal communication. Yet most proteomic analyses prioritize soluble fractions, resulting in systematic underrepresentation of integral membrane proteins and limited access to disease-relevant membrane pathways. Here, we use a membrane-mimetic, data-independent acquisition proteomic workflow to define disease- and drug-induced remodeling of the cortical membrane proteome in an APP mouse model of Alzheimer disease. Female B6C3F1/J mice were aged to 9 months and treated for 8 weeks with or without the M1 muscarinic acetylcholine receptor positive allosteric modulator VU0486846. APP pathology drove a pronounced, genotype-specific remodeling of the membrane proteome, with enrichment of multiple membrane proteins linked to AD, including RyR2, PLD3, ITM2C, and CNTNAP2. Wild-type mice cortical membranes were instead enriched for membrane proteins involved in axon guidance and synaptic organization, such as EPHA5 and ROBO2. In contrast, activation of M1 using the VU0486846 produced minimal membrane proteome changes in wild-type mice but selectively enriched proteins involved in neuronal trafficking and synaptic plasticity in APP mice, including SORCS2, PLXND1, and CADM1. Together, these findings demonstrate that AD-associated proteomic remodeling is strongly concentrated at the membrane level and that M1 receptor activation preferentially engages disease-altered membrane networks rather than inducing widespread proteomic changes. This work establishes peptidisc-enabled membrane proteomics as a powerful approach for identifying membrane-associated biomarkers and evaluating therapeutic target engagement in AD.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biochemistry
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.24.707828v2.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.24.707828v2" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Bhattacharya, A., Antony, F., Aoki, H., Babu, M., Ferguson, S., Abd-Elrahman, K., Duong van Hoa, F.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">The molecular mechanism and activity of Kuenenia stuttgartiensis hydrazine synthase</h2>
            <p class="paper-summary">Anaerobic ammonium-oxidizing (anammox) bacteria convert ammonium and nitrite into dinitrogen gas via the intermediates nitric oxide and hydrazine. To produce hydrazine, anammox bacteria harbor a biochemically unique enzyme: hydrazine synthase. Based on the hydrazine synthase crystal structure it was hypothesized that hydrazine is produced in a two-step mechanism. In this hypothesis, nitric oxide is first reduced to hydroxylamine (first half-reaction), followed by condensation of hydroxylamine with ammonium to hydrazine (second half-reaction). Here, we experimentally investigated the proposed molecular mechanism of hydrazine synthase and characterized and optimized the in vitro activity. First, we optimized the activity of isolated hydrazine synthase from anammox bacterium Kuenenia stuttgartiensis strain MBR1 via an anaerobic isolation method. We further compared hydrazine synthase activity measured via a coupled assay versus that of a newly established direct LC-MS assay. Next, the hypothesized second half-reaction was investigated via the direct LC-MS assay, quantifying biologically produced hydrazine from hydroxylamine and ammonium. Despite variation in hydrazine synthase activity across assays, we determined ammonium and hydroxylamine affinity and investigated product inhibition. Finally, we found that hydrazine synthesis from ammonium and hydroxylamine by isolated hydrazine synthase is oxygen-tolerant, strongly suggesting that the second half-reaction is initiated on an oxidized heme within hydrazine synthase. Taken together, the results corroborate that condensation of ammonium with hydroxylamine to form hydrazine is the second half-reaction of the proposed two-step mechanism for hydrazine synthesis by hydrazine synthase.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707902v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707902v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Vermeir, F. J., Haaijer-Vroomen, S. C. M., van der Velden, P. M. M., Mesman, R., Jansen, R. S., Versantvoort, W., van Niftrik, L.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.300000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Determinants of chromosomal rearrangements in holocentric Leptidea butterflies</h2>
            <p class="paper-summary">Chromosomes can undergo large-scale rearrangements such as fissions and fusions. Occasional rearrangements can be common, especially in organisms with holocentric chromosomes such as butterflies. However, high rates of fissions and fusions have only been observed in a few taxonomic groups. One such group is the Palearctic Leptidea butterflies, where fissions and fusions have resulted in considerable inter- and intraspecific variation in chromosome numbers. The large number of chromosome rearrangements in Leptidea , provides a rare opportunity to study the mutational determinants of chromosome rearrangements within a statistical framework. Using nine chromosome-level genome assemblies and 138 whole-genome re-sequenced individuals, we mapped evolutionary breakpoint regions and quantified the association between annotation features and rearrangements. Evolutionary breakpoint regions were significantly depleted in protein-coding genes and the majority resided in repetitive regions. However, rearrangements were only weakly associated with transposable elements. Instead, the strongest sequence predictors were large clusters of satellite DNA, ribosomal DNA and segmental duplications, with differing patterns among rearrangement types. Copy-number variation was observed in evolutionary breakpoint regions and lineages dominated by fissions or fusions were associated respectively with genome-expansion and -reduction. The results give novel insights into the mechanistic basis of interchromosomal rearrangements.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: evolutionary biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708211v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708211v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Thorn, F., Claret-Imbert, J.-L., Backstrom, N., Boman, J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.3500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Spatial Mechanomics for Tissue-Scale Biomechanical Mapping and Multi-omics Integration</h2>
            <p class="paper-summary">Tissue mechanical properties are spatially heterogeneous and tightly coupled to cellular function, developmental patterning, and disease progression, yet spatially resolved characterization of viscoelastic and microrheological behavior across intact tissues remains limited. Here we introduce spatial mechanomics, a framework for tissue-wide acquisition, quantitative extraction, and computational representation of location-resolved mechanical states. Using BioAFM-based spatial sampling with multi-protocol microrheology, we acquire force responses at defined tissue coordinates and fit physically interpretable viscoelastic models to extract elastic, viscous, and frequency-dependent parameters at each position. These parameters are assembled into per-niche mechanomic feature vectors and reconstructed into tissue-scale mechanomic atlases that resolve heterogeneous mechanical organization. We implement these capabilities in MechScape, an open-source computational platform that supports force curve fitting, spatial feature matrix construction, unsupervised domain discovery, and cross-modal alignment with histological and molecular measurements. Application to murine myocardial tissue reveals that spatial mechanomics identifies distinct mechanical states, quantifies condition-dependent remodeling across all measured parameters, and resolves spatially coherent mechanical domains. This work establishes spatial mechanomics as a quantitative approach for tissue-scale biomechanical mapping and provides a generalizable framework for integrating mechanics as an omics layer in multi-modal tissue analysis.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.703280v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.703280v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xie, W., Wang, Z., Shan, Q., Zhao, Q., Ye, X.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Topological Data Analysis of Spatial Protein Expression in Multiplexed Spatial Proteomics Studies</h2>
            <p class="paper-summary">Multiplexed spatial proteomics platforms generate high-resolution images capturing the spatial expression of proteins in tissue. Images are often fed through a complex pre-processing pipeline to identify individual cells (termed segmentation) and then to predict their phenotypes. It is common to test if the inferred spatial arrangement of cells associates with patient-level outcomes. However, cell segmentation and phenotyping are prone to error and this approach neglects the measured protein levels. Further, new research suggests topological analysis of spatial proteomics may yield more power than alternative approaches. We propose a method, TOASTER, that circumvents reliance on segmentation and phenotyping and instead tests the association between continuous spatial protein expression and a patient-level response variable. TOASTER uses topological data analysis to first characterize the presence of topological features within univariate and bivariate spatial protein expression. The topological structure is summarized using an adaptation of the Nelson-Aalen cumulative hazard function. We can then associate this summary with an outcome using either a functional data analytic approach, a gridwise testing approach, or using kernel association testing. We show via simulation that our approach improves power and controls type I error, even in the presence of gaps or tears in the image which may arise during tissue handling. We apply our approach to a study in triple-negative breast cancer and demonstrate topological features of protein expression associated with immunotherapy response.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707521v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707521v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Samorodnitsky, S. N., Wu, M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Anti-diabetic drug Repaglinide induces Apoptosis, Cell Cycle Arrest, and Inhibits Cell Migration in Human Breast and Lung Cancer Cells.</h2>
            <p class="paper-summary">Introduction: Drug repurposing offers a cost-effective and time-efficient strategy for cancer therapy by leveraging existing drugs with established safety profiles, thus functioning as an alternative therapeutic strategy in demanding diseases such as cancer. Antidiabetic agents, in particular, have demonstrated encouraging anticancer potential. Among them, the non-sulfonylurea insulin secretagogue repaglinide (RPG) has shown emerging anticancer potential, yet its effects on breast and lung cancers remain largely unexplored. Thus, this study investigates the anticancer activity of repaglinide in human breast (MCF-7) and lung (A549) cancer cell lines, focusing on its cytotoxic, pro-apoptotic, anti-proliferative, and anti-migratory effects and the underlying possible molecular mechanisms. Methodology and Results: MTT cytotoxic assay revealed that RPG reduced cell viability in a dose-/time-dependent manner, with an IC (48h) of 100.8Micromolar for MCF-7 and 104Micromolar for A549. Further, the apoptotic effect of RPG on both cell lines was evidenced by double staining assays, comet assay, and western blotting analysis, suggesting that RPG explicitly caused DNA damage and activated intrinsic and extrinsic apoptosis pathways. Additionally, RPG suppressed clonogenicity and enforced G1 arrest in MCF7 and A549 cells by modulating cell cycle regulations as well as cell proliferation pathways. Moreover, RPG markedly suppressed cell motility, as demonstrated by scratch and Transwell migration/invasion assays, which is correlated with reduced MMP-2 and MMP-9 expression, confirmed by gelatin zymography and western blotting. Conclusion: Conclusively, Repaglinide exerts potent anticancer effects in breast and lung cancer cells by modulating key oncogenic signaling pathways, and thus can be considered a promising candidate for repurposing in cancer therapy.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: cancer biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707939v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707939v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: P K, H., K, A., Yarla, N. s., Duddukuri, G. r.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Genetic or pharmacological disruption of the MSH3 Y245/K246 IDL binding pocket slows CAG repeat expansion</h2>
            <p class="paper-summary">Recent genetic studies have shown somatic expansion of the CAG repeat is the key process driving Huntingtons disease (HD) pathogenesis. Recognition of insertion deletion loops (IDLs), lesions prone to form within the CAG repeat, by MutSbeta (MSH3/MSH2) is thought to be the primary event in the expansion process. This starts a cascade that leads to error prone repair and incorporation of additional CAG units into the repeat. In vitro data shows MSH3 binds IDLs through a DNA binding pocket formed by MSH3 residues Y245/K246. In this study, we investigated the significance of this DNA binding motif in CAG repeat expansion using cell lines harboring long, unstable HTT CAG repeats. Genetic disruption of the MSH3 Y245/K246 motif significantly reduced DNA interaction, exhibited MMR deficiency in a frameshift mutator assay and abrogated repeat expansion in a U2OS cell line expressing mutant HTT exon 1. Pharmacological blockade of this site using a small molecule targeting the DNA binding pocket similarly reduced DNA binding and repeat expansion in a U2OS cell line. Crucially, this molecule also slowed CAG repeat expansion in medium spiny neurons derived from HD patient-iPSCs. Targeting of the MSH3 IDL binding pocket may represent a possible therapeutic strategy.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: neuroscience
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.707948v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.707948v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Goold, R., Donaldson, J., Gidney, F., Goff, P., Hamilton, J., Coupland, L., Elmasri, M., Flower, M., Tabrizi, S. J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.550000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MPNN-guided redesign of PET hydrolases with enhanced catalytic activity below the PET glass transition temperature</h2>
            <p class="paper-summary">The enzymatic depolymerization of polyethylene terephthalate (PET) presents a sustainable route for plastic circularity, but its industrial viability is disadvantaged by the need for thermostable enzymes that remain active under mild, energy-efficient conditions. While the Polyester Hydrolase Leipzig 7 (PHL7) rapidly degrades amorphous PET near its melting point, its poor protein expression, inactivation issues at temperatures above 60{degrees}C and slow depolymerization activity below 60{degrees}C limit its practical application. Here, we employ inverse folding models ProteinMPNN and LigandMPNN, informed by structural and evolutionary information, to redesign the sequence of PHL7, aiming to improve protein expression, thermal stability and activity. From 36 designed variants, we identified two (termed D5 and D11) with significantly enhanced PET depolymerization rates at lower temperatures, where enzymatic performance is typically limited. Remarkably, design D5 at 50{degrees}C achieved the same product yield as PHL7 at 70{degrees}C in 24 h PET microparticle degradation assays, with a shifted product profile favoring mono-(2-hydroxyethyl) terephthalate (MHET) over terephthalic acid (TPA). Molecular dynamics simulations revealed that the active redesigns exhibit enhanced local flexibility in key active site regions at 50{degrees}C, providing a mechanistic understanding of their low-temperature catalysis. This work demonstrates that computational sequence redesign can optimize biocatalysts for lower production costs and milder operational conditions. Furthermore, the D5 variant enables a potential route to resynthesize virgin PET via MHET polycondensation, offering an efficient circular economy pathway.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioengineering
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708052v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708052v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Grinen, A., Eltit, V., Duran-Osorio, F., Aviles, J., Zacconi, F. C., Carcamo Noriega, E., Bahl, C. D., Meinen, B. A., Ramirez-Sarmiento, C. A.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.6000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Time-Resolved Single-Molecule FRET Reveals Length-Dependent Nucleosome Decompaction by Poly(ADP-ribose)</h2>
            <p class="paper-summary">Highly charged chains of poly(ADP-ribose) (PAR) are synthesized in the cell as part of their central role in DNA damage response. However, the effects of PAR on nucleosome structure and dynamics remain incompletely understood. Here we combine droplet-based microfluidic mixing with single-molecule Forster resonance energy transfer spectroscopy to resolve the kinetics of PAR-induced nucleosome decompaction in non-equilibrium measurements with millisecond time resolution. This approach avoids surface-adhesion and enables the tether-free observation of nucleosome remodeling. We find that PAR triggers nucleosome decompaction via a length-dependent kinetic threshold: Chains with less than ten ADP-ribose units act slowly and weakly, whereas longer PAR polymers induce efficient and rapid nucleosome opening. The extent and reversibility of decompaction further depend on PAR concentration and ionic strength, reflecting a mechanism dominated by electrostatic interactions. Enzymatic PAR digestion demonstrates that PAR can promote both reversible linker DNA opening and irreversible nucleosome disassembly. Coarse-grained molecular simulations suggest that these effects arise from a competition between PAR and DNA for histone tail binding. Altogether, our results establish PAR length as a key factor controlling chromatin accessibility during DNA repair and highlight droplet-based microfluidics as a powerful platform for studying such biomolecular interactions.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707951v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707951v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yang, T., Gopi, S. R., Pinet, L., Simoni, S., Imhof, R., Nettels, D., Altmeyer, M., Best, R. B., Schuler, B.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Neuronal ketone body utilization couples exercise and time-restricted feeding to cognitive enhancement</h2>
            <p class="paper-summary">Ketogenesis and ketone body metabolism are linked to brain health benefits, including delaying age-related cognitive decline and neurodegeneration. Exercise, particularly when combined with an overnight fast, stimulates ketogenesis and ketone body turnover as well as improves brain metabolism and cognition. Yet, whether ketone metabolism is obligatory for this response is unknown. Here, we use chronic exercise via voluntary wheel running plus time-restricted feeding (VWR+TRF, fasting from ZT10.5-18.5) to explore whether ketone bodies are a potential mediator of exercise-induced brain health benefits in middle-aged mice. To independently distinguish the roles of neuronal ketone body metabolism vs. hepatic ketone body production, we studied middle-age female neuronal-specific SCOT knockout mice and female hepatocyte-specific HMGCS2 knockout mice, respectively. VWR+TRF was compared to sedentary ad-libitum fed (SED+AL) mice to assess the impact on whole-body metabolism (indirect calorimetry), cognition (Barnes Maze and Y-Maze), and molecular adaptations in the hippocampus (proteomics). VWR+TRF robustly upregulated systemic lipid oxidation in all mice, regardless of genotype, during the first 6.5 hours of the dark period. In female SCOT-Neuron-KO mice, we show impaired responses to VWR+TRF in indices of short- and long-term memory. Proteomic analysis of isolated hippocampi revealed that SCOT-Neuron-KO mice failed to globally upregulate key facilitators of synaptic function, including leucine-rich repeated transmembrane proteins, neurexins, and neuroligins. In female HMGCS2-Liver-KO mice, impaired responses to VWR+TRF in indices of short-term memory were paired with an upregulation in ketogenesis machinery in the hippocampal proteome, suggesting potential in vivo evidence of cerebral ketogenesis, a mechanism mitigating an otherwise more pronounced behavioral phenotype. Together, these findings suggest that neuronal ketone body utilization is essential, and hepatic ketone production is contributory, to the full cognitive and synaptic adaptations to exercise plus time-restricted feeding, supporting ketone metabolism as a key mechanistic link between metabolic state and brain health in midlife.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: physiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708044v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708044v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Salathe, S. F., Kugler, B. A., Franczak, E., Davis, X. C., Boakye, F. B., Allen, J., Fulghum, K. L., Queathem, E. D., Morris, E. M., Puchalska, P., Crawford, P. A., Thyfault, J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.7, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Epigenomic profiling of cerebrospinal fluid cells identifies immune regulatory alterations and implicates protocadherins in multiple sclerosis</h2>
            <p class="paper-summary">Multiple sclerosis (MS) is a chronic inflammatory disease of the central nervous system (CNS), where DNA methylation may play a role by connecting genetic and environmental risk factors. We performed whole-genome DNA methylation profiling of cerebrospinal fluid (CSF) cells from relapsing-remitting MS patients and matched controls, identifying 2,710 differentially methylated positions (DMPs) and 4,330 regions (DMRs). These changes were enriched in immune signaling, adhesion and migration processes, and were accompanied by corresponding RNA expression changes. MS-associated methylation changes enriched in the cohesin chromatin regulation pathway mapped to enhancers of T helper 17 (Th17) cells, whereas in other T cell types they were mapping to bivalent enhancers and repressed chromatin. Notably, this pathway comprised multiple Protocadherin (PCDH) genes, typically expressed in neuronal cells, that displayed consistent methylation and expression changes in CSF cells. Expression of shared intracellular domain of PCDH{gamma} cluster proteins was confirmed in peripheral blood T cells by flow cytometry as well as expression of PCDH{gamma} cluster genes in memory CD4+ T cell subsets. Moreover, co-expression analysis suggests a role of PCDH genes in aryl hydrocarbon receptor (AHR) signaling. In summary, DNA methylation changes in CSF resident cells reflect dysregulated T cell activation and migration in MS and suggest a novel role of protocadherin molecules in MS pathogenesis.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: immunology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708054v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708054v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Han, Y., Zheleznyakova, G. Y., Sorini, C., Pahlevan Kakhki, M., Ruffin, N., Liang, H., Hallen, N., Rao Prakash, C., Beckers, V., Ivanova, E., Khademi, M., Karlsson, M. C. I., Piehl, F., Olsson, T., Kelsey, G., Kular, L., Needhamsen, M., Jagodic, M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">The ChIP-FRiP pipeline quantifies co-binding and reveals how antibody background contributes to cohesin ChIP-seq patterns</h2>
            <p class="paper-summary">Quantitative interpretation of ChIP-seq data is instrumental to derive insight into chromatin and transcription factor biology. Here we developed ChIP-FRiP, an end-to-end pipeline enabling systematic comparison of pairwise protein positioning, and applied it to the study of cohesin. In mammalian interphase, loop extruding cohesin complexes are positioned by CTCF barriers to generate locus-specific 3D genome folding patterns. Many aspects of our understanding of cohesin loop extrusion come from interpreting the amount of cohesin ChIP-seq signal at CTCF barriers, which has been reported to change variably after perturbing cohesin co-factors, such as NIPBL, PDS5A/B, and WAPL. Using ChIP-FRiP to homogeneously process 140 cohesin ChIP-seq datasets from 13 publicly available studies, we observed substantial variation attributable to technical effects, obscuring biological interpretability. To better understand how technical considerations, such as antibody specificity, influence apparent cohesin binding patterns, we integrated technical aspects of ChIP-seq into biophysical simulations of loop extrusion. Leveraging a simple biochemical model for background ChIP-seq signal, we derived a strategy to estimate and correct for the background using paired spike-in ChIP-seq data from wild-type and depletion conditions. Our results establish a framework for reliable comparative analysis, demonstrating that accurate background correction is requisite for interpreting the roles of cohesin cofactors in cohesin positioning.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: genomics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708306v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708306v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xiao, Y., Anderson, E. C., Rahmaninejad, H., Nora, E. P., Fudenberg, G.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.800000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Allosteric Inhibition of NDM-1 by Thanatin Preserves the Di-Zinc Center While Restricting Dynamics</h2>
            <p class="paper-summary">The New Delhi metallo- {beta} -lactamase 1 (NDM1) is a major driver of carbapenem resistance in Gram- negative pathogens, yet the molecular basis by which antimicrobial peptides inhibit this enzyme has remained unresolved. Thanatin, a disulfide- stabilized {beta} -hairpin peptide, was previously proposed to inactivate NDM1 by displacing catalytic Zn-ions, but this model lacked direct structural support. Here, we combine high-resolution NMR spectroscopy, intermolecular NOE mapping, HADDOCK-guided docking, and molecular dynamics simulations to reveal a distinct zinc-retaining dynamic allosteric mechanism. Thanatin binds adjacent to the catalytic groove, preserving the native di-zinc coordination environment while simultaneously rigidifying the L3 catalytic loop, as confirmed by Zn-bound spectral fingerprints and EDTA titration experiments. This conformational restriction explains how the peptide inhibits the enzyme while maintaining a zinc-bound but catalytically compromised state, a finding that contrasts with zinc-displacement hypotheses and reconciles prior biochemical observations with structural data. In bacterial assays, this allosteric inhibition translates to a moderate restoration of carbapenem sensitivity, resulting in a 50% reduction in viable cell output even under high-level enzyme expression. Our findings establish a mechanistic framework for designing next-generation peptide inhibitors that target the dynamic vulnerabilities of metallo- {beta} -lactamase.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707299v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707299v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Riviere, G., Kumar, P., Cummins, T., Hsiao, A., Mueller, L. J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.8500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Uncertainty-aware synthetic lethality prediction with pretrained foundation models</h2>
            <p class="paper-summary">Synthetic lethality (SL) offers a promising paradigm for targeted cancer therapy, yet experimental identification of SL gene pairs remains costly, context-dependent, and biased toward well-studied genes. Existing computational approaches often rely on curated protein-protein interaction (PPI) networks and Gene Ontology (GO) annotations, which limit their ability to generalize to novel genes. Here we introduce CILANTRO-SL, a two-stage, graph-free framework that leverages pretrained biological foundation models to predict SL pairs with calibrated uncertainty. In Stage 1, we apply a pretrained single-cell foundation model to bulk RNA-seq profiles of cancer cell lines to obtain context-aware embeddings and perform in silico gene knockouts to generate delta embeddings. These perturbation signals are further conditioned on a data-driven gene prior and supervised with CRISPR viability readouts to learn knockout-aware viability embeddings. In Stage 2, we derive pairwise features from these embeddings and train a lightweight classifier to distinguish SL from non-SL pairs. To enable reliable experimental prioritization, CILANTRO-SL incorporates conformal prediction, producing calibrated and interpretable prediction sets that highlight high-confidence SL candidates. Across two evaluation settings, including zero-shot generalization to unseen gene pairs and to unseen genes, ablation analyses show that viability pretraining and the gene prior substantially improve performance while avoiding reliance on PPI and GO features. CILANTRO-SL therefore transforms pretrained biological representations into practical, uncertainty-aware hypotheses that support robust and scalable discovery of therapeutic targets.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708096v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708096v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Hua, K., Haber, E., Ma, J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MAP: A Knowledge-driven Framework for Predicting Single-cell Responses for Unprofiled Drugs</h2>
            <p class="paper-summary">Predicting how cells respond to chemical perturbations is one of the goals for building virtual cells, yet experimentally profiled compounds cover only a small fraction of this space. Existing models struggle to generalize to unprofiled compounds, as they typically treat drugs as isolated identifiers without encoding their mechanistic relationships. We present MAP, a framework that integrates structured biological knowledge into cellular perturbation modeling and supports zero-shot prediction for small molecules with scarce or absent perturbation profiles. Specifically: (i) we construct MAP-KG, a large-scale knowledge graph tailored for cellular perturbation modeling that unifies 14 public resources, spanning 187k drugs, 23k genes, and 694k mechanistic relationships; (ii) we propose a knowledge-driven pre-training strategy that aligns molecular structures, protein sequence features, and textual mechanistic descriptions into a unified embedding space via contrastive learning, producing mechanism-aware and transferable gene and compound embeddings. The resulting knowledge-informed gene and drug representations are then coupled with a pretrained single-cell foundation model to condition perturbation response prediction; (iii) we evaluate MAP under two zero-shot generalization regimes: unseen cell type-drug combinations and the stricter setting of unprofiled drugs, where it improves top-50 DEG Pearson delta correlation by up to +13.3% and +12.2%, respectively, over the strongest baselines across three benchmarks. We further perform pathway-level functional analysis via GSEA for in-silico screening, where MAP predicts coherent, mechanism-consistent programs on unprofiled candidate drugs, and prioritizes 4 of 5 approved anti-cancer drugs in A-549 (non-small cell lung cancer).</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708091v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708091v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Feng, J., Zhao, Z., Zhang, X., Liu, M., Chen, J., Quan, X., Zhang, J., Wang, Y., Zhang, Y., Xie, W.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 6.95, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Graph Lens Lite: An interactive biological network viewer for displaying, exploring, and sharing disease pathobiology and drug mechanism of action models</h2>
            <p class="paper-summary">Motivation: Biological network visualization together with graph-based analyses are key techniques in systems biology and network medicine to detect patterns and generate new hypotheses regarding disease pathobiology, drug target identification, biomarker prioritization, or digital drug discovery. Network representations are also a way to communicate research findings and share results with colleagues and coworkers. Results: We have developed Graph Lens Lite, a browser-based tool that combines rich visualization capabilities with a streamlined interface for exploring and sharing biological networks. It offers an expressive query language, topological network analysis, GUI-based filtering, visual grouping, customizable layouts, a data-editor, and fine-grained property-based styling options, particularly suited for visualizing molecular models of disease pathobiology or drug mechanism of action. Availability: Graph Lens Lite is available at GitHub (https://github.com/Delta4AI/GraphLensLite).</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708026v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708026v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ley, M., Keska-Izworska, K., Fillinger, L., Walter, S. M., Baumgartel, F., Bono, E., Galou, L., Andorfer, P., Hauser, P., Leierer, J., kratochwill, k., Perco, P.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Integrating Segmental Deuteration iCM-SANS with SAXS and MD for Dynamical Analysis of Multi-domain Proteins</h2>
            <p class="paper-summary">Abstract Multi-domain proteins (MDPs) adopt diverse conformations arising from cooperative inter-domain motions, and such dynamics are intimately coupled to their biological functions. Quantitative characterization of these motions is crucial for elucidating their functional mechanisms. Although small-angle X-ray scattering (SAXS) provides information on overall domain arrangement, the limited experimental constraints hinder reliable discrimination of conformational ensembles derived from molecular dynamics (MD) simulations. To address this limitation, complementary experimental constraints that enable to observe domain-selective structural information are required. Inverse contrast-matching small-angle neutron scattering (iCM-SANS), combined with segmental deuteration, enables selective visualization of individual domains and thus provides such complementary information. However, practical strategies for preparing segmentally deuterated MDPs with arbitrary domain labelling have yet to be established. Here, we develop an experimental protocol that integrates controlled protein deuteration with high-efficiency multi-step protein ligation to generate a segmentally deuterated MDP in high yield. The combined use of SAXS and iCM-SANS yields complementary structural constraints that enhance discrimination of MD-derived conformational ensembles. This protocol expands the applicability of segment-selective visualization and also provides an opportunity for high-precision analysis of dynamics in complex MDPs.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708105v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708105v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Okuda, A., Inoue, R., Kurokawa, M., Martel, A., Porcar, L., Osaki, R., Fukuzawa, K., Weiss, K. L., Pingali, S. V., Urade, R., Sugiyama, M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.050000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">AtPUP5 functions as a plasma membrane flavin transporter regulating localized riboflavin distribution in Arabidopsis</h2>
            <p class="paper-summary">Riboflavin (vitamin B2; RF) and its derivatives flavin mononucleotide (FMN) and flavin adenine dinucleotide (FAD) are indispensable cofactors for redox reactions in plants. While higher plants possess a conserved pathway for de novo riboflavin biosynthesis, how flavins are transported and spatially distributed between tissues remains unresolved. In particular, no genetically defined plasma membrane-localized flavin transporter has been identified in plants. Here, we identify Arabidopsis PURINE PERMEASE 5 (AtPUP5) as the first genetically defined plasma membrane-localized flavin transporter in plants. Using a riboflavin-auxotrophic yeast mutant, we show that AtPUP5 enhances cellular uptake of RF and, to a lesser extent, FMN, whereas FAD uptake is inefficient. In planta, AtPUP5 overexpression increases RF accumulation following external application. In contrast, loss-of-function mutants do not display defects in bulk RF uptake at the whole-plant level, indicating that AtPUP5 is not essential for global riboflavin acquisition. Notably, AtPUP5 deficiency results in RF overaccumulation in reproductive organs, including inflorescences, siliques, and seeds, irrespective of external RF supply. This organ-specific phenotype is fully suppressed by genetic complementation and coincides spatially with strong AtPUP5 promoter activity in reproductive tissues. These findings demonstrate that AtPUP5 functions at the plasma membrane to regulate localized riboflavin distribution in reproductive tissues. Together, our study establishes the first molecular framework for plasma membrane-mediated flavin distribution in plants and positions spatial regulation as a central component of riboflavin homeostasis in plants.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: plant biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708111v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708111v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Shibata, R., Kuwata, H., Sugimoto, T., Kikuchi, M., Maruta, T., Ishikawa, T., Ogawa, T.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.1000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Structural and Metabolic Characterization of Ni(I)-inhibitors Provide a Robust Anti-Methanogenicity Scoring System</h2>
            <p class="paper-summary">Atmospheric methane (CH4) acts as a key contributor to global warming and a short-lived climate forcer. CH4 mitigation represents the most promising means to address short-term climate change. Ruminant enteric CH4 produced by methanogenic archaea represents 27.2% of global CH4 emissions. Only a few of the direct methanogenesis inhibitors identified bear high mitigation potential hence it is important to investigate their underlying modes of action. Here, we elucidated biophysical and thermodynamic interplay between known inhibitors and cofactor F430, to determine their stoichiometric ratios and binding affinities. We leverage this prior in a robust contrastive learning approach to functionally cluster known sixteen inhibitors and 53,959 bovine-linked metabolites. We demonstrate a multi-factor optimization protocol to identify putative inhibitors with: (i) high bacterial membrane permeability, (ii) no adverse effect to ruminal fermentation, (iii) known degradation pathway, and (iv) direct commercial availability. Subsequent in vitro assays and community metabolic modeling with a first set of eight treatment molecules revealed structo-metabolic priors that tie thermodynamic signatures of inhibition to metabolic flux shifts. We established a multi-scale workflow that transforms ostensibly negative compounds into mechanistic insight, linking rumen metabolic flux shifts to MCR--F430-Ni(I) inhibition chemistry as a foundation for rational methane-mitigation design</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: systems biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708075v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708075v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Aryee, R., Zargar, M. R., SR, V., B, A., Dey, S., Mohammed, N. S., Sanjeevan, K. A., Frazier, N. A., Koziel, J. A., Beck, M., Mansell, T. J., Chowdhury, R.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.15, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Analysis of stress-induced surfaceome remodeling reveals surface accumulation of the cation-independent mannose-6-phosphate receptor (CI-M6PR)</h2>
            <p class="paper-summary">To ensure their survival in the face of stressors, cells have evolved stress response programs. While several transcriptional stress responses have been elucidated, little is known about the impact of stressors on membrane transport and the protein composition of the cell surface. Yet, the dynamic remodeling of the surfaceome by processes such as endocytosis is likely central for the adaptation to stress as it shapes cellular responses by influencing ion uptake and numerous signaling cascades. Indeed, we show that different stressors decrease endocytosis, thereby facilitating cellular adaptation. Using quantitative mass spectrometry, we delineate stress-specific surfaceome alterations in response to osmotic, oxidative and heat stress. Among other adaptive changes, we uncover that osmotic stress leads to a striking surface accumulation of the cation-independent mannose-6-phosphate receptor (CI-M6PR). Mechanistically, we demonstrate that osmotic stress decreases the endocytosis of CI-M6PR while upregulating its lysosomal exocytosis. These results suggest that CI-M6PR might play an important role in the cellular resilience against osmotic stress.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: cell biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708128v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708128v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Mazzone, F. R., Graessle, G., Storchova, Z., Raeschle, M., Maritzen, T.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Hypanus brevis: a newly resurrected Eastern South Pacific stingray lineage revealed by integrative taxonomy</h2>
            <p class="paper-summary">Hypanus brevis (Garman, 1880) and H. dipterurus (Jordan & Gilbert, 1880) are currently considered as a conspecific lineage of the "diamond stingray" from the Eastern Pacific. This taxonomic group has been the subject of nomenclatural disputes for about 145 years. To clarify the historical confusion surrounding this lineage, we employed an integrative taxonomic approach using specimens from the Eastern North and Eastern South Pacific (ENP and ESP). The genetic results, based on single and multilocus mitochondrial analyses, revealed a distinct evolutionary unit in the ESP. While morphological analyses detected subtle differences between ENP and ESP specimens, most characters exhibited significant overlap (e.g., disc shape, dentition patterns, body coloration), suggesting low evolutionary divergence. A calibrated molecular clock analysis estimated this divergence at approximately 3.09 Ma. In accordance with Garman's (1880) original description based on specimens from Paita (northern Peru), we formally resurrect H. brevis from synonymy with H. dipterurus. Our findings suggest an anti-tropical speciation pathway, with core populations of H. brevis and H. dipterurus restricted to the temperate waters of the ESP and ENP, respectively. Notably, a single, fixed COI haplotype was detected in all H. brevis specimens from the north-central Peruvian coast. This result may indicate a severe bottleneck event, raising concerns about the genetic health and long-term viability of this vulnerable species. Finally, we analyzed historical fishery data of H. brevis to infer its current population status, suggesting targeted conservation measures and precautionary management to prevent further loss of genetic diversity.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: genetics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708098v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.708098v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Marin, A., Zavalaga, F., Gozzer-Wuest, R., Santos-Rojas, L. E., Reyes-Flores, L. E., Alfaro, R., Bearez, P., Zelada-Mazmela, E.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">AbiOmics: An End-to-End Pipeline to Train Machine Learning Models for Discrimination of Plant Abiotic Stresses Using Transcriptomic Profiling Data</h2>
            <p class="paper-summary">Abiotic stresses are primary constraints on global crop productivity, reducing yields by up to 80%. While traditional phenotypic sensing detects stress only after physiological symptoms emerge and often fails to discriminate specific stressor types, transcriptomic profiling offers a high-dimensional solution, capturing rapid and sensitive molecular shifts. In this study, we developed AbiOmics, the first end-to-end machine learning pipeline specifically designed to identify and discriminate among multiple stressors. This approach represents a previously undocumented method for stress specification using large-scale transcriptomic big data. We identified 320 stress-specific marker genes using a curated collection of 1,243 transcriptomes of Arabidopsis samples treated with four major abiotic stresses, salt, cold, heat, and drought. A single-layer perceptron model trained on these features achieved 91% accuracy during five-fold cross-validation and 93% accuracy on an independent test set. The model demonstrated an unprecedented capacity to generalize to multi-stress conditions, identifying concurrent signatures in combinatorial salt-and-heat treatments. By integrating marker identification with SHAP-based biological interpretation, AbiOmics provides a rigorously validated diagnostic tool superior to conventional sensing. This framework establishes a high-confidence labeling strategy for AI-driven crop management and precision breeding to mitigate climate change impacts.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707868v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.25.707868v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Park, M., Oh, Y., Choi, W., Jo, Y. D.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.300000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Physiological re-replication during human stem cell differentiation</h2>
            <p class="paper-summary">During defined developmental windows in Drosophila, controlled re-replication generates physiological gene amplification. Although gene amplification has also been observed during human stem cell differentiation, re-replication in human cells has largely been linked to tumor-associated genome instability. Here, we demonstrate that re-replication likewise operates as a physiological mechanism in human stem cells. Using Rerep-Seq and DNA fiber-combing, we identify distinct phases of re-replication during the differentiation of human myoblasts into myotubes and during the lineage commitment of mesenchymal stem cells toward adipogenic, osteogenic, chondrogenic, and neuronal fates. In all differentiation systems examined, re-replication occurred within defined temporal windows. FACS-isolated re-replicating cells exhibited elevated gene expression using RNA-Seq specifically within re-replicated genomic regions. Moreover, re-replicated DNA was detected as extranuclear DNA. These findings support a model in which cells that do not undergo re-replication, and thus avoid increased chromosomal instability, may nonetheless boost the expression of differentiation-relevant genes by acquiring re-replicated DNA released from neighboring re-replicating cells. We propose that human stem cells exploit an evolutionarily conserved re-replication mechanism to transiently increase gene copy number and thereby meet the heightened protein demands associated with differentiation.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: genomics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708451v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708451v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Minet, M., Beganovic, A., Rishik, S., Michaeli, E., Yildiz, D., Schmartz, G. P., Schwarz, P. E., Schaefer, M., Taenzer, T., Cucchiarini, M., Ludwig, N., Keller, A., Meese, E., Fischer, U.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.3500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">PantheonOS: An Evolvable Multi-Agent Framework for Automatic Genomics Discovery</h2>
            <p class="paper-summary">The convergence of large language model-powered autonomous agent systems and single-cell biology promises a paradigm shift in biomedical discovery. However, existing biological agent systems, building upon single-agent architectures, are narrowly specialized or overly general, limiting applications to routine analyses. We introduce PantheonOS (PantheonOS.stanford.edu), an evolvable, privacy-preserving multi-agent framework designed to reconcile generality with domain specificity. Critically, PantheonOS enables agentic code evolution, allowing evolving state-of-the-art batch correction and our reinforcement-learning augmented gene panel selection algorithms to achieve super-human performance. PantheonOS drives biological discoveries across systems: uncovering asymmetric paracrine Cer1-Nodal inhibition in proximal-distal axis formation of novel early mouse embryo 3D data; integrating human fetal heart multi-omics with whole-heart data to reveal molecular programs underpin heart diseases; and adaptively selecting virtual cell models to predict cardiac regulatory and perturbation effects. Together, PantheonOS points towards a future where scientific discoveries are increasingly driven by self-evolving AI systems across biology and beyond.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: bioinformatics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.707870v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.707870v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Xu, W., Poussi, E., Zhong, Q., Zeng, Z., Zou, C., Wang, X., Lu, Y., Cui, M., Okamura, D., Huang, C., Ding, J., Zhao, Z., Yang, Y., Pan, X., Vijay, V., Konno, N., Liu, N., Li, L., Ma, X. R., Conley, S. D., Kern, C., Goodyer, W. R., Bintu, B., Zhu, Q., Chi, N. C., He, J., Rognoni, L., Zhang, X., Wu, J., Ellison, D., Rabinovitch, M., Engreitz, J. M., Qiu, X.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Crowder-specific modulation of hepatitis C virus NS3/4A protease activity and local structural dynamics</h2>
            <p class="paper-summary">Macromolecular crowding modulates enzyme behavior in crowder- and protein-specific ways, yet its impact on viral proteases, which are often key therapeutic targets, remains unclear. Here, we investigated the hepatitis C virus NS3/4A protease under increasing concentrations of polyethylene glycols (PEGs), ficoll, dextran, and lysozyme using a fluorescence-based activity assay and intrinsic tryptophan fluorescence. PEGs reduced catalytic activity while leaving substrate binding largely unaffected or moderately enhanced. These effects were accompanied by a moderate tryptophan fluorescence spectral narrowing, consistent with reduced heterogeneity in local conformational environments. In contrast, ficoll enhanced catalytic efficiency despite stronger fluorescence quenching, indicating local structural changes that favored catalysis. Dextran and lysozyme inhibited protease activity through distinct kinetic patterns, likely reflecting differences in their size, shape, and chemical properties. Thermal analysis revealed crowder-specific local structural changes in NS3/4A without global unfolding up to 65{degrees}C, with differences in local stability and flexibility corroborating the observed kinetic effects. These findings demonstrate that macromolecular crowding modulates NS3/4A catalysis through crowder specific effects on local structure.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708426v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708426v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Lobka, M., Trylska, J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Genomic analyses demonstrate the absence of genetic sex determination in the dioecious conifer Taxus baccata</h2>
            <p class="paper-summary">Hundreds of plant lineages have independently evolved dioecy, i.e., separation of female and male flowers on different individuals. In all dioecious plants investigated at the molecular level to date, sex is determined genetically through a sex-determining region (SDR). SDRs have mostly been studied in angiosperms, although dioecy is relatively more common among gymnosperms. Here, we investigate sex determination in the gymnosperm Taxus baccata. We assembled four haplotype-resolved chromosome-level genomes for one female and one male tree, with an average size of 10.04 Gb, and generated resequencing data for 100 phenotypically sexed individuals. Strikingly, k-mer analyses, genome-wide association studies and differential coverage analyses demonstrate the absence of an SDR in the T. baccata genome. This indicates a non-genetic mechanism of sex determination, most likely via a sex-specific epiallele. Given that T.baccata is the first species studied among a large group of conifers, our findings suggest that such a mechanism might be widespread.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: plant biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708407v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708407v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Bross, D., Mittelbach, J., Pippel, M., Mader, M., Lazic, D., Uelze, L., Schroeder, H., Pers-Kamczyc, E., Kurtz, S., Winkler, S., Muller, N. A., Kersten, B.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Protein-guided RNA barcoding links transcriptomes to synaptic architecture</h2>
            <p class="paper-summary">Mammalian brain function relies on the precise synaptic architecture of diverse cell types, yet scalable methods for linking a neuron's transcriptomic profile to its neuroanatomy remain limited. We present Synapse-seq, an in vivo strategy in which cell-identifying barcoded mRNAs are routed to subcellular compartments via targeting proteins and detected by single-cell and spatial genomics. Using AAV delivery for minimal perturbation of gene expression, we directed barcodes to presynaptic terminals (via synaptophysin) in four distinct circuits, or to postsynaptic sites (via nanobodies to endogenous PSD95) of hippocampal excitatory neurons. In the mouse primary visual cortex, presynaptic Synapse-seq recovered known long-range projections and discovered cortical layer subtypes with distinct thalamic innervation. In the anterior cortex, we elucidated simple topographic rules of corticostriatal innervation: intratelencephalic neurons followed a continuous depth-to-target gradient, while extratelencephalic neurons exhibited striatal collaterals that spatially correlated with medullary innervation. Finally, postsynaptic barcoding of excitatory neurons revealed cell type-specific variation in dendritic architectures across and within hippocampal subfields. These data establish Synapse-seq as a versatile, genomics-based approach for the integrated definition of molecular identity and synaptic organization across mammalian brains.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: neuroscience
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.705527v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.705527v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Urke, A., Dolan, M.-J., Silverman, J., Kim, M. T., Pineda, J., Garcia, S., Luu, J., Buckley, A., Kumar, V., Zhao, B., Chan, K., Nadaf, N., Balderrama, K. S., Arnold, D. B., Stevens, B., Deverman, B. E., Macosko, E. Z.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.550000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">The Cytochrome b m.14849T>C (S35P) Variant Induces Structural and Dynamic Alterations in the Heme bL Microenvironment in Multisystem Disease</h2>
            <p class="paper-summary">Mitochondrial Complex III dysfunction is frequently associated with pathogenic variants in the MT-CYB gene, yet the functional consequences of many missense substitutions remain unresolved because they are classified as variants of uncertain significance (VUS). One such variant, m.14849T>C (p.Ser35Pro), has been reported in patients with multisystem mitochondrial phenotypes, including septo-optic dysplasia, cardiomyopathy, and exercise intolerance, although its structural impact on Cytochrome b function remains unclear. In this study, we employed 300 ns all-atom molecular dynamics simulations to assess structural and energetic consequences of the S35P substitution in the Cytochrome b subunit of human mitochondrial Complex III. The S35P variant did not induce global destabilization of the protein scaffold but instead promoted localized perturbations within the heme bL microenvironment. The mutation was associated with loss of a heme-proximal hydrogen-bonding network involving Ser35 and a decrease in electrostatic interaction energy between the protein matrix and the heme bL cofactor. Radial distribution function analysis further supported loosening of local packing around the prosthetic group. Consistent with these local changes, dynamics analyses indicated increased flexibility in distal transmembrane helices that form the heme-pocket scaffold and greater variability in the inter-heme Fe(bL) - Fe(bH) distance. Together, our findings suggest that S35P may exert functional effects by reorganizing the heme bL microenvironment rather than by inducing large-scale structural destabilization, underscoring the value of structure- and dynamics-based evaluation for mitochondrial VUS and suggesting a plausible mechanistic link to the pathophysiology of multisystem mitochondrial diseases.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biophysics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708559v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708559v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Yasar, E., Demir, A. Y., Dogru, S.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.6000000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Constraint Semantics for Multi-level Organization</h2>
            <p class="paper-summary">Biological organisation is inherently multi-level: molecular processes, membrane dynamics, cellular geometry, and tissue context reciprocally constrain one another, often through boundary-mediated feedback. A recurring theme in theoretical biology is that such organisation is not well captured by models that assume a fixed repertoire of variables and a pre-given state space: what counts as a relevant state description can depend on organisational context and history. The principle of biological relativity further sharpens the same challenge from a different angle, emphasising that no level is causally privileged and that cross-level feedback can lead to circular causality. These lines of work motivate a structural multi-level semantics for modeling biological pathways. We introduce a constraint-based semantic framework that distinguishes an evolving organisational scaffold -- namely, the admissible multi-level patterns and interfaces -- from the pathways that traverse and coordinate them. This separation yields mathematical, loop-level diagnostics for boundary-driven circular causality: it identifies when organisational trajectories induce persistent reparameterisations of local state descriptions, and it classifies cyclic regimes into reversible loops, stable history-dependent loops, and unique (rare) organisational reconfigurations. The framework is accompanied by a systematic crosswalk that mainstreams causal, dynamical, and computational approaches, clarifying what is gained when interfaces and local--global consistency are treated as semantic, rather than purely parametric, structures. We demonstrate the approach on a canonical excitable-cell exemplar by modelling a single Hodgkin spike as a cross-level interface loop coupling membrane, molecular, and cellular constraints. Without re-deriving Hodgkin--Huxley kinetics, the resulting diagnostics provide an explicit semantics for boundary-mediated feedback and spike-induced history dependence, including cases in which cyclic activity imprints persistent changes in effective excitability. Together, the case study and comparisons position constraint semantics as a practical mathematical layer for multi-level biological organisation: compatible with existing mechanistic models, yet designed to expose circular causal closure and organisation-dependent state descriptions that standard formalisms typically leave implicit.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: systems biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708558v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708558v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Imtiyaz, S.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Antigenic landscape of a highly mutated SARS-CoV-2 Spike in ongoing viral evolution</h2>
            <p class="paper-summary">The ongoing emergence of SARS-CoV-2 variants in an increasingly immune-experienced population is largely enabled by the plasticity of the Spike protein, which lies at the frontline of host immune pressure. Here, we investigated how extensive remodeling of the N-terminal domain (NTD) in Spike influences its antigenic properties. Using BA.2.87.1, a variant heavily mutated in this domain, we found that even large deletions do not substantially disrupt overall Spike structure or pseudovirus infectivity. However, our structural and binding analyses revealed that the NTD exhibits increased flexibility and that interaction with a heme-metabolite, contributing to assembly of an immunodominant epitope, is lost. These conformational effects, coupled with 33 NTD mutations, compromised the ability of human convalescent antibodies to engage this domain, contributing to their reduced neutralizing capacity. Consequently, BA.2.87.1-like variants may escape recognition by the pre-existing NTD-targeting antibodies, potentially reducing protection. Together, our results highlight the intrinsic adaptability of the Spike beyond the receptor-binding domain, with important implications for immune escape during viral evolution.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: immunology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708159v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708159v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Turelli, P., Eray, E., Raclot, C., Trono, D., Antanasijevic, A.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.7, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Diverse microbial metal resistance and novel metal cycling organisms in copper/nickel mine tailings</h2>
            <p class="paper-summary">Mine tailings contribute to environmental heavy metal contamination through the formation of acid mine drainage (AMD). Microbially-mediated processes such as iron and sulfur redox cycling influence metal mobility. Here, we applied an integrated metagenomic and metaproteomic approach to profile microbial communities across vertical geochemical gradients in legacy copper/nickel tailings in Sudbury, Ontario, Canada. From 43 samples, we recovered 454 non-redundant metagenome-assembled genomes (MAGs), revealing diverse populations within the Actinobacteriota, Desulfobacterota, and uncultured lineages such as Candidatus Eremiobacterota and SZUA-79. Functional profiling identified 301 putative iron- and sulfur-cycling MAGs, including those within the Ca. Eremiobacterota and SZUA-79 phyla. Metal resistance genes were widespread and diverse, with abundances that did not correlate with measured metal concentrations. Proteomic data confirmed in situ expression of selected metal resistance genes and iron/sulfur metabolism genes, despite limited protein recovery from this challenging matrix. Our findings highlight both the depth of microbial diversity in metal resistance and metal biogeochemical cycling in mining waste, as well as the technical challenges that currently limit genomic and proteomic sequencing coverage in low-biomass, metal-rich matrices. This work also presents new protocols for multi-omics data capture and analysis from metal contaminated environments, including new protein extraction and bioinformatic gene annotation tools.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708553v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708553v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Chen, M., Gregoire, D. S., Bain, J. G. S., Blowes, D. W., Hug, L. A.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">lncRNA-ISM1 Promotes Hepatocellular Carcinoma Progression through RBM10-Mediated Alternative Splicing of ISM1 and Akt-S6-Dependent Glucose Metabolic Reprogramming</h2>
            <p class="paper-summary">Isthmin-1 (ISM1) is a recently identified adipokine that promotes glucose uptake and enhances cellular metabolism. While the activity of the ISM1 protein is regulated by glycosylases, its transcriptional and posttranscriptional regulation remain poorly understood. A novel alternatively spliced variant of ISM1 (ISM1-AS) was recently identified. Unlike canonical ISM1, ISM1-AS lacks an AMOP domain, a key structural element required for ISM1 function, suggesting the loss of its metabolic regulatory activity. In this study, we found that ISM1 expression was significantly reduced in HCC tissues and correlated with poor prognosis. Functional assays revealed that ISM1 overexpression markedly suppressed HCC cell proliferation and invasion, whereas ISM1-AS overexpression had the opposite effect. Importantly, ISM1 co-overexpression attenuated the oncogenic effects of ISM1-AS. Knockdown of the antisense transcript lncRNA-ISM1 reduced ISM1-AS expression while increasing ISM1 expression, thereby suppressing HCC proliferation and migration.Our findings demonstrate that lncRNA-ISM1 promotes HCC progression through the RBM10-mediated alternative splicing of ISM1 and activation of the Akt-S6 signalling pathway, highlighting its potential as a therapeutic target for HCC.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: molecular biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708505v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708505v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Li, M., Huang, D., Ren, Y., Wang, Z., Li, Y., Zuo, W., Li, Y., Jin, Y., Xiong, Y.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.800000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Seminal fluid proteins can mitigate sexual conflict: the case of remating in insects</h2>
            <p class="paper-summary">Males during mating never transfer just sperm; to the best of our knowledge, they always deliver a rich seminal fluid as well. The proteins in the seminal fluid have an important sperm supporting role, but they also cause changes in the female physiology and can impose a mating cost. The associated costs and delay in the time of remating, lead to the view that those proteins evolved primarily due to sexual conflict and delay female remating beyond the optimal rate. To examine the role of seminal fluid proteins in sexual conflict we use a mathematical model of reproductive physiology, informed by the accumulated knowledge on Drosophila melanogaster. In accordance with the theory, we find that males always benefit from inducing longer remating intervals in females. But, we also find that this conflict is reduced when female reproduction is regulated by the male proteins. Without seminal fluid proteins females have a single, well-defined, optimal remating rate. However, when seminal proteins are used to regulate females reproduction, females can reach the same offspring production for a range of mating intervals. This wider range of possible remating times could provide females with a buffer against uncertain mating opportunity. It could also allow females to be more selective on male quality, by reducing the cost associated with delaying remating. Our results suggest that, while there is a conflict over the remating rate, seminal fluid proteins reduce its intensity, highlighting their role in aligning the interests of both sexes.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: evolutionary biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708467v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708467v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Michalak, P., Duneau, D., Ferdy, J.-B.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.8500000000000005, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Deciphering the genetic basis of phytoplankton traits through genome-wide association studies</h2>
            <p class="paper-summary">Recently, an inventory of genes in phytoplankton was conducted through expeditions such as TARA Oceans. Approximately 1.5 million genes were identified, of which at least three-quarters have unknown function. Presently, a several research programmes are engaged in the sequencing of marine biodiversity, resulting in a rapid expansion of genomic databases. Access to the genomic sequences of these organisms will soon be readily accessible to the scientific community. Although analysing this data is promising, the characterization of genes or genomes, on the other hand, is progressing very slowly and remains a major challenge for scientists. The aim of this study was to use GWAS approaches to decipher genomic loci without a priori assumptions. The microalga Tisochrysis lutea was selected as a case study due to its economic importance and the extensive knowledge accumulated over the years. Particular attention was paid to pigment and lipid metabolism due to their high commercial value. To implement the GWAS approach, a collection of algal lineages was established (100 lineages) from available polyclonal strains (15 strains). This collection was then phenotyped under two different culture conditions. Of the 31 phenotypic traits investigated, 18 met the requirements for GWAS analysis. Concurrently, each algal lineage was genotyped by whole genome sequencing to inventory all genetic polymorphisms. A mixed model was applied, revealing 13 significant associations between phenotypic traits and alleles. These associations highlight previously unsuspected genomic loci that play a major role in pigment or lipid content. Genes identified at these loci may have a direct or indirect role in these metabolic pathways. Nevertheless, elucidating the molecular mechanisms of the associated genes remains limited without the implementation of functional approaches. Despite the complexity of the process, we conclude that the GWAS approach was effective for deciphering phytoplankton genomes, particularly for quantitative traits of interest. Ideally, this approach should be combined with other functional methods to progressively decode marine genomes.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: genetics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708454v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708454v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Maupetit, A., Segura, V., Pajot, A., Nicolau, E., Bougaran, G., Lacour, T., Berard, J. B., Charrier, A., Schreiber, N., Robert, E., Saint-Jean, B., Carrier, G.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Heavy Metal-Resistant, Plastic-Degrading Bacillus sp. Isolated from Landfill Leachate: Identification and Characterization</h2>
            <p class="paper-summary">Landfill leachates in rapidly urbanizing regions like Dhaka present a complex ecological challenge owing to the concurrent buildup of heavy metals and plastic waste. Despite the severity of this pollution, the role of indigenous multi-functional bacteria in mitigating these mixed contaminants remains poorly understood. This research sought to isolate and characterize bacteria resistant to heavy metals and capable of degrading plastics from the Aminbazar and Matuail landfills and evaluate their bioremediation potential. Physicochemical analysis confirmed extreme contamination, with heavy metal levels (Pb, Cr, Cd, Cu) significantly exceeding WHO safety limits. Out of 81 isolates, nearly half exhibited multi-metal resistance and polyethylene (PE) degradation capacity. Statistical analysis showed a significant correlation between plastic degradation and multi-metal tolerance, suggesting a linked evolutionary adaptation. Enzymatic assays confirmed enzymes (e.g., urease, catalase, citrate and esterase) as drivers of both plastic degradation and heavy metal tolerance in leading isolates. Molecular screening identified the resistance genes pbrA and alkB, while the high prevalence of Class 1 integrons (80% in pbrA-positive isolates) points to a high potential for horizontal gene transfer in these environments. Furthermore, MALDI-TOF MS identified the functional isolates as Bacillus sp. with FTIR verifying the contribution of specific cell-surface functional groups to metal biosorption. These results underscore the promise of native Bacillus strains as promising agents for the development of sustainable, integrated biotechnologies for landfill restoration and complex waste management.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708447v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708447v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Antu, U. S., Sarker, A., Haque, N., Karmakar, J., Khaleque, A., Hossain, M. S., Parvez, M. A. K.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 7.95, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Identification of Antibiofilm Agents against Salmonella enterica from the Pathogen Box Compound Library</h2>
            <p class="paper-summary">Background: Biofilms are central to Salmonella pathogenesis, and targeting their formation is believed to produce less evolutionary pressure of growth inhibition than traditional antibacterials. In this study, we screened the Medicines for Malaria Venture (MMV) Pathogen Box library to identify anti-biofilm agents against S. enterica that possess drug-like properties. Methodology/ Principal Findings: A crystal-violet-based medium-throughput antibiofilm screen of Salmonella enterica serovar Typhimurium ATCC 14028 and a clinical Salmonella enterica serovar Elisabethville isolate was performed on polystyrene surfaces using the 400-compound Pathogen Box library. Compounds that inhibited biofilm formation by >30% and growth by <10% were identified as hits. Salmonella red-dry-rough and motility phenotypes were explored in mechanism of action studies on one hit compound. The Salmonella antibiofilm hit rate was 0.75% for this library. MMV688371 (benzamide) inhibited biofilm formation of S. Typhimurium ATCC 14028 by 33% without inhibiting growth. An ethambutol analogue (MMV687273) and auranofin (MMV688978) met the hit criteria against S. Elisabethville LLD035X. Auranofin showed concentration-dependent, growth-inhibition-independent antibiofilm activity against typhoidal and non-typhoidal Salmonella from Nigeria, and inhibited the motility of S. Elisabethville LLD035X at 5 micromolar. At 5 micro molar, aurothioglucose, an auranofin gold (I) analogue, and non-gold analogue 1-thio-beta-D-glucose tetraacetate, inhibited biofilm formation by 61.30% and 11.39%, respectively, pointing to essentiality of the gold (I) moiety for activity. Conclusions/ Significance: Structurally diverse small molecules can inhibit biofilm formation by Salmonella, and motility inhibition is an important mechanism for this activity. Auranofin inhibits typhoidal and non-typhoidal Salmonella biofilm formation, with its gold content being required for these activities.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.707623v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.707623v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Fagbemi, A. A., Babalola, C. P., Kwasi, D. A., Akinlabi, O. C., Kotila, O., Okeke, I. N.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Heterogeneous dynamics of mobile genetic elements encode and spread antibiotic persistence in bacteria</h2>
            <p class="paper-summary">Bacterial persisters are nonreplicating cells that transiently evade antibiotic killing within an otherwise susceptible, replicating population. While persister formation has traditionally been attributed to phenotypic heterogeneity, reversible functional variations among genetically identical cells, most studies lack direct genomic evidence to exclude the contribution of cryptic and reversible genetic variations. Here, we demonstrate that the dynamics of the Legionella pneumophila integrative and conjugative element (ICEs) pP36 encode and disseminate antibiotic persistence-like traits. During biofilm formation, pP36 undergoes heterogeneous chromosomal excision generating a subpopulation of nonreplicating persisters-like bacteria with a distinct molecular profile. pP36-mediated L. pneumophila growth rate heterogeneity significantly enhances bacterial survival under fluoroquinolone treatment. Our findings reveal that pP36 dynamics not only orchestrate physiological diversity but also serve as a vehicle for horizontal transfer of antibiotic persistence mechanisms among clinical Legionella isolates. While ICEs are recognized as key drivers of antibiotic resistance dissemination, this study establishes that ICEs-mediated phenotypic variations is a critical determinant of persister-like cell formation, reshaping our understanding of bacterial adaptability to antibiotic stress.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708469v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708469v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Dadole, I., Gory, R., Huguet, K. T., Lenuzza, N., Steurer, F., Morin, B., Ginevra, C., Attaiech, L., Charpentier, X., Jarraud, S., Blaha, D., Schmidt, A., Rasigade, J.-P., Personnic, N.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.05, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Opposing regulatory logics converge on ABA receptors to govern the trade-off between growth and drought acclimation</h2>
            <p class="paper-summary">Plants must dynamically balance growth with stress responses, a trade-off centrally governed by the hormone abscisic acid (ABA). How ABA sensitivity is nimbly tuned to meet this challenge remains a fundamental question. Here, we uncover a complex regulatory network of leucine-rich repeat receptor-like kinases (LRR-RLKs) that directly controls ABA receptors stability. We identify GASSHO1 (GSO1) as a core component that phosphorylates the ABA receptors PYL2 and PYL4, marking them for degradation and thus serving as a critical brake on ABA signaling. This brake operates under normal conditions but is released upon drought stress, when accumulated ABA suppresses the expression of GSO1-activating CIF peptides, increasing PYLs and sensitivity. Strikingly, this derepression mechanism opposes our previously identified CEPR2 pathway, where drought-induced CEP peptides inhibit the kinase stabilizing PYLs. The integration of these two antagonistic regulatory modules within a single LRR-RLK network reveals a sophisticated systems-level logic that allows for the precise and dynamic control of ABA perception. This work uncovers a molecular framework that explains how plants finely calibrate the critical balance between growth and drought acclimation.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: molecular biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708209v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708209v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Wu, C., Gu, S., Liu, X., Zhang, Y., Zhang, L., Liu, Q., Lu, J., Huang, J., Yang, G., Yan, K., Zheng, C., Zhang, S.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Circular Smad1-Encoded Polypeptide Regulates Myogenesis</h2>
            <p class="paper-summary">The majority of RNAs transcribed from the genome are non-coding RNAs (ncRNAs) that are involved in regulating the expression of protein-coding genes. However, a growing body of research highlights several novel microproteins encoded by unconventional ncRNAs such as long non-coding RNAs and circular RNAs (circRNAs) as important regulators of disease and development. Although several circRNAs have recently been reported to translate into functional peptides in diverse tissues, their roles in skeletal muscle remain largely unexplored. In this study, polyribosome-associated RNA sequencing and publicly available translatable circRNAs from the riboCIRC database were curated to discover potential protein-coding circRNAs in mouse C2C12 skeletal muscle cells. We validated a few circRNAs with high potential of translating into proteins in mouse C2C12 cells, including circular Smad1 (circSmad1) that encodes a 194 amino acid peptide called circSmad1-194aa. Interestingly, silencing of circSmad1 in C2C12 cells resulted in loss of myotube fusion and maturation. CircSmad1-194aa was found to contain the DNA-binding SMAD1-MH1 domain that localized into the nucleus during myogenesis. Moreover, CircSmad1-194aa associates with the BMP-responsive element (BRE) in the Id1 promoter that is known to inhibit Myod1-driven myoblast differentiation. We propose that circSmad1-194aa promotes myogenesis by masking Id1-BRE from SMAD complex interaction, leading to suppression of ID1 expression and upregulation of MYOD1. Together, our findings identify circSmad1-194aa as a novel regulator of skeletal muscle differentiation and highlight the potential for the discovery of other functional circRNA-derived peptides in muscle pathophysiology.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: molecular biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708385v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708385v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Sinha, T., Dutta, S., Prasad, P., Panda, A. C.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.15, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Alternaria atra from distinct ecological roles share functional genomic repertoires</h2>
            <p class="paper-summary">Fungi, particularly ascomycetes, exhibit diverse ecological lifestyles, including endophytism, pathogenicity, and saprotrophy. Species of the genus Alternaria are taxonomically and ecologically diverse, yet the genomic determinants underlying different lifestyles remain poorly understood. Here, we investigate lifestyle-associated genomic variation in Alternaria atra using two newly collected isolates obtained as plant endophytes. We confirm their taxonomic identity and generate draft genome assemblies for both isolates. We assess their phenotypic behaviour under laboratory conditions and examine their genomic features alongside those of a previously published A. atra isolate described as pathogenic. Despite differing isolation histories, the endophytic and pathogenic isolates exhibit similar behaviour under laboratory conditions and possess highly comparable genomic repertoires, including predicted effector proteins, carbohydrate-active enzymes, and biosynthetic gene clusters. We detect no clear genomic signatures distinguishing endophytic and pathogenic origins or lifestyles. These findings suggest that A. atra harbours a shared genomic repertoire compatible with multiple ecological strategies, supporting a model of lifestyle plasticity rather than fixed genomic specialization. Our results add to growing evidence that genome content alone does not reliably predict ecological roles in ascomycete fungi.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: genomics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708203v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708203v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Schmey, T., Bahar, K., Tominello-Ramirez, C., Sepulveda Chavera, G., Stam, R.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.200000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">The genome of the reef-building coral Porites harrisoni from the southern Persian/Arabian Gulf</h2>
            <p class="paper-summary">We present a genome assembly from the coral species Porites harrisoni from the southern Persian/Arabian Gulf, the hottest ocean basin where corals live. The assembly is 626.7 Mb in size, spanning 1,883 contigs with a contig N50 of 807.4 kb, including a single-contig mitochondrial genome. The assembly has a BUSCO completeness of 86.3% (single = 72.5%, duplicated = 13.7%, fragmented = 1.2%, missing = 12.5%) using the eukaryota_odb10 reference set (n = 255). A total of 59.23% of the nuclear genome consists of repeats, comprising 15.89% retroelements, 10.00% DNA transposons, and 31.71% unclassified repeats. Gene annotation of this nuclear genome assembly identified 27,823 protein-coding genes. The mitogenome has an assembly size of 18,639 bp with 13 protein-coding genes as well as 2 tRNAs and 2 rRNAs. The genome of P. harrisoni provides a valuable genomic resource of a coral from an extreme environment, which will enable comparative analyses, enhancing our understanding of the genomic architecture underlying thermal resilience. Such comparisons will contribute to elucidating the evolutionary basis of heat tolerance and adaptive capacity of corals in the context of rapid climate change.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: genomics
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708201v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708201v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Fiesinger, A., Sharaf, A., Alderdice, R., Perna, G., Manns, H., Burt, J. A., Voolstra, C. R.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Early proteomic signatures of Alzheimer`s disease in the retina and brain of 3xTg-AD mice</h2>
            <p class="paper-summary">Visual dysfunction and retinal structural alterations often precede brain pathology and cognitive decline in Alzheimer`s disease (AD), yet the molecular basis of these early changes and their relationship to the brain pathology remain unclear. Here, we performed quantitative proteomic profiling of retina and brain from 1-month of age triple-transgenic (3xTg-AD) mice harboring human PS1M146V, APPSwe, and tauP301L mutations, preceding detectable morphological abnormalities. Proteomic analysis identified 92 significantly altered proteins in the retina and 130 in the brain, with eight overlapping proteins between tissues. These shared proteins included three hemoglobin subunits (HBB1, HBB2, A8DUK4) and five proteins involved in metabolic regulation and intracellular transport. In addition to individual protein changes, pathway analysis demonstrated that mitochondrial metabolism and intracellular transport were commonly dysregulated in both tissues. Brain proteome was characterized by broad changes in mitochondrial-associated proteins, including respiratory chain components and mitochondrial ribosomal subunits, as well as proteins related to autophagy and synaptic vesicle pathways. In contrast, the retinal proteome was characterized by downregulation of vision-related proteins, altered small molecule transporters, and a marked reduction of the mitochondrial enzyme succinate-CoA ligase subunit beta (SUCB2). As SUCB2 links mitochondrial metabolism to epigenetic regulation through succinylation and lactylation, its depletion may promote mitochondria-to-nucleus signaling and early transcriptional reprogramming in the AD retina. Together, these findings demonstrate early metabolic and transport dysregulation in both retina and brain and highlight selective alterations of visual proteins in the retina. These early retinal proteomic changes provide valuable insight into understanding early metabolic disturbances in the eye and brain for AD detection.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biochemistry
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708380v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708380v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Puja, A., McNeel, R., Xu, R., Zhu, S., Hansman, D., Du, J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.3, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Pro-IL-1 activates NF-B independently of maturation</h2>
            <p class="paper-summary">Interleukin-1{beta} (IL-1{beta}) is an essential pro-inflammatory cytokine which functions as a key factor in innate immunity. The precursor protein, pro-IL-1{beta}, has long been regarded as an inactive form in innate immune responses. Here, we unveil the biological function and regulation of pro-IL-1{beta} in activating the NF-{kappa}B signaling pathway, which is distinct from IL-1{beta} signaling. The expression and release of pro-IL-1{beta} are induced by inflammatory stimuli and then pro-IL-1{beta} acutely activates the gene transcription driven by NF-{kappa}B in a dose dependent manner. This activity is resistant to IL-1 receptor antagonist (IL-1Ra). The signal transduction triggered by pro-IL-1{beta} relies on MyD88 and endocytosis. We further demonstrate that the N-terminal pro-peptide primarily contributes to this activity. Furthermore, we identify TLR7 and TLR8 as the binding partners of pro-IL-1{beta} in vitro and as the potential receptors mediating pro-IL-1{beta}-induced NF-{kappa}B activation. Collectively, this study sheds light on the unique cytokine function of pro-IL-1{beta} and provides new insights into the functional characterization of pro-cytokines in innate immunity.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biochemistry
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708191v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708191v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Mou, L., Zhang, C.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.35, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">The pod components of the Shigella T3SS sorting platform accommodate multiple copies of Spa33 (SctQ)</h2>
            <p class="paper-summary">The bacterial type III secretion system (T3SS) uses a membrane-embedded injectisome assembly to export effector proteins into host cells. While atomic-level structural details have been revealed for much of the T3SS apparatus, the model of the cytoplasmic sorting platform remains largely low-resolution. A central structural element of the sorting platform is the so-called "pod" protein, SctQ, which anchors the sorting platform to the inner membrane via interaction with the adaptor protein SctK, and connects to the central ATPase via the spoke protein SctL. SctQ proteins also interact with alternatively translated homodimers of their C-terminal SPOA2 domains. Low resolution electron density maps have provided an outline of the sorting platform architecture, and fluorescence microscopy studies have suggested a 1:4:2 SctK:SctQ:SctL stoichiometry. While there are experimental and AlphaFold structures of the individual components or complexes of sorting platform pod, there is currently no model for the pod structure that adequately fits the electron density or accounts for the proposed stoichiometry. Here we use AlphaFold to generate a model of the Shigella pod complex in which two copies of the SctQ protein, Spa33, bind the adaptor protein MxiK, with each copy of Spa33 bound to an alternately translated SPOA2-SPOA2 domain. We show through mutation of energetically critical interface residues, predicted by computational mutant scanning, that both Spa33 binding sites on MxiK are required for T3SS activity in Shigella flexneri, as well as binding of Spa33 to the SPOA2-SPOA2 homodimer. We find that this model fits well to the upper two-thirds of the pod electron density, albeit in a manner that places the protein components slightly closer to the inner membrane than traditionally presented. Further, cryogenic electron tomography of mutant injectisomes reveals the lack of a complete sorting platform and may suggest an alternative model featuring four copies of Spa33 that still fits the upper pod electron density but better explains unfilled density in the lower third of the pod.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biochemistry
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708312v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708312v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Whittier, S. K., Tachiyama, S., Heydari, S., Picking, W. L., Liu, J., Picking, W. D.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Faf2/Ubx2 potentiates p97/Cdc48 segregase activity via functional priming and reinforcement of the UT3 domain in Ufd1</h2>
            <p class="paper-summary">The AAA+ segregase p97/Cdc48 extracts poly-ubiquitinated proteins from entrenched cellular environments to maintain proteostasis. However, its functional capacity is not uniform; extracting substrates from dense assemblies like stress granules or membrane necessitates a fully potentiated state. Here, using precisely synthesized ubiquitinated substrates, we show that the Faf2/Ubx2 cofactor hyperactivates p97/Cdc48, lowering its minimal ubiquitin chain requirement and broadening linkage specificity. Cryo-EM analysis of the activated complex demonstrates that Faf2/Ubx2 potentiates p97/Cdc48 segregase activity by structurally remodeling the substrate recognition module (K48-diUbProx) and priming its engagement with the AAA+ motor (p97/Cdc48-Npl4). These coordinated actions establish a 'pump-unit' architecture that drives efficient substrate processing. Our findings illuminate a conserved mechanism that unlocks the maximal capacity of p97/Cdc48 and provide a blueprint for therapeutically targeting its hyper-activated state in disease.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biochemistry
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708172v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708172v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ren, Y., Zheng, Q., Duan, Y., Xu, Z., Qu, Q., Weng, Y., Cui, S., Yu, Y., Pan, M., Liu, L.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.450000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Structural basis of the lobster carapace blue colour mediated by an HPR protein</h2>
            <p class="paper-summary">The chemical basis underlying the striking blue hue of live H. americanus, known as American lobster, are studied in evolutionary biology and in polyene physical chemistry. Carapace colouration is generated by the antioxidant astaxanthin bound within the carotenoprotein crustacyanin complexes. Here, we present the ex vivo structure of the most abundant -crustacyanin and {beta}-crustacyanin forms, determined respectively by cryo-electron microscopy and X-ray crystallography to a resolution of 2.75 [A]. Our structural analysis reveals -crustacyanin as an elongated arrangement of {beta}-crustacyanin heterodimers tethered by an heptatricopeptide repeat (HPR) protein. In vitro complex formation between the {beta}-crustacyanin unit with a synthetic heptatricopeptide reproduces the observed blue colour of -crustacyanin, identifying the HPR protein, in concert with crustacyanins, as contributor in tuning carapace colour. Overall, these results explain how nature adjusts the colour across the entire visible spectrum by exploiting the bathochromic shift of astaxanthin from its unbound red form ({lambda}max = 472 nm) firstly to the {beta}-crustacyanin violet bound form ({lambda}max = 591 nm), and then to the -crustacyanin bound blue form ({lambda}max = 631 nm).</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: biochemistry
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708136v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708136v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Cedri, M. C., Bansia, H., Amici, A., Ortore, M. G., McCarthy, A., Mueller-Dieckmann, C., Raffaelli, N., Durbeej, B., Lingas, R., Wang, T., Des Georges, A., Cianci, M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Benefits and Challenges of Integrating a Generative AI Assisted Reading Guide in an Undergraduate Journal Club Assignment</h2>
            <p class="paper-summary">Developing scientific reading skills is critical for undergraduate STEM students due to the unique formatting and use of specialized jargon in scientific literature. Generative AI tools such as ChatGPT offer students the ability to ask questions about what they are reading interactively. Previously, we reported the development of a ChatGPT-assisted reading guide that combined structured, active reading strategies with using ChatGPT to clarify unfamiliar words and concepts in real time. In the initial study, undergraduates found the use of the ChatGPT-assisted reading guide helpful in their understanding of an abstract and introduction of a journal article. Here, the ChatGPT-assisted reading guide was used in a journal club assignment for an undergraduate chemistry course. ChatGPT transcripts were analyzed for common types of interactions, and students were surveyed about their experience. Overall, students reported that using the ChatGPT-assisted reading guide was helpful in understanding the article and helped them have more productive class discussions. However, some students also expressed skepticism about using AI tools, citing concerns about accuracy of AI-generated information and the effect of using AI on their own learning.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: scientific communication and education
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708236v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708236v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Ringer McDonald, A., Vazquez, A. V.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.55, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Variation of Human Transfer RNA Demand and Supply</h2>
            <p class="paper-summary">Codons function as translation units in open-reading-frames (ORF) of genes to encode for proteins. Transfer RNAs (tRNAs) mediate the connection of every codon to its cognate amino acid. Despite the cooperation between messenger and transfer RNA during translation, approaches to integrate codon usage and tRNA quantities remain to be established. Using matched mRNA- and tRNA-sequencing of peripheral blood cells, we apply a precision-biology approach quantitatively integrating transcriptomic codon- and corresponding tRNA-abundance. Thereby, we classify codons as highly or lowly supplied and compare optimality of synonymous codons. Additionally, we describe substantial differences regarding the conservation of a codon's tRNA-supply among healthy donors. A meta-ORF-analysis demonstrates depletion of lowly supplied codons at translation start sites. Discrepancy between codon- and tRNA-abundance, and codon-preference depending on the distance to the translation start site, seem to be non-random and could affect translational speed and thus provide a novel level of regulation of protein abundance.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: systems biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708130v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708130v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Kuelp, M., Bonig, H., Rieger, M. A.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.6, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Cross hybridization Inference for Phylogenetic Resolution (CIPHR)-FISH enables microbiome imaging with strain level taxonomic resolution</h2>
            <p class="paper-summary">The spatial organization of microbial communities is a critical determinant of host-microbe interactions, yet species-level mapping remains challenging due to high 16S rRNA sequence homology and spectral crosstalk in multiplexed fluorescence in situ hybridization (FISH). To address this challenge, we developed Cross-hybridization Inference for Phylogenetic Resolution (CIPHR)-FISH, a pipeline that integrates strategic probe design with supervised machine learning. CIPHR-FISH transforms probe cross-hybridization and spectral overlap, traditionally viewed as experimental noise, into informative molecular signatures. Using a gnotobiotic zebrafish model colonized with a defined mix of 10 zebrafish bacterial strains, we trained a support vector machine (SVM) on empirical hybridization patterns from pure bacterial cultures. CIPHR-FISH achieved 99.2 % macro-averaged accuracy, significantly outperforming standard linear unmixing (62.5 %), and successfully discriminated strains with 99.7% sequence homology. Applying this tool to gnotobiotic zebrafish larvae revealed distinct biogeographies: the intestinal bulb hosted highly structured, multi-layered polymicrobial aggregates, while the skin exhibited sparse, uniformly dispersed individual bacterial cells. Notably, we observed significant inter-individual variation in spatial community structure that was obscured by traditional bulk 16S rRNA sequencing. CIPHR-FISH provides a robust, scalable framework for high-resolution spatial biology by converting the limitations of molecular labeling into a rich data source for taxonomic classification. This approach enables the quantification of micro-scale ecological and stochastic forces that shape the microbiome across hosts.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708344v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708344v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Adade, E. E., Wang, R., Henneberry, C. M., Lemus, A. A., Stevick, R. J., Perez-Pascual, D., Audrain, B., Orsino, A. J., Farnsworth, D. R., Ghigo, J.-M., Valm, A. M.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">A sterol reductase responsible for the unusual 8(14)-unsaturation in bacterial sterol production and degradation</h2>
            <p class="paper-summary">Sterols are a class of lipids that play a crucial role in human health through their essential physiological roles and as a point of interaction between commensal and pathogenic bacteria. The biosynthesis and modification of these lipids is a well-characterized process in many eukaryotes and increasingly in bacteria. However, the proteins responsible for formation of the unusual 8(14)-unsaturation found in the sterols produced by aerobic methanotrophs, dinoflagellates, nematodes, and marine sponges, remains unknown. Here, we utilize a heterologous expression system to identify a bacterial 8,14-sterol reductase (8,14-Bsr) responsible for generating the 8(14)-unsaturation in the aerobic methanotroph Methylococcus capsulatus. This enzyme modifies the direct product of C-14 demethylation, reducing one double bond in the nuclear core structure and isomerizing the other to produce an 8(14)-sterol. We subsequently tested the requirement of putative active site residues for catalysis through site directed mutagenesis, identifying residues likely involved in interacting with the sterol substrate and directly catalyzing this reaction. Bioinformatic analysis of the distribution of 8,14-Bsr reveals it is unique to the bacterial domain, found primarily in the Methylococcaceae family, the Mycobacteriales order, and yet uncultured members of the Myxococcota phylum. Further phylogenetic analysis of 8,14-Bsr suggests it shares an evolutionary history with the C-14 demethylase in these organisms and that these two enzymes were likely inherited together. These results provide insight into novel sterol biochemistry, further delimiting sterol biosynthesis in the bacterial domain from eukaryotes and illustrating the importance of molecular characterization to identify bacterial proteins that interact with sterols.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: microbiology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708318v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708318v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Lee, A. K., Giner, J.-L., Welander, P. V.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.700000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">RNA-binding is the essential biological function of the Drosophila protein Brat</h2>
            <p class="paper-summary">Brain tumor (Brat) is a Drosophila TRIM-NHL protein required for embryogenesis and neural stem cell differentiation. Although structural and biochemical studies established that the Brat NHL domain specifically binds RNA, the in vivo requirement for this activity has not been directly tested. Here, we used structure-guided mutagenesis and genome engineering to determine whether RNA recognition is essential for Brat function during development. The direct interaction between Brats NHL domain and RNA containing Brat Binding Sites (BBS) can be abolished by alanine substitution of three separate residues on the NHL surface. We introduced these point mutations into the endogenous brat locus by CRISPR-mediated Scarless Gene Editing to generate three independent RNA-binding defective mutant (RBDmt) alleles. Complementation tests demonstrated that each allele behaves as a strong loss-of-function mutation: homozygotes and hemizygotes are inviable, and RBDmt alleles fail to complement classical brat null and hypomorphic alleles. Lethal phase analysis revealed death predominantly during late larval and pupal stages, consistent with known brat alleles. Consistent with the namesake brat phenotype, RBDmt larval brains exhibited widespread expression of neuroblast markers and a marked reduction of neuronal differentiation. In embryos, these alleles failed to complement female sterile brat alleles and recapitulated characteristic abdominal segmentation defects. Finally, RT-qPCR showed increased expression of endogenous Brat target mRNAs in mutant larvae, consistent with loss of Brat-mediated repression. Together, these results demonstrate that direct RNA binding is the essential molecular activity of Brat and that post-transcriptional regulation of Brat target mRNAs underlies its critical roles across development.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: molecular biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708583v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.27.708583v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Connacher, R. P., Hu, Y., Roden, R., Toledo, J., DesMarais, A., O'Connor, M. B., Lipshitz, H. D., Goldstrohm, A. C.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Integrin beta 1 and mannose receptor 2 are involved in the antifungal activity of bronchial epithelial cells through Aspergillus fumigatus lectin FleA interactions</h2>
            <p class="paper-summary">Aspergillus fumigatus is a world-wide saprophyte filamentous fungus which released conidia, its infectious morphotype, in the atmosphere. These conidia are inhaled daily by humans and can colonize the respiratory tract, where they may develop into hyphae, the invasive morphotype. We previously showed that bronchial epithelial cells (BECs) restrict A. fumigatus virulence by inhibiting conidial germination and filament formation through a process requiring PI3K signaling and the conidial fucose-specific lectin FleA. In the present study, we are looking to identify host factors and cellular partners involved in the BEC antifungal response and to define the molecular interactions underpinning FleA recognition. For this, we analyzed transcriptome of BECs infected with A. fumigatus in the presence or absence of the PI3K inhibitor LY294002. Functional involvement of candidate genes was assessed by siRNA knockdown and readouts of fungal filamentation (microscopic scoring and galactomannan release). FleA-interacting host proteins were identified by biotin-FleA affinity co-precipitation coupled to Tandem mass spectrometry, and validated by surface plasmon resonance and biolayer interferometry. The spatiotemporal dynamics of FleA and candidate partners were analyzed by confocal microscopy and proximity ligation assay. We demonstrated that BEC antifungal activity involves at least two complementary pathways: a PI3K/laminin-332 axis promoting conidial adhesion, and a FleA-dependent pathway engaging ITGB1 and MRC2 consistent with lectin uptake and trafficking toward LAMP1-positive compartments. These findings nominate FleA-host receptor interactions as attractive targets for anti-adhesive strategies against A. fumigatus.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: cell biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708144v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708144v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Millet, N., Moreau, A., Tarizzo, M., Marti, L., Varrot, A., Gillon, E., Richard, N., Pionneau, C., Chardonnet, S., Varet, H., Morichon, R., Guitard, J., Guillot, L., Balloy, V., Bigot, J.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.8, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Tissue composition shapes differential skeletal integration strategies during axolotl limb regeneration</h2>
            <p class="paper-summary">Limb regeneration requires not only rebuilding the missing structures, but also integrating them with the stump tissues. Osteoclast-mediated tissue resorption is essential for skeletal integration during regeneration. However, given the cellular and structural heterogeneity along the limb skeleton, it is unknown if skeletal tissue composition impacts resorption and, if so, how it is regulated. Here, we show that osteoclast-mediated skeletal resorption is primarily activated in amputations damaging calcified regions of the skeleton, but not in cartilaginous areas. Using a combination of spatial transcriptomics and bulk RNA sequencing, we found that amputations in calcified regions trigger the sustained expression of RANKL and the chemokine Loc138491483/Ccl24-like. We also demonstrate that Loc138491483/Ccl24-like is sufficient to induce osteoclast presence in non-resorbing amputations. Finally, our data suggests that the transcriptomic profile of the apical ectodermal cap is modified according to the underlying tissue types injured by the amputation. Overall, our work reveals that tissue composition at the amputation plane directs important adaptations of the regenerative program to the damaged tissues, particularly regarding integration strategies. These context-dependent responses will ultimately contribute to the near-seamless tissue integration of the regenerating axolotl limb regardless of the amputation position.</p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: biorxiv
              
              &middot; Categories: developmental biology
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708175v1.full.pdf" target="_blank" class="paper-link">
                    <i class="fas fa-file-pdf mr-1"></i> PDF
                </a>
                
                <a href="https://www.biorxiv.org/content/10.64898/2026.02.26.708175v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Aires, R., Keeley, S. D., Brandt, K., Carreira, M., Gunes, D. B., Savci, Y., Friedrich, U. A., Dahl, A., Aztekin, C., Sandoval-Guzman, T.</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.85, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">A Comparison of the Molecular Orbital (MO) and Orbital Exchange Methods for Calculating the Chemical Bond</h2>
            <p class="paper-summary"></p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: chemrxiv
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://doi.org/10.26434/chemrxiv.15000469/v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Paul Merrithew</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">FragBERTa: Exploring Fragment-based Molecular Representation Learning with SAFE</h2>
            <p class="paper-summary"></p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: chemrxiv
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://doi.org/10.26434/chemrxiv.15000476/v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Neerav Kaushal, Ajay MNV Penmatsa</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 8.950000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Local Solvent Ordering Drives Supramolecular Chirality Inversion</h2>
            <p class="paper-summary"></p>
            

            
            <p class="paper-authors" style="font-style: normal;">
              Source: chemrxiv
              
            </p>
            

            
            
            

            

            <div style="display: flex; gap: 0.75rem; flex-wrap: wrap;">
                
                <a href="https://doi.org/10.26434/chemrxiv.15000475/v1" target="_blank" class="paper-link">
                    <i class="fas fa-link mr-1"></i> Link
                </a>
            </div>
            
            <p class="paper-authors">Authors: Triza Pal, Akta Singh, Subinoy Adhikari, Jagannath Mondal, Debangshu Chaudhuri</p>
            
        </motion.div>
        
    </div>

    <footer class="footer">
        Generated on 2026-02-28 04:32:04 UTC. Powered by <a href="https://github.com/onion-liu" target="_blank">onion-liu</a>.
    </footer>

</body>
</html>