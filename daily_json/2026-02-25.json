[
  {
    "uid": "arxiv:2602.21137v1",
    "source": "arxiv",
    "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics",
    "abstract": "Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.",
    "url": "http://arxiv.org/abs/2602.21137v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21137v1",
    "authors": [
      "Joseph Raj Vishal",
      "Nagasiri Poluri",
      "Katha Naik",
      "Rutuja Patil",
      "Kashyap Hegde Kota",
      "Krishna Vinod",
      "Prithvi Jai Ramesh",
      "Mohammad Farhadi",
      "Yezhou Yang",
      "Bharatesh Chakravarthi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T17:33:12+00:00",
    "updated": "2026-02-24T17:33:12+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.21042v1",
    "source": "arxiv",
    "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
    "abstract": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
    "url": "http://arxiv.org/abs/2602.21042v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21042v1",
    "authors": [
      "Bonan Liu",
      "Zeyu Zhang",
      "Bingbing Meng",
      "Han Wang",
      "Hanshuo Zhang",
      "Chengping Wang",
      "Daji Ergu",
      "Ying Cai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T16:02:49+00:00",
    "updated": "2026-02-24T16:02:49+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.21015v1",
    "source": "arxiv",
    "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
    "abstract": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
    "url": "http://arxiv.org/abs/2602.21015v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21015v1",
    "authors": [
      "Yuhao Wu",
      "Maojia Song",
      "Yihuai Lan",
      "Lei Wang",
      "Zhiqiang Hu",
      "Yao Xiao",
      "Heng Zhou",
      "Weihua Zheng",
      "Dylan Raharja",
      "Soujanya Poria",
      "Roy Ka-Wei Lee"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T15:33:02+00:00",
    "updated": "2026-02-24T15:33:02+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20999v1",
    "source": "arxiv",
    "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models",
    "abstract": "Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.",
    "url": "http://arxiv.org/abs/2602.20999v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20999v1",
    "authors": [
      "Bowen Zheng",
      "Yongli Xiang",
      "Ziming Hong",
      "Zerong Lin",
      "Chaojian Yu",
      "Tongliang Liu",
      "Xinge You"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T15:20:01+00:00",
    "updated": "2026-02-24T15:20:01+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20994v1",
    "source": "arxiv",
    "title": "Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures",
    "abstract": "Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and provide qualitative or uncertain cues (``mild,'' ``possible''). Classical RSuper losses (e.g., sum volume consistency) can over-constrain or hallucinate unreported findings under such incompleteness, and are unable to utilize these hierarchical findings or exploit the priors of varied lesion types in a merged dataset. We explicitly parse the global quantitative and modality-wise qualitative findings and introduce a unified, one-sided, uncertainty-aware formulation (MS-RSuper) that: (i) aligns modality-specific qualitative cues (e.g., T1c enhancement, FLAIR edema) with their corresponding substructures using existence and absence losses; (ii) enforces one-sided lower-bounds for partial quantitative cues (e.g., largest lesion size, minimal multiplicity); and (iii) adds extra- vs. intra-axial anatomical priors to respect cohort differences. Certainty tokens scale penalties; missing cues are down-weighted. On 1238 report-labeled BraTS-MET/MEN scans, our MS-RSuper largely outperforms both a sparsely-supervised baseline and a naive RSuper method.",
    "url": "http://arxiv.org/abs/2602.20994v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20994v1",
    "authors": [
      "Yubin Ge",
      "Yongsong Huang",
      "Xiaofeng Liu"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-24T15:14:04+00:00",
    "updated": "2026-02-24T15:14:04+00:00",
    "extra": {
      "primary_category": "eess.IV"
    }
  },
  {
    "uid": "arxiv:2602.20989v1",
    "source": "arxiv",
    "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
    "abstract": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.",
    "url": "http://arxiv.org/abs/2602.20989v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20989v1",
    "authors": [
      "Zheng Gu",
      "Min Lu",
      "Zhida Sun",
      "Dani Lischinski",
      "Daniel Cohen-O",
      "Hui Huang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T15:10:31+00:00",
    "updated": "2026-02-24T15:10:31+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20981v1",
    "source": "arxiv",
    "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
    "abstract": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.",
    "url": "http://arxiv.org/abs/2602.20981v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20981v1",
    "authors": [
      "Christian Simon",
      "MAsato Ishii",
      "Wei-Yao Wang",
      "Koichi Saito",
      "Akio Hayakawa",
      "Dongseok Shim",
      "Zhi Zhong",
      "Shuyang Cui",
      "Shusuke Takahashi",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-24T15:01:39+00:00",
    "updated": "2026-02-24T15:01:39+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20980v1",
    "source": "arxiv",
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.",
    "url": "http://arxiv.org/abs/2602.20980v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20980v1",
    "authors": [
      "Yang Zhang",
      "Danyang Li",
      "Yuxuan Li",
      "Xin Zhang",
      "Tianyu Xie",
      "Mingming Cheng",
      "Xiang Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-24T15:01:30+00:00",
    "updated": "2026-02-24T15:01:30+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20972v1",
    "source": "arxiv",
    "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?",
    "abstract": "Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\\% to 80\\% of human performance, while achieving over 90\\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\\% to 80\\% of the difference.",
    "url": "http://arxiv.org/abs/2602.20972v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20972v1",
    "authors": [
      "Ming-Kun Xie",
      "Jia-Hao Xiao",
      "Zhiqiang Kou",
      "Zhongnian Li",
      "Gang Niu",
      "Masashi Sugiyama"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T14:53:16+00:00",
    "updated": "2026-02-24T14:53:16+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20951v1",
    "source": "arxiv",
    "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
    "abstract": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.",
    "url": "http://arxiv.org/abs/2602.20951v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20951v1",
    "authors": [
      "Jaehyun Park",
      "Minyoung Ahn",
      "Minkyu Kim",
      "Jonghyun Lee",
      "Jae-Gil Lee",
      "Dongmin Park"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-24T14:34:13+00:00",
    "updated": "2026-02-24T14:34:13+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20913v1",
    "source": "arxiv",
    "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
    "abstract": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1",
    "url": "http://arxiv.org/abs/2602.20913v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20913v1",
    "authors": [
      "Jihao Qiu",
      "Lingxi Xie",
      "Xinyue Huo",
      "Qi Tian",
      "Qixiang Ye"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T13:49:47+00:00",
    "updated": "2026-02-24T13:49:47+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20903v1",
    "source": "arxiv",
    "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
    "abstract": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
    "url": "http://arxiv.org/abs/2602.20903v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20903v1",
    "authors": [
      "Hanshen Zhu",
      "Yuliang Liu",
      "Xuecheng Wu",
      "An-Lan Wang",
      "Hao Feng",
      "Dingkang Yang",
      "Chao Feng",
      "Can Huang",
      "Jingqun Tang",
      "Xiang Bai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T13:40:23+00:00",
    "updated": "2026-02-24T13:40:23+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20880v1",
    "source": "arxiv",
    "title": "When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance",
    "abstract": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.",
    "url": "http://arxiv.org/abs/2602.20880v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20880v1",
    "authors": [
      "Yongli Xiang",
      "Ziming Hong",
      "Zhaoqing Wang",
      "Xiangyu Zhao",
      "Bo Han",
      "Tongliang Liu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T13:20:31+00:00",
    "updated": "2026-02-24T13:20:31+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20853v1",
    "source": "arxiv",
    "title": "On the Explainability of Vision-Language Models in Art History",
    "abstract": "Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.",
    "url": "http://arxiv.org/abs/2602.20853v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20853v1",
    "authors": [
      "Stefanie Schneider"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T12:53:28+00:00",
    "updated": "2026-02-24T12:53:28+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20839v1",
    "source": "arxiv",
    "title": "Training-Free Multi-Concept Image Editing",
    "abstract": "Editing images with diffusion models without training remains challenging. While recent optimisation-based methods achieve strong zero-shot edits from text, they struggle to preserve identity or capture details that language alone cannot express. Many visual concepts such as facial structure, material texture, or object geometry are impossible to express purely through text prompts alone. To address this gap, we introduce a training-free framework for concept-based image editing, which unifies Optimised DDS with LoRA-driven concept composition, where the training data of the LoRA represent the concept. Our approach enables combining and controlling multiple visual concepts directly within the diffusion process, integrating semantic guidance from text with low-level cues from pretrained concept adapters. We further refine DDS for stability and controllability through ordered timesteps, regularisation, and negative-prompt guidance. Quantitative and qualitative results demonstrate consistent improvements over existing training-free diffusion editing methods on InstructPix2Pix and ComposLoRA benchmarks. Code will be made publicly available.",
    "url": "http://arxiv.org/abs/2602.20839v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20839v1",
    "authors": [
      "Niki Foteinopoulou",
      "Ignas Budvytis",
      "Stephan Liwicki"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T12:27:51+00:00",
    "updated": "2026-02-24T12:27:51+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20818v1",
    "source": "arxiv",
    "title": "GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection",
    "abstract": "Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.",
    "url": "http://arxiv.org/abs/2602.20818v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20818v1",
    "authors": [
      "Yingying Guo",
      "Ke Zhang",
      "Zirong Zeng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T11:54:54+00:00",
    "updated": "2026-02-24T11:54:54+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20773v1",
    "source": "arxiv",
    "title": "Federated Learning for Cross-Modality Medical Image Segmentation via Augmentation-Driven Generalization",
    "abstract": "Artificial intelligence has emerged as a transformative tool in medical image analysis, yet developing robust and generalizable segmentation models remains difficult due to fragmented, privacy-constrained imaging data siloed across institutions. While federated learning (FL) enables collaborative model training without centralizing data, cross-modality domain shifts pose a critical challenge, particularly when models trained on one modality fail to generalize to another. Many existing solutions require paired multimodal data per patient or rely on complex architectures, both of which are impractical in real clinical settings. In this work, we consider a realistic FL scenario where each client holds single-modality data (CT or MRI), and systematically investigate augmentation strategies for cross-modality generalization. Using abdominal organ segmentation and whole-heart segmentation as representative multi-class and binary segmentation benchmarks, we evaluate convolution-based spatial augmentation, frequency-domain manipulation, domain-specific normalization, and global intensity nonlinear (GIN) augmentation. Our results show that GIN consistently outperforms alternatives in both centralized and federated settings by simulating cross-modality appearance variations while preserving anatomical structure. For the pancreas, Dice score improved from 0.073 to 0.437, a 498% gain. Our federated approach achieves 93-98% of centralized training accuracy, demonstrating strong cross-modality generalization without compromising data privacy, pointing toward feasible federated AI deployment across diverse healthcare systems.",
    "url": "http://arxiv.org/abs/2602.20773v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20773v1",
    "authors": [
      "Sachin Dudda Nagaraju",
      "Ashkan Moradi",
      "Bendik Skarre Abrahamsen",
      "Mattijs Elschot"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T11:13:01+00:00",
    "updated": "2026-02-24T11:13:01+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20752v1",
    "source": "arxiv",
    "title": "OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation",
    "abstract": "Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.",
    "url": "http://arxiv.org/abs/2602.20752v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20752v1",
    "authors": [
      "Tian Lan",
      "Lei Xu",
      "Zimu Yuan",
      "Shanggui Liu",
      "Jiajun Liu",
      "Jiaxin Liu",
      "Weilai Xiang",
      "Hongyu Yang",
      "Dong Jiang",
      "Jianxin Yin",
      "Dingyu Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-24T10:29:10+00:00",
    "updated": "2026-02-24T10:29:10+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20739v1",
    "source": "arxiv",
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "abstract": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
    "url": "http://arxiv.org/abs/2602.20739v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20739v1",
    "authors": [
      "Shitian Zhao",
      "Shaoheng Lin",
      "Ming Li",
      "Haoquan Zhang",
      "Wenshuo Peng",
      "Kaipeng Zhang",
      "Chen Wei"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-02-24T10:08:33+00:00",
    "updated": "2026-02-24T10:08:33+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20731v1",
    "source": "arxiv",
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "abstract": "Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.",
    "url": "http://arxiv.org/abs/2602.20731v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20731v1",
    "authors": [
      "Aram Davtyan",
      "Yusuf Sahin",
      "Yasaman Haghighi",
      "Sebastian Stapf",
      "Pablo Acuaviva",
      "Alexandre Alahi",
      "Paolo Favaro"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-24T09:53:50+00:00",
    "updated": "2026-02-24T09:53:50+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20725v1",
    "source": "arxiv",
    "title": "Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation",
    "abstract": "Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.",
    "url": "http://arxiv.org/abs/2602.20725v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20725v1",
    "authors": [
      "Junwei Shu",
      "Wenjie Liu",
      "Changgu Chen",
      "Hantang Liu",
      "Yang Li",
      "Changbo Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T09:44:12+00:00",
    "updated": "2026-02-24T09:44:12+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20721v1",
    "source": "arxiv",
    "title": "CleanStyle: Plug-and-Play Style Conditioning Purification for Text-to-Image Stylization",
    "abstract": "Style transfer in diffusion models enables controllable visual generation by injecting the style of a reference image. However, recent encoder-based methods, while efficient and tuning-free, often suffer from content leakage, where semantic elements from the style image undesirably appear in the output, impairing prompt fidelity and stylistic consistency. In this work, we introduce CleanStyle, a plug-and-play framework that filters out content-related noise from the style embedding without retraining. Motivated by empirical analysis, we observe that such leakage predominantly stems from the tail components of the style embedding, which are isolated via Singular Value Decomposition (SVD). To address this, we propose CleanStyleSVD (CS-SVD), which dynamically suppresses tail components using a time-aware exponential schedule, providing clean, style-preserving conditional embeddings throughout the denoising process. Furthermore, we present Style-Specific Classifier-Free Guidance (SS-CFG), which reuses the suppressed tail components to construct style-aware unconditional inputs. Unlike conventional methods that use generic negative embeddings (e.g., zero vectors), SS-CFG introduces targeted negative signals that reflect style-specific but prompt-irrelevant visual elements. This enables the model to effectively suppress these distracting patterns during generation, thereby improving prompt fidelity and enhancing the overall visual quality of stylized outputs. Our approach is lightweight, interpretable, and can be seamlessly integrated into existing encoder-based diffusion models without retraining. Extensive experiments demonstrate that CleanStyle substantially reduces content leakage, improves stylization quality and improves prompt alignment across a wide range of style references and prompts.",
    "url": "http://arxiv.org/abs/2602.20721v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20721v1",
    "authors": [
      "Xiaoman Feng",
      "Mingkun Lei",
      "Yang Wang",
      "Dingwen Fu",
      "Chi Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T09:33:05+00:00",
    "updated": "2026-02-24T09:33:05+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20685v1",
    "source": "arxiv",
    "title": "RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation",
    "abstract": "World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Pl\u00fccker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.",
    "url": "http://arxiv.org/abs/2602.20685v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20685v1",
    "authors": [
      "Yichen Xie",
      "Chensheng Peng",
      "Mazen Abdelfattah",
      "Yihan Hu",
      "Jiezhi Yang",
      "Eric Higgins",
      "Ryan Brigden",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T08:41:40+00:00",
    "updated": "2026-02-24T08:41:40+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20672v1",
    "source": "arxiv",
    "title": "BBQ-to-Image: Numeric Bounding Box and Qolor Control in Large-Scale Text-to-Image Models",
    "abstract": "Text-to-image models have rapidly advanced in realism and controllability, with recent approaches leveraging long, detailed captions to support fine-grained generation. However, a fundamental parametric gap remains: existing models rely on descriptive language, whereas professional workflows require precise numeric control over object location, size, and color. In this work, we introduce BBQ, a large-scale text-to-image model that directly conditions on numeric bounding boxes and RGB triplets within a unified structured-text framework. We obtain precise spatial and chromatic control by training on captions enriched with parametric annotations, without architectural modifications or inference-time optimization. This also enables intuitive user interfaces such as object dragging and color pickers, replacing ambiguous iterative prompting with precise, familiar controls. Across comprehensive evaluations, BBQ achieves strong box alignment and improves RGB color fidelity over state-of-the-art baselines. More broadly, our results support a new paradigm in which user intent is translated into an intermediate structured language, consumed by a flow-based transformer acting as a renderer and naturally accommodating numeric parameters.",
    "url": "http://arxiv.org/abs/2602.20672v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20672v1",
    "authors": [
      "Eliran Kachlon",
      "Alexander Visheratin",
      "Nimrod Sarid",
      "Tal Hacham",
      "Eyal Gutflaish",
      "Saar Huberman",
      "Hezi Zisman",
      "David Ruppin",
      "Ron Mokady"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T08:22:42+00:00",
    "updated": "2026-02-24T08:22:42+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20673v1",
    "source": "arxiv",
    "title": "GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio",
    "abstract": "A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.",
    "url": "http://arxiv.org/abs/2602.20673v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20673v1",
    "authors": [
      "Hao Zhang",
      "Lue Fan",
      "Qitai Wang",
      "Wenbo Li",
      "Zehuan Wu",
      "Lewei Lu",
      "Zhaoxiang Zhang",
      "Hongsheng Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T08:22:42+00:00",
    "updated": "2026-02-24T08:22:42+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20666v1",
    "source": "arxiv",
    "title": "BoxSplitGen: A Generative Model for 3D Part Bounding Boxes in Varying Granularity",
    "abstract": "Human creativity follows a perceptual process, moving from abstract ideas to finer details during creation. While 3D generative models have advanced dramatically, models specifically designed to assist human imagination in 3D creation -- particularly for detailing abstractions from coarse to fine -- have not been explored. We propose a framework that enables intuitive and interactive 3D shape generation by iteratively splitting bounding boxes to refine the set of bounding boxes. The main technical components of our framework are two generative models: the box-splitting generative model and the box-to-shape generative model. The first model, named BoxSplitGen, generates a collection of 3D part bounding boxes with varying granularity by iteratively splitting coarse bounding boxes. It utilizes part bounding boxes created through agglomerative merging and learns the reverse of the merging process -- the splitting sequences. The model consists of two main components: the first learns the categorical distribution of the box to be split, and the second learns the distribution of the two new boxes, given the set of boxes and the indication of which box to split. The second model, the box-to-shape generative model, is trained by leveraging the 3D shape priors learned by an existing 3D diffusion model while adapting the model to incorporate bounding box conditioning. In our experiments, we demonstrate that the box-splitting generative model outperforms token prediction models and the inpainting approach with an unconditional diffusion model. Also, we show that our box-to-shape model, based on a state-of-the-art 3D diffusion model, provides superior results compared to a previous model.",
    "url": "http://arxiv.org/abs/2602.20666v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20666v1",
    "authors": [
      "Juil Koo",
      "Wei-Tung Lin",
      "Chanho Park",
      "Chanhyeok Park",
      "Minhyuk Sung"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T08:15:25+00:00",
    "updated": "2026-02-24T08:15:25+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20664v1",
    "source": "arxiv",
    "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?",
    "abstract": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.",
    "url": "http://arxiv.org/abs/2602.20664v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20664v1",
    "authors": [
      "Hailong Yan",
      "Shice Liu",
      "Tao Wang",
      "Xiangtao Zhang",
      "Yijie Zhong",
      "Jinwei Chen",
      "Le Zhang",
      "Bo Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T08:14:24+00:00",
    "updated": "2026-02-24T08:14:24+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20583v1",
    "source": "arxiv",
    "title": "PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models",
    "abstract": "Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.",
    "url": "http://arxiv.org/abs/2602.20583v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20583v1",
    "authors": [
      "Wonyong Seo",
      "Jaeho Moon",
      "Jaehyup Lee",
      "Soo Ye Kim",
      "Munchurl Kim"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T06:11:08+00:00",
    "updated": "2026-02-24T06:11:08+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20577v1",
    "source": "arxiv",
    "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion",
    "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.",
    "url": "http://arxiv.org/abs/2602.20577v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20577v1",
    "authors": [
      "Jiaru Zhang",
      "Manav Gagvani",
      "Can Cui",
      "Juntong Peng",
      "Ruqi Zhang",
      "Ziran Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T05:59:10+00:00",
    "updated": "2026-02-24T05:59:10+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20575v1",
    "source": "arxiv",
    "title": "An interactive enhanced driving dataset for autonomous driving",
    "abstract": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.",
    "url": "http://arxiv.org/abs/2602.20575v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20575v1",
    "authors": [
      "Haojie Feng",
      "Peizhi Zhang",
      "Mengjie Tian",
      "Xinrui Zhang",
      "Zhuoren Li",
      "Junpeng Huang",
      "Xiurong Wang",
      "Junfan Zhu",
      "Jianzhou Wang",
      "Dongxiao Yin",
      "Lu Xiong"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T05:57:18+00:00",
    "updated": "2026-02-24T05:57:18+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20569v1",
    "source": "arxiv",
    "title": "AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents",
    "abstract": "We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.",
    "url": "http://arxiv.org/abs/2602.20569v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20569v1",
    "authors": [
      "Jiaqi Wu",
      "Yuchen Zhou",
      "Muduo Xu",
      "Zisheng Liang",
      "Simiao Ren",
      "Jiayu Xue",
      "Meige Yang",
      "Siying Chen",
      "Jingheng Huan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T05:37:35+00:00",
    "updated": "2026-02-24T05:37:35+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20549v1",
    "source": "arxiv",
    "title": "Sample-efficient evidence estimation of score based priors for model selection",
    "abstract": "The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.",
    "url": "http://arxiv.org/abs/2602.20549v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20549v1",
    "authors": [
      "Frederic Wang",
      "Katherine L. Bouman"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ME"
    ],
    "published": "2026-02-24T05:06:46+00:00",
    "updated": "2026-02-24T05:06:46+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20531v1",
    "source": "arxiv",
    "title": "A Lightweight Vision-Language Fusion Framework for Predicting App Ratings from User Interfaces and Metadata",
    "abstract": "App ratings are among the most significant indicators of the quality, usability, and overall user satisfaction of mobile applications. However, existing app rating prediction models are largely limited to textual data or user interface (UI) features, overlooking the importance of jointly leveraging UI and semantic information. To address these limitations, this study proposes a lightweight vision--language framework that integrates both mobile UI and semantic information for app rating prediction. The framework combines MobileNetV3 to extract visual features from UI layouts and DistilBERT to extract textual features. These multimodal features are fused through a gated fusion module with Swish activations, followed by a multilayer perceptron (MLP) regression head. The proposed model is evaluated using mean absolute error (MAE), root mean square error (RMSE), mean squared error (MSE), coefficient of determination (R2), and Pearson correlation. After training for 20 epochs, the model achieves an MAE of 0.1060, an RMSE of 0.1433, an MSE of 0.0205, an R2 of 0.8529, and a Pearson correlation of 0.9251. Extensive ablation studies further demonstrate the effectiveness of different combinations of visual and textual encoders. Overall, the proposed lightweight framework provides valuable insights for developers and end users, supports sustainable app development, and enables efficient deployment on edge devices.",
    "url": "http://arxiv.org/abs/2602.20531v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20531v1",
    "authors": [
      "Azrin Sultana",
      "Firoz Ahmed"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T04:17:50+00:00",
    "updated": "2026-02-24T04:17:50+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20520v1",
    "source": "arxiv",
    "title": "How Do Inpainting Artifacts Propagate to Language?",
    "abstract": "We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.",
    "url": "http://arxiv.org/abs/2602.20520v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20520v1",
    "authors": [
      "Pratham Yashwante",
      "Davit Abrahamyan",
      "Shresth Grover",
      "Sukruth Rao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-24T03:46:33+00:00",
    "updated": "2026-02-24T03:46:33+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20501v1",
    "source": "arxiv",
    "title": "Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models",
    "abstract": "What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.",
    "url": "http://arxiv.org/abs/2602.20501v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20501v1",
    "authors": [
      "Qing Zhang",
      "Xuesong Li",
      "Jing Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T02:59:15+00:00",
    "updated": "2026-02-24T02:59:15+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20497v1",
    "source": "arxiv",
    "title": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration",
    "abstract": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.",
    "url": "http://arxiv.org/abs/2602.20497v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20497v1",
    "authors": [
      "Peiliang Cai",
      "Jiacheng Liu",
      "Haowen Xu",
      "Xinyu Wang",
      "Chang Zou",
      "Linfeng Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-24T02:53:28+00:00",
    "updated": "2026-02-24T02:53:28+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20476v1",
    "source": "arxiv",
    "title": "SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens",
    "abstract": "Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent (\"walk to the couch\") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.",
    "url": "http://arxiv.org/abs/2602.20476v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20476v1",
    "authors": [
      "Anindita Ghosh",
      "Vladislav Golyanik",
      "Taku Komura",
      "Philipp Slusallek",
      "Christian Theobalt",
      "Rishabh Dabral"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-24T02:09:12+00:00",
    "updated": "2026-02-24T02:09:12+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20417v1",
    "source": "arxiv",
    "title": "gQIR: Generative Quanta Image Reconstruction",
    "abstract": "Capturing high-quality images from only a few detected photons is a fundamental challenge in computational imaging. Single-photon avalanche diode (SPAD) sensors promise high-quality imaging in regimes where conventional cameras fail, but raw \\emph{quanta frames} contain only sparse, noisy, binary photon detections. Recovering a coherent image from a burst of such frames requires handling alignment, denoising, and demosaicing (for color) under noise statistics far outside those assumed by standard restoration pipelines or modern generative models. We present an approach that adapts large text-to-image latent diffusion models to the photon-limited domain of quanta burst imaging. Our method leverages the structural and semantic priors of internet-scale diffusion models while introducing mechanisms to handle Bernoulli photon statistics. By integrating latent-space restoration with burst-level spatio-temporal reasoning, our approach produces reconstructions that are both photometrically faithful and perceptually pleasing, even under high-speed motion. We evaluate the method on synthetic benchmarks and new real-world datasets, including the first color SPAD burst dataset and a challenging \\textit{Deforming (XD)} video benchmark. Across all settings, the approach substantially improves perceptual quality over classical and modern learning-based baselines, demonstrating the promise of adapting large generative priors to extreme photon-limited sensing. Code at \\href{https://github.com/Aryan-Garg/gQIR}{https://github.com/Aryan-Garg/gQIR}.",
    "url": "http://arxiv.org/abs/2602.20417v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20417v1",
    "authors": [
      "Aryan Garg",
      "Sizhuo Ma",
      "Mohit Gupta"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-23T23:33:00+00:00",
    "updated": "2026-02-23T23:33:00+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20412v1",
    "source": "arxiv",
    "title": "SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images",
    "abstract": "The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\\% accuracy and +69.62\\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.",
    "url": "http://arxiv.org/abs/2602.20412v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20412v1",
    "authors": [
      "Aayush Dhakal",
      "Subash Khanal",
      "Srikumar Sastry",
      "Jacob Arndt",
      "Philipe Ambrozio Dias",
      "Dalton Lunga",
      "Nathan Jacobs"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-23T23:22:41+00:00",
    "updated": "2026-02-23T23:22:41+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20360v1",
    "source": "arxiv",
    "title": "Momentum Guidance: Plug-and-Play Guidance for Flow Models",
    "abstract": "Flow-based generative models have become a strong framework for high-quality generative modeling, yet pretrained models are rarely used in their vanilla conditional form: conditional samples without guidance often appear diffuse and lack fine-grained detail due to the smoothing effects of neural networks. Existing guidance techniques such as classifier-free guidance (CFG) improve fidelity but double the inference cost and typically reduce sample diversity. We introduce Momentum Guidance (MG), a new dimension of guidance that leverages the ODE trajectory itself. MG extrapolates the current velocity using an exponential moving average of past velocities and preserves the standard one-evaluation-per-step cost. It matches the effect of standard guidance without extra computation and can further improve quality when combined with CFG. Experiments demonstrate MG's effectiveness across benchmarks. Specifically, on ImageNet-256, MG achieves average improvements in FID of 36.68% without CFG and 25.52% with CFG across various sampling settings, attaining an FID of 1.597 at 64 sampling steps. Evaluations on large flow-based models like Stable Diffusion 3 and FLUX.1-dev further confirm consistent quality enhancements across standard metrics.",
    "url": "http://arxiv.org/abs/2602.20360v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20360v1",
    "authors": [
      "Runlong Liao",
      "Jian Yu",
      "Baiyu Su",
      "Chi Zhang",
      "Lizhang Chen",
      "Qiang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2026-02-23T21:06:35+00:00",
    "updated": "2026-02-23T21:06:35+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20354v1",
    "source": "arxiv",
    "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism",
    "abstract": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.",
    "url": "http://arxiv.org/abs/2602.20354v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20354v1",
    "authors": [
      "Bhavik Chandna",
      "Kelsey R. Allen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-23T21:00:48+00:00",
    "updated": "2026-02-23T21:00:48+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20351v1",
    "source": "arxiv",
    "title": "BiRQA: Bidirectional Robust Quality Assessment for Images",
    "abstract": "Full-Reference image quality assessment (FR IQA) is important for image compression, restoration and generative modeling, yet current neural metrics remain slow and vulnerable to adversarial perturbations. We present BiRQA, a compact FR IQA metric model that processes four fast complementary features within a bidirectional multiscale pyramid. A bottom-up attention module injects fine-scale cues into coarse levels through an uncertainty-aware gate, while a top-down cross-gating block routes semantic context back to high resolution. To enhance robustness, we introduce Anchored Adversarial Training, a theoretically grounded strategy that uses clean \"anchor\" samples and a ranking loss to bound pointwise prediction error under attacks. On five public FR IQA benchmarks BiRQA outperforms or matches the previous state of the art (SOTA) while running ~3x faster than previous SOTA models. Under unseen white-box attacks it lifts SROCC from 0.30-0.57 to 0.60-0.84 on KADID-10k, demonstrating substantial robustness gains. To our knowledge, BiRQA is the only FR IQA model combining competitive accuracy with real-time throughput and strong adversarial resilience.",
    "url": "http://arxiv.org/abs/2602.20351v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20351v1",
    "authors": [
      "Aleksandr Gushchin",
      "Dmitriy S. Vatolin",
      "Anastasia Antsiferova"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-23T20:52:56+00:00",
    "updated": "2026-02-23T20:52:56+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20330v1",
    "source": "arxiv",
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "abstract": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.",
    "url": "http://arxiv.org/abs/2602.20330v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20330v1",
    "authors": [
      "Jingcheng Yang",
      "Tianhu Xiong",
      "Shengyi Qian",
      "Klara Nahrstedt",
      "Mingyuan Wu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-23T20:26:45+00:00",
    "updated": "2026-02-23T20:26:45+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20328v1",
    "source": "arxiv",
    "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems",
    "abstract": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.",
    "url": "http://arxiv.org/abs/2602.20328v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20328v1",
    "authors": [
      "Romario Gualdr\u00f3n-Hurtado",
      "Roman Jacome",
      "Rafael S. Suarez",
      "Henry Arguello"
    ],
    "categories": [
      "cs.CV",
      "eess.IV",
      "math.OC"
    ],
    "published": "2026-02-23T20:24:00+00:00",
    "updated": "2026-02-23T20:24:00+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20161v2",
    "source": "arxiv",
    "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "abstract": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
    "url": "http://arxiv.org/abs/2602.20161v2",
    "pdf_url": "https://arxiv.org/pdf/2602.20161v2",
    "authors": [
      "Abdelrahman Shaker",
      "Ahmed Heakl",
      "Jaseel Muhammad",
      "Ritesh Thawkar",
      "Omkar Thawakar",
      "Senmao Li",
      "Hisham Cholakkal",
      "Ian Reid",
      "Eric P. Xing",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-23T18:59:58+00:00",
    "updated": "2026-02-24T10:29:43+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.20119v1",
    "source": "arxiv",
    "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
    "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
    "url": "http://arxiv.org/abs/2602.20119v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20119v1",
    "authors": [
      "Jiahui Fu",
      "Junyu Nan",
      "Lingfeng Sun",
      "Hongyu Li",
      "Jianing Qian",
      "Jennifer L. Barry",
      "Kris Kitani",
      "George Konidaris"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-02-23T18:35:18+00:00",
    "updated": "2026-02-23T18:35:18+00:00",
    "extra": {
      "primary_category": "cs.RO"
    }
  },
  {
    "uid": "arxiv:2602.20114v1",
    "source": "arxiv",
    "title": "Benchmarking Unlearning for Vision Transformers",
    "abstract": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
    "url": "http://arxiv.org/abs/2602.20114v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20114v1",
    "authors": [
      "Kairan Zhao",
      "Iurie Luca",
      "Peter Triantafillou"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-23T18:33:16+00:00",
    "updated": "2026-02-23T18:33:16+00:00",
    "extra": {
      "primary_category": "cs.CV"
    }
  },
  {
    "uid": "arxiv:2602.21154v1",
    "source": "arxiv",
    "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning",
    "abstract": "Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.",
    "url": "http://arxiv.org/abs/2602.21154v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21154v1",
    "authors": [
      "Ziwei Niu",
      "Hao Sun",
      "Shujun Bian",
      "Xihong Yang",
      "Lanfen Lin",
      "Yuxin Liu",
      "Yueming Jin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-24T17:59:21+00:00",
    "updated": "2026-02-24T17:59:21+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.21092v1",
    "source": "arxiv",
    "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology",
    "abstract": "Curvature notions on graphs provide a theoretical description of graph topology, highlighting bottlenecks and denser connected regions. Artifacts of the message passing paradigm in Graph Neural Networks, such as oversmoothing and oversquashing, have been attributed to these regions. However, it remains unclear how the topology of a graph interacts with the learned preferences of GNNs. Through Massive Activations, which correspond to extreme edge activation values in Graph Transformers, we probe this correspondence. Our findings on synthetic graphs and molecular benchmarks reveal that MAs do not preferentially concentrate on curvature extremes, despite their theoretical link to information flow. On the Long Range Graph Benchmark, we identify a systemic \\textit{curvature shift}: global attention mechanisms exacerbate topological bottlenecks, drastically increasing the prevalence of negative curvature. Our work reframes curvature as a diagnostic probe for understanding when and why graph learning fails.",
    "url": "http://arxiv.org/abs/2602.21092v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21092v1",
    "authors": [
      "Floriano Tori",
      "Lorenzo Bini",
      "Marco Sorbi",
      "St\u00e9phane Marchand-Maillet",
      "Vincent Ginis"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-24T16:52:36+00:00",
    "updated": "2026-02-24T16:52:36+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20918v1",
    "source": "arxiv",
    "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts",
    "abstract": "Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.",
    "url": "http://arxiv.org/abs/2602.20918v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20918v1",
    "authors": [
      "Hyewon Jang",
      "Nikolai Ilinykh",
      "Sharid Lo\u00e1iciga",
      "Jey Han Lau",
      "Shalom Lappin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-24T13:54:38+00:00",
    "updated": "2026-02-24T13:54:38+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20877v1",
    "source": "arxiv",
    "title": "E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications",
    "abstract": "Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.",
    "url": "http://arxiv.org/abs/2602.20877v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20877v1",
    "authors": [
      "Jiwoo Kang",
      "Yeon-Chang Lee"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2026-02-24T13:19:42+00:00",
    "updated": "2026-02-24T13:19:42+00:00",
    "extra": {
      "primary_category": "cs.IR"
    }
  },
  {
    "uid": "arxiv:2602.20723v1",
    "source": "arxiv",
    "title": "Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation",
    "abstract": "Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fusion both crucial and challenging. Existing approaches often rely on shared fusion pathways, leading to entangled representations and modality imbalance. To address these issues, we propose \\textbf{MAGNET}, a \\textbf{M}odality-Guided Mixture of \\textbf{A}daptive \\textbf{G}raph Experts \\textbf{N}etwork with Progressive \\textbf{E}ntropy-\\textbf{T}riggered Routing for Multimodal Recommendation, designed to enhance controllability, stability, and interpretability in multimodal fusion. MAGNET couples interaction-conditioned expert routing with structure-aware graph augmentation, so that both \\emph{what} to fuse and \\emph{how} to fuse are explicitly controlled and interpretable. At the representation level, a dual-view graph learning module augments the interaction graph with content-induced edges, improving coverage for sparse and long-tail items while preserving collaborative structure via parallel encoding and lightweight fusion. At the fusion level, MAGNET employs structured experts with explicit modality roles -- dominant, balanced, and complementary -- enabling a more interpretable and adaptive combination of behavioral, visual, and textual cues. To further stabilize sparse routing and prevent expert collapse, we introduce a two-stage entropy-weighting mechanism that monitors routing entropy. This mechanism automatically transitions training from an early coverage-oriented regime to a later specialization-oriented regime, progressively balancing expert utilization and routing confidence. Extensive experiments on public benchmarks demonstrate consistent improvements over strong baselines.",
    "url": "http://arxiv.org/abs/2602.20723v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20723v1",
    "authors": [
      "Ji Dai",
      "Quan Fang",
      "Dengsheng Cai"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-24T09:36:45+00:00",
    "updated": "2026-02-24T09:36:45+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20670v1",
    "source": "arxiv",
    "title": "CAMEL: Confidence-Gated Reflection for Reward Modeling",
    "abstract": "Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.",
    "url": "http://arxiv.org/abs/2602.20670v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20670v1",
    "authors": [
      "Zirui Zhu",
      "Hailun Xu",
      "Yang Luo",
      "Yong Liu",
      "Kanchan Sarkar",
      "Kun Xu",
      "Yang You"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-24T08:20:08+00:00",
    "updated": "2026-02-24T08:20:08+00:00",
    "extra": {
      "primary_category": "cs.CL"
    }
  },
  {
    "uid": "arxiv:2602.20659v1",
    "source": "arxiv",
    "title": "Recursive Belief Vision Language Model",
    "abstract": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to \u03c00. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.",
    "url": "http://arxiv.org/abs/2602.20659v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20659v1",
    "authors": [
      "Vaidehi Bagaria",
      "Bijo Sebastian",
      "Nirav Patel"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-24T08:02:16+00:00",
    "updated": "2026-02-24T08:02:16+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20643v1",
    "source": "arxiv",
    "title": "TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer",
    "abstract": "Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.",
    "url": "http://arxiv.org/abs/2602.20643v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20643v1",
    "authors": [
      "Jiawei Wang",
      "Chuang Yang",
      "Jiawei Yong",
      "Xiaohang Xu",
      "Hongjun Wang",
      "Noboru Koshizuka",
      "Shintaro Fukushima",
      "Ryosuke Shibasaki",
      "Renhe Jiang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-24T07:44:19+00:00",
    "updated": "2026-02-24T07:44:19+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20624v1",
    "source": "arxiv",
    "title": "Physics-based phenomenological characterization of cross-modal bias in multimodal models",
    "abstract": "The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as \"treat like cases as like\") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.",
    "url": "http://arxiv.org/abs/2602.20624v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20624v1",
    "authors": [
      "Hyeongmo Kim",
      "Sohyun Kang",
      "Yerin Choi",
      "Seungyeon Ji",
      "Junhyuk Woo",
      "Hyunsuk Chung",
      "Soyeon Caren Han",
      "Kyungreem Han"
    ],
    "categories": [
      "cs.AI",
      "cond-mat.stat-mech"
    ],
    "published": "2026-02-24T07:21:08+00:00",
    "updated": "2026-02-24T07:21:08+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20558v1",
    "source": "arxiv",
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "abstract": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.",
    "url": "http://arxiv.org/abs/2602.20558v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20558v1",
    "authors": [
      "Yucheng Shi",
      "Ying Li",
      "Yu Wang",
      "Yesu Feng",
      "Arjun Rao",
      "Rein Houthooft",
      "Shradha Sehgal",
      "Jin Wang",
      "Hao Zhen",
      "Ninghao Liu",
      "Linas Baltrunas"
    ],
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "published": "2026-02-24T05:15:24+00:00",
    "updated": "2026-02-24T05:15:24+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20517v1",
    "source": "arxiv",
    "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
    "abstract": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.",
    "url": "http://arxiv.org/abs/2602.20517v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20517v1",
    "authors": [
      "Rakshit Trivedi",
      "Kartik Sharma",
      "David C Parkes"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-24T03:37:42+00:00",
    "updated": "2026-02-24T03:37:42+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20480v1",
    "source": "arxiv",
    "title": "VINA: Variational Invertible Neural Architectures",
    "abstract": "The distinctive architectural features of normalizing flows (NFs), notably bijectivity and tractable Jacobians, make them well-suited for generative modeling. Invertible neural networks (INNs) build on these principles to address supervised inverse problems, enabling direct modeling of both forward and inverse mappings. In this paper, we revisit these architectures from both theoretical and practical perspectives and address a key gap in the literature: the lack of theoretical guarantees on approximation quality under realistic assumptions, whether for posterior inference in INNs or for generative modeling with NFs.\n  We introduce a unified framework for INNs and NFs based on variational unsupervised loss functions, inspired by analogous formulations in related areas such as generative adversarial networks (GANs) and the Precision-Recall divergence for training normalizing flows. Within this framework, we derive theoretical performance guarantees, quantifying posterior accuracy for INNs and distributional accuracy for NFs, under assumptions that are weaker and more practically realistic than those used in prior work.\n  Building on these theoretical results, we conduct extensive case studies to distill general design principles and practical guidelines. We conclude by demonstrating the effectiveness of our approach on a realistic ocean-acoustic inversion problem.",
    "url": "http://arxiv.org/abs/2602.20480v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20480v1",
    "authors": [
      "Shubhanshu Shekhar",
      "Mohammad Javad Khojasteh",
      "Ananya Acharya",
      "Tony Tohme",
      "Kamal Youcef-Toumi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-24T02:16:44+00:00",
    "updated": "2026-02-24T02:16:44+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20459v1",
    "source": "arxiv",
    "title": "PreScience: A Benchmark for Forecasting Scientific Contributions",
    "abstract": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.",
    "url": "http://arxiv.org/abs/2602.20459v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20459v1",
    "authors": [
      "Anirudh Ajith",
      "Amanpreet Singh",
      "Jay DeYoung",
      "Nadav Kunievsky",
      "Austin C. Kozlowski",
      "Oyvind Tafjord",
      "James Evans",
      "Daniel S. Weld",
      "Tom Hope",
      "Doug Downey"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-24T01:37:53+00:00",
    "updated": "2026-02-24T01:37:53+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20449v1",
    "source": "arxiv",
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "abstract": "Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.",
    "url": "http://arxiv.org/abs/2602.20449v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20449v1",
    "authors": [
      "Anna Hart",
      "Chi Han",
      "Jeonghwan Kim",
      "Huimin Zhao",
      "Heng Ji"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "q-bio.BM"
    ],
    "published": "2026-02-24T01:18:30+00:00",
    "updated": "2026-02-24T01:18:30+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20422v1",
    "source": "arxiv",
    "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning",
    "abstract": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.",
    "url": "http://arxiv.org/abs/2602.20422v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20422v1",
    "authors": [
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-23T23:41:22+00:00",
    "updated": "2026-02-23T23:41:22+00:00",
    "extra": {
      "primary_category": "cs.AI"
    }
  },
  {
    "uid": "arxiv:2602.20344v1",
    "source": "arxiv",
    "title": "Hierarchical Molecular Representation Learning via Fragment-Based Self-Supervised Embedding Prediction",
    "abstract": "Graph self-supervised learning (GSSL) has demonstrated strong potential for generating expressive graph embeddings without the need for human annotations, making it particularly valuable in domains with high labeling costs such as molecular graph analysis. However, existing GSSL methods mostly focus on node- or edge-level information, often ignoring chemically relevant substructures which strongly influence molecular properties. In this work, we propose Graph Semantic Predictive Network (GraSPNet), a hierarchical self-supervised framework that explicitly models both atomic-level and fragment-level semantics. GraSPNet decomposes molecular graphs into chemically meaningful fragments without predefined vocabularies and learns node- and fragment-level representations through multi-level message passing with masked semantic prediction at both levels. This hierarchical semantic supervision enables GraSPNet to learn multi-resolution structural information that is both expressive and transferable. Extensive experiments on multiple molecular property prediction benchmarks demonstrate that GraSPNet learns chemically meaningful representations and consistently outperforms state-of-the-art GSSL methods in transfer learning settings.",
    "url": "http://arxiv.org/abs/2602.20344v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20344v1",
    "authors": [
      "Jiele Wu",
      "Haozhe Ma",
      "Zhihan Guo",
      "Thanh Vinh Vo",
      "Tze Yun Leong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "published": "2026-02-23T20:41:44+00:00",
    "updated": "2026-02-23T20:41:44+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20306v1",
    "source": "arxiv",
    "title": "Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation",
    "abstract": "High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs.",
    "url": "http://arxiv.org/abs/2602.20306v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20306v1",
    "authors": [
      "Davide Carrara",
      "Marc Hirschvogel",
      "Francesca Bonizzoni",
      "Stefano Pagani",
      "Simone Pezzuto",
      "Francesco Regazzoni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "published": "2026-02-23T19:40:55+00:00",
    "updated": "2026-02-23T19:40:55+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20271v1",
    "source": "arxiv",
    "title": "Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning",
    "abstract": "Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.",
    "url": "http://arxiv.org/abs/2602.20271v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20271v1",
    "authors": [
      "Stefan Faulkner",
      "Reza Zandehshahvar",
      "Vahid Eghbal Akhlaghi",
      "Sebastien Ouellet",
      "Carsten Jordan",
      "Pascal Van Hentenryck"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.AP"
    ],
    "published": "2026-02-23T19:01:03+00:00",
    "updated": "2026-02-23T19:01:03+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20113v1",
    "source": "arxiv",
    "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion",
    "abstract": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.",
    "url": "http://arxiv.org/abs/2602.20113v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20113v1",
    "authors": [
      "Yisi Liu",
      "Nicholas Lee",
      "Gopala Anumanchipalli"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2026-02-23T18:32:59+00:00",
    "updated": "2026-02-23T18:32:59+00:00",
    "extra": {
      "primary_category": "cs.SD"
    }
  },
  {
    "uid": "arxiv:2602.21133v1",
    "source": "arxiv",
    "title": "SOM-VQ: Topology-Aware Tokenization for Interactive Generative Models",
    "abstract": "Vector-quantized representations enable powerful discrete generative models but lack semantic structure in token space, limiting interpretable human control. We introduce SOM-VQ, a tokenization method that combines vector quantization with Self-Organizing Maps to learn discrete codebooks with explicit low-dimensional topology. Unlike standard VQ-VAE, SOM-VQ uses topology-aware updates that preserve neighborhood structure: nearby tokens on a learned grid correspond to semantically similar states, enabling direct geometric manipulation of the latent space. We demonstrate that SOM-VQ produces more learnable token sequences in the evaluated domains while providing an explicit navigable geometry in code space. Critically, the topological organization enables intuitive human-in-the-loop control: users can steer generation by manipulating distances in token space, achieving semantic alignment without frame-level constraints. We focus on human motion generation - a domain where kinematic structure, smooth temporal continuity, and interactive use cases (choreography, rehabilitation, HCI) make topology-aware control especially natural - demonstrating controlled divergence and convergence from reference sequences through simple grid-based sampling. SOM-VQ provides a general framework for interpretable discrete representations applicable to music, gesture, and other interactive generative domains.",
    "url": "http://arxiv.org/abs/2602.21133v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21133v1",
    "authors": [
      "Alessandro Londei",
      "Denise Lanzieri",
      "Matteo Benati"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-24T17:29:04+00:00",
    "updated": "2026-02-24T17:29:04+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20758v1",
    "source": "arxiv",
    "title": "Deep unfolding of MCMC kernels: scalable, modular & explainable GANs for high-dimensional posterior sampling",
    "abstract": "Markov chain Monte Carlo (MCMC) methods are fundamental to Bayesian computation, but can be computationally intensive, especially in high-dimensional settings. Push-forward generative models, such as generative adversarial networks (GANs), variational auto-encoders and normalising flows offer a computationally efficient alternative for posterior sampling. However, push-forward models are opaque as they lack the modularity of Bayes Theorem, leading to poor generalisation with respect to changes in the likelihood function. In this work, we introduce a novel approach to GAN architecture design by applying deep unfolding to Langevin MCMC algorithms. This paradigm maps fixed-step iterative algorithms onto modular neural networks, yielding architectures that are both flexible and amenable to interpretation. Crucially, our design allows key model parameters to be specified at inference time, offering robustness to changes in the likelihood parameters. We train these unfolded samplers end-to-end using a supervised regularized Wasserstein GAN framework for posterior sampling. Through extensive Bayesian imaging experiments, we demonstrate that our proposed approach achieves high sampling accuracy and excellent computational efficiency, while retaining the physics consistency, adaptability and interpretability of classical MCMC strategies.",
    "url": "http://arxiv.org/abs/2602.20758v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20758v1",
    "authors": [
      "Jonathan Spence",
      "Tob\u00edas I. Liaudat",
      "Konstantinos Zygalakis",
      "Marcelo Pereyra"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-24T10:37:10+00:00",
    "updated": "2026-02-24T10:37:10+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20573v1",
    "source": "arxiv",
    "title": "Benchmarking GNN Models on Molecular Regression Tasks with CKA-Based Representation Analysis",
    "abstract": "Molecules are commonly represented as SMILES strings, which can be readily converted to fixed-size molecular fingerprints. These fingerprints serve as feature vectors to train ML/DL models for molecular property prediction tasks in the field of computational chemistry, drug discovery, biochemistry, and materials science. Recent research has demonstrated that SMILES can be used to construct molecular graphs where atoms are nodes ($V$) and bonds are edges ($E$). These graphs can subsequently be used to train geometric DL models like GNN. GNN learns the inherent structural relationships within a molecule rather than depending on fixed-size fingerprints. Although GNN are powerful aggregators, their efficacy on smaller datasets and inductive biases across different architectures is less studied. In our present study, we performed a systematic benchmarking of four different GNN architectures across a diverse domain of datasets (physical chemistry, biological, and analytical). Additionally, we have also implemented a hierarchical fusion (GNN+FP) framework for target prediction. We observed that the fusion framework consistently outperforms or matches the performance of standalone GNN (RMSE improvement > $7\\%$) and baseline models. Further, we investigated the representational similarity using centered kernel alignment (CKA) between GNN and fingerprint embeddings and found that they occupy highly independent latent spaces (CKA $\\le0.46$). The cross-architectural CKA score suggests a high convergence between isotopic models like GCN, GraphSAGE and GIN (CKA $\\geq0.88$), with GAT learning moderately independent representation (CKA $0.55-0.80$).",
    "url": "http://arxiv.org/abs/2602.20573v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20573v1",
    "authors": [
      "Rajan",
      "Ishaan Gupta"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-24T05:53:24+00:00",
    "updated": "2026-02-24T05:53:24+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20557v1",
    "source": "arxiv",
    "title": "GENSR: Symbolic Regression Based in Equation Generative Space",
    "abstract": "Symbolic Regression (SR) tries to reveal the hidden equations behind observed data. However, most methods search within a discrete equation space, where the structural modifications of equations rarely align with their numerical behavior, leaving fitting error feedback too noisy to guide exploration. To address this challenge, we propose GenSR, a generative latent space-based SR framework following the `map construction -> coarse localization -> fine search'' paradigm. Specifically, GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE) to reparameterize symbolic equations into a generative latent space with symbolic continuity and local numerical smoothness. This space can be regarded as a well-structured `map'' of the equation space, providing directional signals for search. At inference, the CVAE coarsely localizes the input data to promising regions in the latent space. Then, a modified CMA-ES refines the candidate region, leveraging smooth latent gradients. From a Bayesian perspective, GenSR reframes the SR task as maximizing the conditional distribution $p(\\mathrm{Equ.} \\mid \\mathrm{Num.})$, with CVAE training achieving this objective through the Evidence Lower Bound (ELBO). This new perspective provides a theoretical guarantee for the effectiveness of GenSR. Extensive experiments show that GenSR jointly optimizes predictive accuracy, expression simplicity, and computational efficiency, while remaining robust under noise.",
    "url": "http://arxiv.org/abs/2602.20557v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20557v1",
    "authors": [
      "Qian Li",
      "Yuxiao Hu",
      "Juncheng Liu",
      "Yuntian Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.SC"
    ],
    "published": "2026-02-24T05:14:34+00:00",
    "updated": "2026-02-24T05:14:34+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20528v1",
    "source": "arxiv",
    "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning",
    "abstract": "The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a \"thinking\" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.",
    "url": "http://arxiv.org/abs/2602.20528v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20528v1",
    "authors": [
      "Justin Lovelace",
      "Christian Belardi",
      "Sofian Zalouk",
      "Adhitya Polavaram",
      "Srivatsa Kundurthy",
      "Kilian Q. Weinberger"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-24T04:09:31+00:00",
    "updated": "2026-02-24T04:09:31+00:00",
    "extra": {
      "primary_category": "cs.CL"
    }
  },
  {
    "uid": "arxiv:2602.20463v1",
    "source": "arxiv",
    "title": "A Long-Short Flow-Map Perspective for Drifting Models",
    "abstract": "This paper provides a reinterpretation of the Drifting Model~\\cite{deng2026generative} through a semigroup-consistent long-short flow-map factorization. We show that a global transport process can be decomposed into a long-horizon flow map followed by a short-time terminal flow map admitting a closed-form optimal velocity representation, and that taking the terminal interval length to zero recovers exactly the drifting field together with a conservative impulse term required for flow-map consistency. Based on this perspective, we propose a new likelihood learning formulation that aligns the long-short flow-map decomposition with density evolution under transport. We validate the framework through both theoretical analysis and empirical evaluations on benchmark tests, and further provide a theoretical interpretation of the feature-space optimization while highlighting several open problems for future study.",
    "url": "http://arxiv.org/abs/2602.20463v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20463v1",
    "authors": [
      "Zhiqi Li",
      "Bo Zhu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-24T01:48:52+00:00",
    "updated": "2026-02-24T01:48:52+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20309v1",
    "source": "arxiv",
    "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
    "abstract": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
    "url": "http://arxiv.org/abs/2602.20309v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20309v1",
    "authors": [
      "Jingxuan Zhang",
      "Yunta Hsieh",
      "Zhongwei Wang",
      "Haokun Lin",
      "Xin Wang",
      "Ziqi Wang",
      "Yingtie Lei",
      "Mi Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-23T19:55:54+00:00",
    "updated": "2026-02-23T19:55:54+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20293v1",
    "source": "arxiv",
    "title": "Discrete Diffusion with Sample-Efficient Estimators for Conditionals",
    "abstract": "We study a discrete denoising diffusion framework that integrates a sample-efficient estimator of single-site conditionals with round-robin noising and denoising dynamics for generative modeling over discrete state spaces. Rather than approximating a discrete analog of a score function, our formulation treats single-site conditional probabilities as the fundamental objects that parameterize the reverse diffusion process. We employ a sample-efficient method known as Neural Interaction Screening Estimator (NeurISE) to estimate these conditionals in the diffusion dynamics. Controlled experiments on synthetic Ising models, MNIST, and scientific data sets produced by a D-Wave quantum annealer, synthetic Potts model and one-dimensional quantum systems demonstrate the proposed approach. On the binary data sets, these experiments demonstrate that the proposed approach outperforms popular existing methods including ratio-based approaches, achieving improved performance in total variation, cross-correlations, and kernel density estimation metrics.",
    "url": "http://arxiv.org/abs/2602.20293v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20293v1",
    "authors": [
      "Karthik Elamvazhuthi",
      "Abhijith Jayakumar",
      "Andrey Y. Lokhov"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-23T19:20:37+00:00",
    "updated": "2026-02-23T19:20:37+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20232v1",
    "source": "arxiv",
    "title": "Coupled Cluster con M\u014dLe: Molecular Orbital Learning for Neural Wavefunctions",
    "abstract": "Density functional theory (DFT) is the most widely used method for calculating molecular properties; however, its accuracy is often insufficient for quantitative predictions. Coupled-cluster (CC) theory is the most successful method for achieving accuracy beyond DFT and for predicting properties that closely align with experiment. It is known as the ''gold standard'' of quantum chemistry. Unfortunately, the high computational cost of CC limits its widespread applicability. In this work, we present the Molecular Orbital Learning (M\u014dLe) architecture, an equivariant machine learning model that directly predicts CC's core mathematical objects, the excitation amplitudes, from the mean-field Hartree-Fock molecular orbitals as inputs. We test various aspects of our model and demonstrate its remarkable data efficiency and out-of-distribution generalization to larger molecules and off-equilibrium geometries, despite being trained only on small equilibrium geometries. Finally, we also examine its ability to reduce the number of cycles required to converge CC calculations. M\u014dLe can set the foundations for high-accuracy wavefunction-based ML architectures to accelerate molecular design and complement force-field approaches.",
    "url": "http://arxiv.org/abs/2602.20232v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20232v1",
    "authors": [
      "Luca Thiede",
      "Abdulrahman Aldossary",
      "Andreas Burger",
      "Jorge Arturo Campos-Gonzalez-Angulo",
      "Ning Wang",
      "Alexander Zook",
      "Melisa Alkan",
      "Kouhei Nakaji",
      "Taylor Lee Patti",
      "J\u00e9r\u00f4me Florian Gonthier",
      "Mohammad Ghazi Vakili",
      "Al\u00e1n Aspuru-Guzik"
    ],
    "categories": [
      "cs.LG",
      "physics.chem-ph"
    ],
    "published": "2026-02-23T18:54:46+00:00",
    "updated": "2026-02-23T18:54:46+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20132v1",
    "source": "arxiv",
    "title": "LAD: Learning Advantage Distribution for Reasoning",
    "abstract": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
    "url": "http://arxiv.org/abs/2602.20132v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20132v1",
    "authors": [
      "Wendi Li",
      "Sharon Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-23T18:44:10+00:00",
    "updated": "2026-02-23T18:44:10+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20126v1",
    "source": "arxiv",
    "title": "Adaptation to Intrinsic Dependence in Diffusion Language Models",
    "abstract": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.",
    "url": "http://arxiv.org/abs/2602.20126v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20126v1",
    "authors": [
      "Yunxiao Zhao",
      "Changxiao Cai"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.ST",
      "stat.ML"
    ],
    "published": "2026-02-23T18:41:34+00:00",
    "updated": "2026-02-23T18:41:34+00:00",
    "extra": {
      "primary_category": "cs.LG"
    }
  },
  {
    "uid": "arxiv:2602.20995v1",
    "source": "arxiv",
    "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs",
    "abstract": "Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.",
    "url": "http://arxiv.org/abs/2602.20995v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20995v1",
    "authors": [
      "Junyu Bi",
      "Xinting Niu",
      "Daixuan Cheng",
      "Kun Yuan",
      "Tao Wang",
      "Binbin Cao",
      "Jian Wu",
      "Yuning Jiang"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "published": "2026-02-24T15:14:49+00:00",
    "updated": "2026-02-24T15:14:49+00:00",
    "extra": {
      "primary_category": "cs.IR"
    }
  },
  {
    "uid": "arxiv:2602.20976v1",
    "source": "arxiv",
    "title": "Evaluating Proactive Risk Awareness of Large Language Models",
    "abstract": "As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.",
    "url": "http://arxiv.org/abs/2602.20976v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20976v1",
    "authors": [
      "Xuan Luo",
      "Yubin Chen",
      "Zhiyu Hou",
      "Linpu Yu",
      "Geng Tu",
      "Jing Li",
      "Ruifeng Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "published": "2026-02-24T15:00:00+00:00",
    "updated": "2026-02-24T15:00:00+00:00",
    "extra": {
      "primary_category": "cs.CL"
    }
  },
  {
    "uid": "arxiv:2602.21111v1",
    "source": "arxiv",
    "title": "Density Functional Theory Predictions of Derivative Thermodynamic Properties of a Confined Fluid",
    "abstract": "Fluids in nanopores are of importance for many engineering applications, including energy storage in supercapacitors, hydrocarbons recovery from unconventional sources, or water desalination. Thermodynamic properties of fluids confined in nanopores differ from the properties of the same fluids in bulk. Density functional theory (DFT) has been widely used for modeling thermodynamics of confined fluids. However, it is rarely used for calculations of derivative thermodynamic properties. Here we use a rather simple DFT model for argon based on the Percus-Yevick equation, and showed that with standard parametrization it fails to predict derivative properties. However, slight adjustment in parameters leads to quantitative predictions of isothermal compressibility and thermal expansion coefficient at a selected temperature. Using the adjusted parameterization we performed the calculations of compressibility of argon confined in carbon slit pores of various sizes, and demonstrated that the compressibility of argon in confinement is lower than that in bulk and is pore size dependent. We confirmed the DFT predictions using the Monte Carlo molecular simulations. In addition to isothermal compressibility, we calculated the thermal expansion coefficient of confined argon. Our calculations showed that it behaves similar to compressibility -- it is always lower than the bulk value and gradually increases for smaller pore sizes. For several selected pore sizes we verified the DFT calculations by Monte Carlo simulations. Overall, our results suggest that the classical DFT can be utilized for calculations of derivative thermodynamic properties of confined fluids, which are computationally challenging to predict using molecular simulations.",
    "url": "http://arxiv.org/abs/2602.21111v1",
    "pdf_url": "https://arxiv.org/pdf/2602.21111v1",
    "authors": [
      "Gennady Y. Gor",
      "Geordy Jomon",
      "Andrei L. Kolesnikov"
    ],
    "categories": [
      "physics.chem-ph",
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "published": "2026-02-24T17:08:37+00:00",
    "updated": "2026-02-24T17:08:37+00:00",
    "extra": {
      "primary_category": "physics.chem-ph"
    }
  },
  {
    "uid": "arxiv:2602.20456v1",
    "source": "arxiv",
    "title": "Continuous Local Symmetry: Connection to Reactivity and Recognition",
    "abstract": "Symmetry is one of the most beautiful yet mysterious concepts in science. In chemical systems, presence of local symmetries at specific fragments often serve as driving forces behind many physicochemical properties, including stability, spectroscopy, and reactivity. Moreover, degree of symmetry varies continuously with molecular dynamics and intermolecular interactions, making it a hidden but decisive factor. In this study, we propose a theoretical framework to quantify continuous degrees of symmetry and chirality localized within constrained regions of a molecular environment. Application of this method to reaction sites of dendralene molecules reveals strong correlations between local symmetry and molecular stability, parity-dependent behavior, and Diels-Alder reactivity. Additionally, representations of local chirality fields in porphyrins uncover unique signatures accounting for the chirality recognition power. Overall, these findings highlight the potentials of local symmetries within a molecular framework on predicting chemical properties.",
    "url": "http://arxiv.org/abs/2602.20456v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20456v1",
    "authors": [
      "Duc Anh Lai",
      "Devin A. Matthews"
    ],
    "categories": [
      "physics.chem-ph"
    ],
    "published": "2026-02-24T01:32:35+00:00",
    "updated": "2026-02-24T01:32:35+00:00",
    "extra": {
      "primary_category": "physics.chem-ph"
    }
  },
  {
    "uid": "arxiv:2602.20391v1",
    "source": "arxiv",
    "title": "Transient Plastic Spin Labeling with Chlorine Dioxide",
    "abstract": "Plastic waste, being one of the most important problems for humankind, poses severe threats to ecosystems, wildlife, and human health. Tracing, quantifying, and identifying types of plastic waste is of crucial importance to understand its environmental pathways and develop targeted strategies for reduction, recycling, and remediation. To contribute to addressing this global issue, we investigated the spin-labeling capabilities of aqueous chlorine dioxide (ClO$_2$) radicals upon introduction into poly(ethylene terephthalate) and utilized electron spin resonance spectroscopy for detection. The technique is capable of identifying plastic species as the unpaired electron of the radical molecule is strongly sensitive to its local environment through its coupling parameters. Temperature-dependent measurements revealed that the molecules are immobilized at low temperatures and exhibit well-resolved anisotropic and hyperfine spectra that are quantitatively described by a model spin Hamiltonian. Even above the melting point of water, some degrees of freedom remain blocked as a result of the polymer matrix. Furthermore, employing a time-series measurement at room temperature enabled us to determine the diffusion coefficient of the molecule in the polymer.",
    "url": "http://arxiv.org/abs/2602.20391v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20391v1",
    "authors": [
      "Bence G. M\u00e1rkus",
      "S\u00e1ndor Kollarics",
      "Krist\u00f3f K\u00e1ly-Kullai",
      "Bernadett Juh\u00e1sz",
      "D\u00e1vid Beke",
      "L\u00e1szl\u00f3 Forr\u00f3",
      "Zolt\u00e1n Noszticzius",
      "Ferenc Simon"
    ],
    "categories": [
      "cond-mat.other",
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "published": "2026-02-23T22:11:47+00:00",
    "updated": "2026-02-23T22:11:47+00:00",
    "extra": {
      "primary_category": "cond-mat.other"
    }
  },
  {
    "uid": "arxiv:2602.20320v1",
    "source": "arxiv",
    "title": "Environment-Induced Exciton Renormalization in the Photosystem II Reaction Center",
    "abstract": "Protein electrostatics tune excitation energies in the Photosystem II reaction center (PSII-RC), yet a fully quantum-mechanical many-body description of how the surrounding protein environment renormalizes excitons has remained computationally inaccessible. The Bethe-Salpeter equation (BSE) within many-body perturbation theory accurately describes excitonic physics through an explicit electron-hole interaction, but is prohibitively expensive for systems containing thousands of valence electrons. Here, we show that for sufficiently large systems the BSE becomes simpler to solve when treated with modern stochastic sampling techniques, as atomistic interactions self-average. In this regime, the effective electron-hole interaction mediated by the environment is governed by collective $k$-dependent polarization. These insights enable an ab initio study of the PSII-RC in which all six chlorins forming the hexameric dye core are treated explicitly together with a roughly seven Angstrom local protein environment. We directly compare the low-lying optical excitations of the isolated chromophore hexamer (1276 valence electrons) and the protein-dye cluster (3238 valence electrons). For $Q_y$ excitations near 680 nm, inclusion of the protein environment induces polarization-dependent energy shifts, redistributes spectral weight, and alters exciton delocalization and pigment character. Lateral and transverse asymmetries in the low-lying excited states are captured at the BSE level of theory. These results establish that we now have the tools for many-body calculations of biological nanostructures.",
    "url": "http://arxiv.org/abs/2602.20320v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20320v1",
    "authors": [
      "Tucker Allen",
      "Barry Y. Li",
      "Nadine C. Bradbury",
      "Daniel Neuhauser"
    ],
    "categories": [
      "physics.chem-ph"
    ],
    "published": "2026-02-23T20:10:28+00:00",
    "updated": "2026-02-23T20:10:28+00:00",
    "extra": {
      "primary_category": "physics.chem-ph"
    }
  },
  {
    "uid": "arxiv:2602.20140v1",
    "source": "arxiv",
    "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
    "abstract": "Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameters given a molecular graph. This lattice-aware generation interfaces directly with downstream relaxation and lattice-energy ranking, positioning PackFlow as a scalable proposal engine within standard CSP pipelines. To explicitly steer generation toward physically favourable regions, we propose physics alignment, a reinforcement learning post-training stage that uses machine-learned interatomic potential energies and forces as stability proxies. Physics alignment improves physical validity without altering inference-time sampling. We validate PackFlow's performance against heuristic baselines through two distinct evaluations. First, on a broad unseen set of molecular systems, we demonstrate superior candidate generation capability, with proposals exhibiting greater structural similarity to experimental polymorphs. Second, we assess the full end-to-end workflow on two unseen CSP blind-test case studies, including relaxation and lattice-energy analysis. In both settings, PackFlow outperforms heuristics-based methods by concentrating probability mass in low-energy basins, yielding candidates that relax into lower-energy minima and offering a practical route to amortize the relax-and-rank bottleneck.",
    "url": "http://arxiv.org/abs/2602.20140v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20140v1",
    "authors": [
      "Akshay Subramanian",
      "Elton Pan",
      "Juno Nam",
      "Maurice Weiler",
      "Shuhui Qu",
      "Cheol Woo Park",
      "Tommi S. Jaakkola",
      "Elsa Olivetti",
      "Rafael Gomez-Bombarelli"
    ],
    "categories": [
      "physics.chem-ph"
    ],
    "published": "2026-02-23T18:52:13+00:00",
    "updated": "2026-02-23T18:52:13+00:00",
    "extra": {
      "primary_category": "physics.chem-ph"
    }
  }
]